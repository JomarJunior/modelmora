{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ModelMora","text":"<p>Neural Network Model Management and Inference Serving Microservice</p>"},{"location":"#overview","title":"Overview","text":"<p>ModelMora is a high-performance inference serving microservice designed to manage and serve multiple neural network models efficiently. Built for the MiraVeja ecosystem, it provides robust model lifecycle management, intelligent request queuing, and multi-process isolation for optimal resource utilization.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\ude80 High Performance: Async-first architecture with validated &lt;1\u03bcs queue latency</li> <li>\ud83d\udd12 Memory Isolation: Multi-process architecture with 5000x better memory cleanup than GC</li> <li>\ud83c\udfaf Intelligent Queuing: Priority-based request scheduling with 730k ops/sec capacity</li> <li>\ud83d\udd04 Lazy Loading: On-demand model loading with 2s load time for cached models</li> <li>\ud83d\udcca Multiple Protocols: REST and gRPC APIs for different use cases</li> <li>\ud83d\udc33 Container Ready: Docker and Kubernetes deployment support</li> </ul>"},{"location":"#architecture-highlights","title":"Architecture Highlights","text":"<p>ModelMora follows Domain-Driven Design (DDD) principles with clear bounded contexts:</p> <ul> <li>Registry: Model metadata and configuration management</li> <li>Lifecycle: Model loading, unloading, and health monitoring</li> <li>Inference: Request queuing, batching, and execution</li> <li>Observability: Metrics, logging, and tracing</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install dependencies\npoetry install\n\n# Start the service\npoetry run modelmora serve\n\n# Register a model\npoetry run modelmora install sentence-transformers/all-MiniLM-L6-v2\n\n# Run inference\ncurl -X POST http://localhost:8000/infer/all-MiniLM-L6-v2 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\": \"Hello, world!\"}'\n</code></pre>"},{"location":"#performance-benchmarks","title":"Performance Benchmarks","text":"<p>All architectural decisions validated through production-grade POCs:</p> Metric Value Status Queue Latency 0.7\u03bcs enqueue \u2705 Validated Model Load Time 2s (cached) \u2705 Validated Memory Cleanup 0.1MB leak (subprocess) \u2705 Validated gRPC Throughput 31.92 MB/s \u2705 Validated Queue Capacity 730k ops/sec \u2705 Validated"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Embedding Services: Text and image embeddings at scale</li> <li>Text Generation: LLM inference with batching support</li> <li>Image Generation: Diffusion models with streaming responses</li> <li>Multi-Model Serving: Efficient resource sharing across models</li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<ul> <li>\u2705 Phase 0: Requirements &amp; Planning - Complete</li> <li>\ud83d\udea7 Phase 1: MVP Core - In Progress</li> <li>\ud83d\udcc5 Phase 2: Production Ready - Planned</li> <li>\ud83d\udcc5 Phase 3: Scale &amp; Polish - Planned</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started</li> <li>Architecture</li> <li>API Reference</li> <li>Development Roadmap</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"api_design/","title":"ModelMora - API Design Document","text":"<p>Version: 1.0.0 Date: 2025-12-04 Status: Draft</p>"},{"location":"api_design/#1-introduction","title":"1. Introduction","text":""},{"location":"api_design/#11-purpose","title":"1.1 Purpose","text":"<p>This document defines the API contracts for ModelMora, including REST endpoints (OpenAPI specification), gRPC service definitions (Protocol Buffers), and message schemas for event-driven communication. These specifications serve as the contract between ModelMora and its clients.</p>"},{"location":"api_design/#12-api-versioning-strategy","title":"1.2 API Versioning Strategy","text":"<ul> <li>REST API: Version prefix in URL path (<code>/api/v1/...</code>)</li> <li>gRPC: Version suffix in package name (<code>modelmora.v1</code>)</li> <li>Breaking Changes: Require major version increment</li> <li>Deprecation: Supported for 2 minor versions before removal</li> </ul>"},{"location":"api_design/#13-api-principles","title":"1.3 API Principles","text":"<ul> <li>Resource-Oriented: REST endpoints follow RESTful conventions</li> <li>Idempotent Operations: PUT/DELETE operations are idempotent</li> <li>Consistency: Common error formats and status codes</li> <li>Validation: Input validation with detailed error messages</li> <li>Documentation: Self-documenting with OpenAPI/Swagger UI</li> </ul>"},{"location":"api_design/#14-poc-validated-performance-characteristics","title":"1.4 POC-Validated Performance Characteristics","text":"<p>REST API Performance:</p> <ul> <li>Suitable for: Small requests/responses (embeddings, text, metadata)</li> <li>Typical latency: &lt;50ms for simple inference</li> <li>Synchronous by default, async endpoints for long-running tasks</li> </ul> <p>gRPC Streaming Performance: \u2705 (POC 2: Validated)</p> <ul> <li>Throughput 31.92 MB/s (single client), 33.31 MB/s (10 concurrent clients)</li> <li>Latency: 23.53ms per chunk average</li> <li>Use cases:</li> <li>Image generation (3MB+ per result)</li> <li>Large batch embeddings</li> <li>Video/audio processing</li> <li>Protocol: Server-side streaming with Protocol Buffers</li> <li>Verdict: gRPC performs well for large payloads, both protocols viable</li> </ul> <p>API Selection Guide:</p> Use Case Protocol Rationale Text embedding (&lt;1KB) REST Simple, low overhead Batch embeddings (&gt;100KB) gRPC Streaming efficiency Image generation (3MB+) gRPC Validated 31.92 MB/s throughput Model management (CRUD) REST Resource-oriented operations Health checks REST Simple request/response Real-time streaming gRPC Bidirectional streaming support"},{"location":"api_design/#2-rest-api-specification-openapi-30","title":"2. REST API Specification (OpenAPI 3.0)","text":""},{"location":"api_design/#21-openapi-metadata","title":"2.1 OpenAPI Metadata","text":"<pre><code>openapi: 3.0.3\ninfo:\n  title: ModelMora API\n  version: 1.0.0\n  description: |\n    ModelMora is a lightweight model serving framework for managing and executing\n    machine learning model inference at scale.\n\n    ## Features\n    - Model registry with version management\n    - Lazy model loading with LRU eviction\n    - Synchronous and asynchronous inference\n    - Priority-based request queuing\n    - Batch processing for improved throughput\n\n  contact:\n    name: MiraVeja Team\n    email: support@miraveja.com\n  license:\n    name: MIT\n    url: https://opensource.org/licenses/MIT\n\nservers:\n  - url: http://localhost:8000/api/v1\n    description: Local development server\n  - url: https://modelmora.miraveja.com/api/v1\n    description: Production server\n\ntags:\n  - name: Registry\n    description: Model catalog management\n  - name: Inference\n    description: Model inference operations\n  - name: Lifecycle\n    description: Model lifecycle management\n  - name: Jobs\n    description: Asynchronous job management\n  - name: Health\n    description: Health check endpoints\n  - name: Metrics\n    description: Observability and metrics\n\nsecurity:\n  - BearerAuth: []\n</code></pre>"},{"location":"api_design/#22-common-components","title":"2.2 Common Components","text":""},{"location":"api_design/#221-security-schemes","title":"2.2.1 Security Schemes","text":"<pre><code>components:\n  securitySchemes:\n    BearerAuth:\n      type: http\n      scheme: bearer\n      bearerFormat: JWT\n      description: JWT token for API authentication (optional in MVP)\n</code></pre>"},{"location":"api_design/#222-common-schemas","title":"2.2.2 Common Schemas","text":"<pre><code>components:\n  schemas:\n    # Error Response\n    Error:\n      type: object\n      required:\n        - error_code\n        - message\n      properties:\n        error_code:\n          type: string\n          example: \"MODEL_NOT_FOUND\"\n        message:\n          type: string\n          example: \"Model 'invalid-model-id' not found in registry\"\n        details:\n          type: object\n          additionalProperties: true\n        trace_id:\n          type: string\n          format: uuid\n          description: Distributed tracing ID\n\n    # Validation Error\n    ValidationError:\n      type: object\n      required:\n        - error_code\n        - message\n        - validation_errors\n      properties:\n        error_code:\n          type: string\n          example: \"VALIDATION_ERROR\"\n        message:\n          type: string\n          example: \"Request validation failed\"\n        validation_errors:\n          type: array\n          items:\n            type: object\n            properties:\n              field:\n                type: string\n                example: \"model_id\"\n              message:\n                type: string\n                example: \"Field is required\"\n\n    # Pagination\n    PaginationMeta:\n      type: object\n      properties:\n        total:\n          type: integer\n          example: 42\n        page:\n          type: integer\n          example: 1\n        page_size:\n          type: integer\n          example: 20\n        total_pages:\n          type: integer\n          example: 3\n</code></pre>"},{"location":"api_design/#23-registry-api","title":"2.3 Registry API","text":""},{"location":"api_design/#231-register-model","title":"2.3.1 Register Model","text":"<pre><code>paths:\n  /models:\n    post:\n      tags:\n        - Registry\n      summary: Register a new model\n      description: |\n        Registers a new model in the ModelMora catalog. The model files are not\n        downloaded immediately; use the `/download` endpoint or lazy loading will\n        occur on first inference request.\n      operationId: registerModel\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/RegisterModelRequest'\n            examples:\n              embedding_model:\n                summary: Text embedding model\n                value:\n                  model_id: \"sentence-transformers/all-MiniLM-L6-v2\"\n                  version: \"v2.2.2\"\n                  task_type: \"txt2embed\"\n                  display_name: \"MiniLM-L6 Sentence Embeddings\"\n                  config:\n                    vector_size: 384\n                    max_length: 512\n                    batch_size: 32\n              image_gen_model:\n                summary: Text-to-image model\n                value:\n                  model_id: \"stabilityai/stable-diffusion-2-1\"\n                  version: \"fp16\"\n                  task_type: \"txt2img\"\n                  resource_requirements:\n                    gpu_vram_mb: 8192\n      responses:\n        '201':\n          description: Model registered successfully\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ModelResponse'\n        '400':\n          description: Invalid request\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ValidationError'\n        '409':\n          description: Model already exists\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Error'\n\ncomponents:\n  schemas:\n    RegisterModelRequest:\n      type: object\n      required:\n        - model_id\n        - task_type\n      properties:\n        model_id:\n          type: string\n          description: HuggingFace model identifier\n          example: \"sentence-transformers/all-MiniLM-L6-v2\"\n        version:\n          type: string\n          description: Specific version or branch name\n          example: \"v2.2.2\"\n          default: \"main\"\n        task_type:\n          $ref: '#/components/schemas/TaskType'\n        display_name:\n          type: string\n          example: \"MiniLM-L6 Sentence Embeddings\"\n        description:\n          type: string\n          example: \"Lightweight sentence embedding model\"\n        source_url:\n          type: string\n          format: uri\n          description: Custom download URL (overrides HuggingFace)\n        resource_requirements:\n          $ref: '#/components/schemas/ResourceRequirements'\n        config:\n          type: object\n          description: Model-specific configuration\n          additionalProperties: true\n\n    ModelResponse:\n      type: object\n      properties:\n        model_id:\n          type: string\n        version:\n          type: string\n        task_type:\n          $ref: '#/components/schemas/TaskType'\n        display_name:\n          type: string\n        description:\n          type: string\n        status:\n          type: string\n          enum: [registered, downloading, ready, failed]\n        resource_requirements:\n          $ref: '#/components/schemas/ResourceRequirements'\n        config:\n          type: object\n          additionalProperties: true\n        created_at:\n          type: string\n          format: date-time\n        updated_at:\n          type: string\n          format: date-time\n\n    TaskType:\n      type: string\n      enum:\n        - txt2embed\n        - img2embed\n        - txt2img\n        - img2txt\n        - txt2txt\n      description: |\n        - `txt2embed`: Text to embedding vector\n        - `img2embed`: Image to embedding vector\n        - `txt2img`: Text to image generation\n        - `img2txt`: Image to text (captioning)\n        - `txt2txt`: Text to text (generation, translation)\n\n    ResourceRequirements:\n      type: object\n      properties:\n        memory_mb:\n          type: integer\n          description: Estimated RAM requirement\n          example: 2048\n        gpu_vram_mb:\n          type: integer\n          description: Estimated VRAM requirement\n          example: 4096\n        cpu_threads:\n          type: integer\n          description: Recommended CPU threads\n          example: 4\n</code></pre>"},{"location":"api_design/#232-list-models","title":"2.3.2 List Models","text":"<pre><code>paths:\n  /models:\n    get:\n      tags:\n        - Registry\n      summary: List registered models\n      description: Retrieve all models with optional filtering\n      operationId: listModels\n      parameters:\n        - name: task_type\n          in: query\n          schema:\n            $ref: '#/components/schemas/TaskType'\n          description: Filter by task type\n        - name: status\n          in: query\n          schema:\n            type: string\n            enum: [registered, downloading, ready, failed]\n        - name: page\n          in: query\n          schema:\n            type: integer\n            default: 1\n            minimum: 1\n        - name: page_size\n          in: query\n          schema:\n            type: integer\n            default: 20\n            minimum: 1\n            maximum: 100\n      responses:\n        '200':\n          description: List of models\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  data:\n                    type: array\n                    items:\n                      $ref: '#/components/schemas/ModelResponse'\n                  meta:\n                    $ref: '#/components/schemas/PaginationMeta'\n</code></pre>"},{"location":"api_design/#233-get-model-details","title":"2.3.3 Get Model Details","text":"<pre><code>paths:\n  /models/{model_id}:\n    get:\n      tags:\n        - Registry\n      summary: Get model details\n      operationId: getModel\n      parameters:\n        - name: model_id\n          in: path\n          required: true\n          schema:\n            type: string\n          description: URL-encoded model identifier\n          example: \"sentence-transformers%2Fall-MiniLM-L6-v2\"\n      responses:\n        '200':\n          description: Model details\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ModelResponse'\n        '404':\n          description: Model not found\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/Error'\n</code></pre>"},{"location":"api_design/#234-update-model","title":"2.3.4 Update Model","text":"<pre><code>paths:\n  /models/{model_id}:\n    put:\n      tags:\n        - Registry\n      summary: Update model configuration\n      operationId: updateModel\n      parameters:\n        - name: model_id\n          in: path\n          required: true\n          schema:\n            type: string\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/UpdateModelRequest'\n      responses:\n        '200':\n          description: Model updated\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ModelResponse'\n        '404':\n          description: Model not found\n\ncomponents:\n  schemas:\n    UpdateModelRequest:\n      type: object\n      properties:\n        display_name:\n          type: string\n        description:\n          type: string\n        config:\n          type: object\n          additionalProperties: true\n        resource_requirements:\n          $ref: '#/components/schemas/ResourceRequirements'\n</code></pre>"},{"location":"api_design/#235-delete-model","title":"2.3.5 Delete Model","text":"<pre><code>paths:\n  /models/{model_id}:\n    delete:\n      tags:\n        - Registry\n      summary: Delete model\n      description: Removes model from registry and deletes cached files\n      operationId: deleteModel\n      parameters:\n        - name: model_id\n          in: path\n          required: true\n          schema:\n            type: string\n        - name: force\n          in: query\n          schema:\n            type: boolean\n            default: false\n          description: Force deletion even if model is loaded\n      responses:\n        '204':\n          description: Model deleted successfully\n        '404':\n          description: Model not found\n        '409':\n          description: Model is currently loaded (use force=true)\n</code></pre>"},{"location":"api_design/#236-download-model","title":"2.3.6 Download Model","text":"<pre><code>paths:\n  /models/{model_id}/download:\n    post:\n      tags:\n        - Registry\n      summary: Download model files\n      description: Explicitly downloads model files to cache\n      operationId: downloadModel\n      parameters:\n        - name: model_id\n          in: path\n          required: true\n          schema:\n            type: string\n      responses:\n        '202':\n          description: Download started\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  model_id:\n                    type: string\n                  status:\n                    type: string\n                    enum: [downloading]\n                  message:\n                    type: string\n                    example: \"Download started in background\"\n        '404':\n          description: Model not found\n</code></pre>"},{"location":"api_design/#237-generate-lock-file","title":"2.3.7 Generate Lock File","text":"<pre><code>paths:\n  /models/lock:\n    post:\n      tags:\n        - Registry\n      summary: Generate lock file\n      description: Creates a lock file with pinned model versions\n      operationId: generateLock\n      responses:\n        '200':\n          description: Lock file content\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/LockFile'\n\ncomponents:\n  schemas:\n    LockFile:\n      type: object\n      properties:\n        version:\n          type: string\n          example: \"1.0\"\n        generated_at:\n          type: string\n          format: date-time\n        models:\n          type: array\n          items:\n            type: object\n            properties:\n              model_id:\n                type: string\n              version:\n                type: string\n              source_url:\n                type: string\n              checksum:\n                type: string\n              task_type:\n                $ref: '#/components/schemas/TaskType'\n</code></pre>"},{"location":"api_design/#24-inference-api","title":"2.4 Inference API","text":""},{"location":"api_design/#241-synchronous-inference","title":"2.4.1 Synchronous Inference","text":"<p>POC-Validated Queue Performance:</p> <ul> <li>Priority queue latency: 0.7\u03bcs enqueue (negligible)</li> <li>4 priority levels: CRITICAL(1), HIGH(2), NORMAL(3), LOW(4)</li> <li>100% priority ordering correctness under load</li> <li>Capacity: 730,108 ops/sec (730x above target)</li> </ul> <pre><code>paths:\n  /infer/{model_id}:\n    post:\n      tags:\n        - Inference\n      summary: Execute synchronous inference\n      description: |\n        Executes inference and waits for result. Suitable for low-latency\n        requests like embeddings. For long-running tasks, use async endpoint.\n      operationId: infer\n      parameters:\n        - name: model_id\n          in: path\n          required: true\n          schema:\n            type: string\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/InferenceRequest'\n            examples:\n              text_embedding:\n                summary: Text embedding request\n                value:\n                  input_data:\n                    text: \"This is a sample sentence to embed\"\n                  parameters:\n                    normalize: true\n              text_generation:\n                summary: Text generation request\n                value:\n                  input_data:\n                    text: \"Once upon a time\"\n                  parameters:\n                    max_tokens: 100\n                    temperature: 0.7\n                  priority: \"medium\"\n              image_generation:\n                summary: Image generation (should use async)\n                value:\n                  input_data:\n                    text: \"A beautiful sunset over mountains\"\n                  parameters:\n                    width: 512\n                    height: 512\n                    steps: 50\n                  priority: \"low\"\n                  timeout_seconds: 300\n      responses:\n        '200':\n          description: Inference completed\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/InferenceResponse'\n        '400':\n          description: Invalid input\n        '404':\n          description: Model not found\n        '408':\n          description: Request timeout\n        '503':\n          description: Model loading or service unavailable\n\ncomponents:\n  schemas:\n    InferenceRequest:\n      type: object\n      required:\n        - input_data\n      properties:\n        input_data:\n          $ref: '#/components/schemas/InputData'\n        parameters:\n          type: object\n          description: Task-specific inference parameters\n          additionalProperties: true\n        priority:\n          type: string\n          enum: [high, medium, low]\n          default: medium\n          description: |\n            - `high`: Fast tasks (embeddings, &lt;1s)\n            - `medium`: Text generation (1-10s)\n            - `low`: Image generation (&gt;10s)\n        timeout_seconds:\n          type: integer\n          minimum: 1\n          maximum: 600\n          default: 60\n\n    InputData:\n      type: object\n      description: Task-specific input data structure\n      properties:\n        text:\n          oneOf:\n            - type: string\n            - type: array\n              items:\n                type: string\n          example: \"Sample text input\"\n        image:\n          type: string\n          description: Base64-encoded image or URL\n          example: \"data:image/png;base64,iVBORw0KG...\"\n      additionalProperties: true\n\n    InferenceResponse:\n      type: object\n      properties:\n        model_id:\n          type: string\n        output_data:\n          $ref: '#/components/schemas/OutputData'\n        metadata:\n          type: object\n          properties:\n            inference_time_ms:\n              type: number\n              description: Model inference duration\n            total_time_ms:\n              type: number\n              description: Total request duration\n            batch_size:\n              type: integer\n            device:\n              type: string\n              enum: [cpu, cuda]\n\n    OutputData:\n      type: object\n      description: Task-specific output data structure\n      properties:\n        embedding:\n          type: array\n          items:\n            type: number\n          example: [0.123, -0.456, 0.789]\n        embeddings:\n          type: array\n          description: For batch embedding requests\n          items:\n            type: array\n            items:\n              type: number\n        text:\n          type: string\n          example: \"Generated text output\"\n        image_url:\n          type: string\n          format: uri\n          description: Presigned URL for generated image\n        image_base64:\n          type: string\n          description: Base64-encoded image (if inline)\n      additionalProperties: true\n</code></pre>"},{"location":"api_design/#242-asynchronous-inference","title":"2.4.2 Asynchronous Inference","text":"<pre><code>paths:\n  /infer/{model_id}/async:\n    post:\n      tags:\n        - Inference\n      summary: Execute asynchronous inference\n      description: |\n        Submits inference request and returns immediately with job ID.\n        Use `/jobs/{job_id}` endpoint to check status and retrieve result.\n      operationId: inferAsync\n      parameters:\n        - name: model_id\n          in: path\n          required: true\n          schema:\n            type: string\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/InferenceRequest'\n      responses:\n        '202':\n          description: Request accepted\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/JobReference'\n        '400':\n          description: Invalid input\n        '404':\n          description: Model not found\n\ncomponents:\n  schemas:\n    JobReference:\n      type: object\n      properties:\n        job_id:\n          type: string\n          format: uuid\n          example: \"123e4567-e89b-12d3-a456-426614174000\"\n        status:\n          $ref: '#/components/schemas/JobStatus'\n        created_at:\n          type: string\n          format: date-time\n        status_url:\n          type: string\n          format: uri\n          example: \"/api/v1/jobs/123e4567-e89b-12d3-a456-426614174000\"\n\n    JobStatus:\n      type: string\n      enum:\n        - queued\n        - processing\n        - completed\n        - failed\n        - cancelled\n</code></pre>"},{"location":"api_design/#25-job-management-api","title":"2.5 Job Management API","text":""},{"location":"api_design/#251-get-job-status","title":"2.5.1 Get Job Status","text":"<pre><code>paths:\n  /jobs/{job_id}:\n    get:\n      tags:\n        - Jobs\n      summary: Get job status\n      operationId: getJobStatus\n      parameters:\n        - name: job_id\n          in: path\n          required: true\n          schema:\n            type: string\n            format: uuid\n      responses:\n        '200':\n          description: Job status\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/JobStatusResponse'\n        '404':\n          description: Job not found\n\ncomponents:\n  schemas:\n    JobStatusResponse:\n      type: object\n      properties:\n        job_id:\n          type: string\n          format: uuid\n        model_id:\n          type: string\n        status:\n          $ref: '#/components/schemas/JobStatus'\n        progress:\n          type: number\n          minimum: 0\n          maximum: 100\n          description: Progress percentage (if available)\n        created_at:\n          type: string\n          format: date-time\n        started_at:\n          type: string\n          format: date-time\n        completed_at:\n          type: string\n          format: date-time\n        error:\n          $ref: '#/components/schemas/Error'\n</code></pre>"},{"location":"api_design/#252-get-job-result","title":"2.5.2 Get Job Result","text":"<pre><code>paths:\n  /jobs/{job_id}/result:\n    get:\n      tags:\n        - Jobs\n      summary: Retrieve job result\n      operationId: getJobResult\n      parameters:\n        - name: job_id\n          in: path\n          required: true\n          schema:\n            type: string\n            format: uuid\n      responses:\n        '200':\n          description: Job result\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/InferenceResponse'\n        '404':\n          description: Job not found\n        '409':\n          description: Job not completed yet\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  error_code:\n                    type: string\n                    example: \"JOB_NOT_READY\"\n                  message:\n                    type: string\n                  status:\n                    $ref: '#/components/schemas/JobStatus'\n</code></pre>"},{"location":"api_design/#253-cancel-job","title":"2.5.3 Cancel Job","text":"<pre><code>paths:\n  /jobs/{job_id}:\n    delete:\n      tags:\n        - Jobs\n      summary: Cancel job\n      description: Attempts to cancel a queued or processing job\n      operationId: cancelJob\n      parameters:\n        - name: job_id\n          in: path\n          required: true\n          schema:\n            type: string\n            format: uuid\n      responses:\n        '200':\n          description: Job cancelled\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  job_id:\n                    type: string\n                  status:\n                    type: string\n                    example: \"cancelled\"\n        '404':\n          description: Job not found\n        '409':\n          description: Job already completed or failed\n</code></pre>"},{"location":"api_design/#26-lifecycle-management-api","title":"2.6 Lifecycle Management API","text":""},{"location":"api_design/#261-load-model","title":"2.6.1 Load Model","text":"<pre><code>paths:\n  /models/{model_id}/load:\n    post:\n      tags:\n        - Lifecycle\n      summary: Force load model\n      description: Explicitly loads model into memory (warmup)\n      operationId: loadModel\n      parameters:\n        - name: model_id\n          in: path\n          required: true\n          schema:\n            type: string\n      responses:\n        '200':\n          description: Model loaded\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/ModelStatusResponse'\n        '404':\n          description: Model not found\n        '507':\n          description: Insufficient memory\n\ncomponents:\n  schemas:\n    ModelStatusResponse:\n      type: object\n      properties:\n        model_id:\n          type: string\n        state:\n          type: string\n          enum: [unloaded, loading, loaded, unhealthy]\n        memory_usage_mb:\n          type: number\n        device:\n          type: string\n          enum: [cpu, cuda]\n        loaded_at:\n          type: string\n          format: date-time\n        last_used_at:\n          type: string\n          format: date-time\n        health_status:\n          type: object\n          properties:\n            healthy:\n              type: boolean\n            last_check_at:\n              type: string\n              format: date-time\n            error_message:\n              type: string\n</code></pre>"},{"location":"api_design/#262-unload-model","title":"2.6.2 Unload Model","text":"<pre><code>paths:\n  /models/{model_id}/unload:\n    post:\n      tags:\n        - Lifecycle\n      summary: Unload model from memory\n      operationId: unloadModel\n      parameters:\n        - name: model_id\n          in: path\n          required: true\n          schema:\n            type: string\n      responses:\n        '200':\n          description: Model unloaded\n        '404':\n          description: Model not found or not loaded\n</code></pre>"},{"location":"api_design/#263-system-status","title":"2.6.3 System Status","text":"<pre><code>paths:\n  /status:\n    get:\n      tags:\n        - Lifecycle\n      summary: Get system status\n      description: Returns overall system status and loaded models\n      operationId: getStatus\n      responses:\n        '200':\n          description: System status\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  version:\n                    type: string\n                    example: \"1.0.0\"\n                  uptime_seconds:\n                    type: integer\n                  memory:\n                    type: object\n                    properties:\n                      total_mb:\n                        type: number\n                      used_mb:\n                        type: number\n                      available_mb:\n                        type: number\n                      threshold_mb:\n                        type: number\n                  gpu:\n                    type: object\n                    properties:\n                      available:\n                        type: boolean\n                      devices:\n                        type: array\n                        items:\n                          type: object\n                          properties:\n                            device_id:\n                              type: integer\n                            name:\n                              type: string\n                            memory_total_mb:\n                              type: number\n                            memory_used_mb:\n                              type: number\n                  loaded_models:\n                    type: array\n                    items:\n                      $ref: '#/components/schemas/ModelStatusResponse'\n                  queue_depth:\n                    type: object\n                    properties:\n                      high_priority:\n                        type: integer\n                      medium_priority:\n                        type: integer\n                      low_priority:\n                        type: integer\n</code></pre>"},{"location":"api_design/#27-health-check-api","title":"2.7 Health Check API","text":"<pre><code>paths:\n  /health/live:\n    get:\n      tags:\n        - Health\n      summary: Liveness probe\n      description: Kubernetes liveness probe endpoint\n      operationId: liveness\n      responses:\n        '200':\n          description: Service is alive\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  status:\n                    type: string\n                    example: \"ok\"\n\n  /health/ready:\n    get:\n      tags:\n        - Health\n      summary: Readiness probe\n      description: Kubernetes readiness probe endpoint\n      operationId: readiness\n      responses:\n        '200':\n          description: Service is ready\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  status:\n                    type: string\n                    example: \"ready\"\n                  checks:\n                    type: object\n                    properties:\n                      database:\n                        type: boolean\n                      queue:\n                        type: boolean\n        '503':\n          description: Service not ready\n\n  /health/models:\n    get:\n      tags:\n        - Health\n      summary: Model health status\n      description: Health status of all loaded models\n      operationId: modelHealth\n      responses:\n        '200':\n          description: Model health status\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  healthy_count:\n                    type: integer\n                  unhealthy_count:\n                    type: integer\n                  models:\n                    type: array\n                    items:\n                      type: object\n                      properties:\n                        model_id:\n                          type: string\n                        healthy:\n                          type: boolean\n                        last_check_at:\n                          type: string\n                          format: date-time\n                        error:\n                          type: string\n</code></pre>"},{"location":"api_design/#28-metrics-api","title":"2.8 Metrics API","text":"<pre><code>paths:\n  /metrics:\n    get:\n      tags:\n        - Metrics\n      summary: Prometheus metrics\n      description: Exposes metrics in Prometheus format\n      operationId: metrics\n      responses:\n        '200':\n          description: Prometheus metrics\n          content:\n            text/plain:\n              schema:\n                type: string\n              example: |\n                # HELP modelmora_requests_total Total inference requests\n                # TYPE modelmora_requests_total counter\n                modelmora_requests_total{model_id=\"all-MiniLM-L6-v2\",status=\"success\"} 1234\n\n                # HELP modelmora_inference_duration_seconds Inference duration\n                # TYPE modelmora_inference_duration_seconds histogram\n                modelmora_inference_duration_seconds_bucket{model_id=\"all-MiniLM-L6-v2\",le=\"0.1\"} 500\n                modelmora_inference_duration_seconds_bucket{model_id=\"all-MiniLM-L6-v2\",le=\"0.5\"} 800\n                modelmora_inference_duration_seconds_sum{model_id=\"all-MiniLM-L6-v2\"} 245.3\n\n                # HELP modelmora_queue_depth Current queue depth\n                # TYPE modelmora_queue_depth gauge\n                modelmora_queue_depth{priority=\"high\"} 2\n                modelmora_queue_depth{priority=\"medium\"} 5\n                modelmora_queue_depth{priority=\"low\"} 12\n</code></pre>"},{"location":"api_design/#3-grpc-service-specification-protocol-buffers","title":"3. gRPC Service Specification (Protocol Buffers)","text":""},{"location":"api_design/#31-package-structure","title":"3.1 Package Structure","text":"<pre><code>protos/\n\u251c\u2500\u2500 modelmora/\n\u2502   \u2514\u2500\u2500 v1/\n\u2502       \u251c\u2500\u2500 common.proto\n\u2502       \u251c\u2500\u2500 inference.proto\n\u2502       \u251c\u2500\u2500 registry.proto\n\u2502       \u2514\u2500\u2500 management.proto\n</code></pre>"},{"location":"api_design/#32-common-types-commonproto","title":"3.2 Common Types (<code>common.proto</code>)","text":"<pre><code>syntax = \"proto3\";\n\npackage modelmora.v1;\n\noption go_package = \"github.com/miraveja/modelmora/gen/go/v1\";\n\n// Task types supported by ModelMora\nenum TaskType {\n  TASK_TYPE_UNSPECIFIED = 0;\n  TASK_TYPE_TXT2EMBED = 1;\n  TASK_TYPE_IMG2EMBED = 2;\n  TASK_TYPE_TXT2IMG = 3;\n  TASK_TYPE_IMG2TXT = 4;\n  TASK_TYPE_TXT2TXT = 5;\n}\n\n// Priority levels for inference requests\nenum Priority {\n  PRIORITY_UNSPECIFIED = 0;\n  PRIORITY_LOW = 1;\n  PRIORITY_MEDIUM = 2;\n  PRIORITY_HIGH = 3;\n}\n\n// Job status for async operations\nenum JobStatus {\n  JOB_STATUS_UNSPECIFIED = 0;\n  JOB_STATUS_QUEUED = 1;\n  JOB_STATUS_PROCESSING = 2;\n  JOB_STATUS_COMPLETED = 3;\n  JOB_STATUS_FAILED = 4;\n  JOB_STATUS_CANCELLED = 5;\n}\n\n// Model state in lifecycle\nenum ModelState {\n  MODEL_STATE_UNSPECIFIED = 0;\n  MODEL_STATE_UNLOADED = 1;\n  MODEL_STATE_LOADING = 2;\n  MODEL_STATE_LOADED = 3;\n  MODEL_STATE_UNHEALTHY = 4;\n}\n\n// Resource requirements for a model\nmessage ResourceRequirements {\n  int32 memory_mb = 1;\n  int32 gpu_vram_mb = 2;\n  int32 cpu_threads = 3;\n}\n\n// Error details with code and message\nmessage Error {\n  string code = 1;\n  string message = 2;\n  map&lt;string, string&gt; details = 3;\n  string trace_id = 4;\n}\n\n// Pagination metadata\nmessage PaginationRequest {\n  int32 page = 1;\n  int32 page_size = 2;\n}\n\nmessage PaginationResponse {\n  int32 total = 1;\n  int32 page = 2;\n  int32 page_size = 3;\n  int32 total_pages = 4;\n}\n</code></pre>"},{"location":"api_design/#33-inference-service-inferenceproto","title":"3.3 Inference Service (<code>inference.proto</code>)","text":"<pre><code>syntax = \"proto3\";\n\npackage modelmora.v1;\n\nimport \"modelmora/v1/common.proto\";\nimport \"google/protobuf/struct.proto\";\nimport \"google/protobuf/timestamp.proto\";\n\noption go_package = \"github.com/miraveja/modelmora/gen/go/v1\";\n\n// InferenceService handles model inference operations\nservice InferenceService {\n  // Execute synchronous inference\n  rpc Infer(InferenceRequest) returns (InferenceResponse);\n\n  // Execute asynchronous inference (returns job ID)\n  rpc InferAsync(InferenceRequest) returns (JobReference);\n\n  // Stream inference (for real-time use cases)\n  rpc InferStream(stream InferenceRequest) returns (stream InferenceResponse);\n\n  // Get job status\n  rpc GetJobStatus(GetJobStatusRequest) returns (JobStatusResponse);\n\n  // Get job result\n  rpc GetJobResult(GetJobResultRequest) returns (InferenceResponse);\n\n  // Cancel job\n  rpc CancelJob(CancelJobRequest) returns (CancelJobResponse);\n}\n\n// Inference request message\nmessage InferenceRequest {\n  // Model identifier\n  string model_id = 1;\n\n  // Input data (task-specific)\n  InputData input_data = 2;\n\n  // Inference parameters (task-specific)\n  google.protobuf.Struct parameters = 3;\n\n  // Request priority\n  Priority priority = 4;\n\n  // Timeout in seconds\n  int32 timeout_seconds = 5;\n\n  // Client request ID (for tracing)\n  string request_id = 6;\n}\n\n// Input data structure\nmessage InputData {\n  oneof data {\n    string text = 1;\n    TextBatch text_batch = 2;\n    bytes image = 3;\n    ImageBatch image_batch = 4;\n  }\n\n  // Additional custom fields\n  google.protobuf.Struct custom_data = 10;\n}\n\nmessage TextBatch {\n  repeated string texts = 1;\n}\n\nmessage ImageBatch {\n  repeated bytes images = 1;\n}\n\n// Inference response message\nmessage InferenceResponse {\n  // Model identifier\n  string model_id = 1;\n\n  // Output data (task-specific)\n  OutputData output_data = 2;\n\n  // Inference metadata\n  InferenceMetadata metadata = 3;\n\n  // Error (if failed)\n  Error error = 4;\n}\n\n// Output data structure\nmessage OutputData {\n  oneof data {\n    Embedding embedding = 1;\n    EmbeddingBatch embedding_batch = 2;\n    string text = 3;\n    string image_url = 4;\n    bytes image_data = 5;\n  }\n\n  // Additional custom fields\n  google.protobuf.Struct custom_data = 10;\n}\n\nmessage Embedding {\n  repeated float values = 1;\n}\n\nmessage EmbeddingBatch {\n  repeated Embedding embeddings = 1;\n}\n\n// Inference execution metadata\nmessage InferenceMetadata {\n  double inference_time_ms = 1;\n  double total_time_ms = 2;\n  int32 batch_size = 3;\n  string device = 4;\n  google.protobuf.Timestamp timestamp = 5;\n}\n\n// Job reference for async operations\nmessage JobReference {\n  string job_id = 1;\n  JobStatus status = 2;\n  google.protobuf.Timestamp created_at = 3;\n}\n\n// Get job status request\nmessage GetJobStatusRequest {\n  string job_id = 1;\n}\n\n// Job status response\nmessage JobStatusResponse {\n  string job_id = 1;\n  string model_id = 2;\n  JobStatus status = 3;\n  float progress = 4;\n  google.protobuf.Timestamp created_at = 5;\n  google.protobuf.Timestamp started_at = 6;\n  google.protobuf.Timestamp completed_at = 7;\n  Error error = 8;\n}\n\n// Get job result request\nmessage GetJobResultRequest {\n  string job_id = 1;\n}\n\n// Cancel job request\nmessage CancelJobRequest {\n  string job_id = 1;\n}\n\n// Cancel job response\nmessage CancelJobResponse {\n  string job_id = 1;\n  JobStatus status = 2;\n}\n</code></pre>"},{"location":"api_design/#34-registry-service-registryproto","title":"3.4 Registry Service (<code>registry.proto</code>)","text":"<pre><code>syntax = \"proto3\";\n\npackage modelmora.v1;\n\nimport \"modelmora/v1/common.proto\";\nimport \"google/protobuf/struct.proto\";\nimport \"google/protobuf/timestamp.proto\";\n\noption go_package = \"github.com/miraveja/modelmora/gen/go/v1\";\n\n// RegistryService manages model catalog\nservice RegistryService {\n  // Register a new model\n  rpc RegisterModel(RegisterModelRequest) returns (Model);\n\n  // List all models\n  rpc ListModels(ListModelsRequest) returns (ListModelsResponse);\n\n  // Get model details\n  rpc GetModel(GetModelRequest) returns (Model);\n\n  // Update model configuration\n  rpc UpdateModel(UpdateModelRequest) returns (Model);\n\n  // Delete model\n  rpc DeleteModel(DeleteModelRequest) returns (DeleteModelResponse);\n\n  // Download model files\n  rpc DownloadModel(DownloadModelRequest) returns (DownloadModelResponse);\n\n  // Generate lock file\n  rpc GenerateLock(GenerateLockRequest) returns (LockFile);\n}\n\n// Register model request\nmessage RegisterModelRequest {\n  string model_id = 1;\n  string version = 2;\n  TaskType task_type = 3;\n  string display_name = 4;\n  string description = 5;\n  string source_url = 6;\n  ResourceRequirements resource_requirements = 7;\n  google.protobuf.Struct config = 8;\n}\n\n// Model message\nmessage Model {\n  string model_id = 1;\n  string version = 2;\n  TaskType task_type = 3;\n  string display_name = 4;\n  string description = 5;\n  string status = 6;\n  ResourceRequirements resource_requirements = 7;\n  google.protobuf.Struct config = 8;\n  google.protobuf.Timestamp created_at = 9;\n  google.protobuf.Timestamp updated_at = 10;\n}\n\n// List models request\nmessage ListModelsRequest {\n  TaskType task_type = 1;\n  string status = 2;\n  PaginationRequest pagination = 3;\n}\n\n// List models response\nmessage ListModelsResponse {\n  repeated Model models = 1;\n  PaginationResponse pagination = 2;\n}\n\n// Get model request\nmessage GetModelRequest {\n  string model_id = 1;\n}\n\n// Update model request\nmessage UpdateModelRequest {\n  string model_id = 1;\n  string display_name = 2;\n  string description = 3;\n  google.protobuf.Struct config = 4;\n  ResourceRequirements resource_requirements = 5;\n}\n\n// Delete model request\nmessage DeleteModelRequest {\n  string model_id = 1;\n  bool force = 2;\n}\n\n// Delete model response\nmessage DeleteModelResponse {\n  string model_id = 1;\n  bool deleted = 2;\n}\n\n// Download model request\nmessage DownloadModelRequest {\n  string model_id = 1;\n}\n\n// Download model response\nmessage DownloadModelResponse {\n  string model_id = 1;\n  string status = 2;\n  string message = 3;\n}\n\n// Generate lock request\nmessage GenerateLockRequest {}\n\n// Lock file message\nmessage LockFile {\n  string version = 1;\n  google.protobuf.Timestamp generated_at = 2;\n  repeated LockedModel models = 3;\n}\n\nmessage LockedModel {\n  string model_id = 1;\n  string version = 2;\n  string source_url = 3;\n  string checksum = 4;\n  TaskType task_type = 5;\n}\n</code></pre>"},{"location":"api_design/#35-management-service-managementproto","title":"3.5 Management Service (<code>management.proto</code>)","text":"<pre><code>syntax = \"proto3\";\n\npackage modelmora.v1;\n\nimport \"modelmora/v1/common.proto\";\nimport \"google/protobuf/timestamp.proto\";\n\noption go_package = \"github.com/miraveja/modelmora/gen/go/v1\";\n\n// ManagementService handles lifecycle and system operations\nservice ManagementService {\n  // Load model into memory\n  rpc LoadModel(LoadModelRequest) returns (ModelStatus);\n\n  // Unload model from memory\n  rpc UnloadModel(UnloadModelRequest) returns (UnloadModelResponse);\n\n  // Get system status\n  rpc GetSystemStatus(GetSystemStatusRequest) returns (SystemStatus);\n\n  // Get model health status\n  rpc GetModelHealth(GetModelHealthRequest) returns (ModelHealthResponse);\n\n  // Health check endpoints\n  rpc LivenessCheck(LivenessRequest) returns (LivenessResponse);\n  rpc ReadinessCheck(ReadinessRequest) returns (ReadinessResponse);\n}\n\n// Load model request\nmessage LoadModelRequest {\n  string model_id = 1;\n}\n\n// Model status message\nmessage ModelStatus {\n  string model_id = 1;\n  ModelState state = 2;\n  double memory_usage_mb = 3;\n  string device = 4;\n  google.protobuf.Timestamp loaded_at = 5;\n  google.protobuf.Timestamp last_used_at = 6;\n  HealthStatus health_status = 7;\n}\n\nmessage HealthStatus {\n  bool healthy = 1;\n  google.protobuf.Timestamp last_check_at = 2;\n  string error_message = 3;\n}\n\n// Unload model request\nmessage UnloadModelRequest {\n  string model_id = 1;\n}\n\n// Unload model response\nmessage UnloadModelResponse {\n  string model_id = 1;\n  bool unloaded = 2;\n}\n\n// Get system status request\nmessage GetSystemStatusRequest {}\n\n// System status response\nmessage SystemStatus {\n  string version = 1;\n  int64 uptime_seconds = 2;\n  MemoryStatus memory = 3;\n  GPUStatus gpu = 4;\n  repeated ModelStatus loaded_models = 5;\n  QueueStatus queue = 6;\n}\n\nmessage MemoryStatus {\n  double total_mb = 1;\n  double used_mb = 2;\n  double available_mb = 3;\n  double threshold_mb = 4;\n}\n\nmessage GPUStatus {\n  bool available = 1;\n  repeated GPUDevice devices = 2;\n}\n\nmessage GPUDevice {\n  int32 device_id = 1;\n  string name = 2;\n  double memory_total_mb = 3;\n  double memory_used_mb = 4;\n}\n\nmessage QueueStatus {\n  int32 high_priority = 1;\n  int32 medium_priority = 2;\n  int32 low_priority = 3;\n}\n\n// Get model health request\nmessage GetModelHealthRequest {}\n\n// Model health response\nmessage ModelHealthResponse {\n  int32 healthy_count = 1;\n  int32 unhealthy_count = 2;\n  repeated ModelStatus models = 3;\n}\n\n// Liveness check\nmessage LivenessRequest {}\n\nmessage LivenessResponse {\n  string status = 1;\n}\n\n// Readiness check\nmessage ReadinessRequest {}\n\nmessage ReadinessResponse {\n  string status = 1;\n  HealthChecks checks = 2;\n}\n\nmessage HealthChecks {\n  bool database = 1;\n  bool queue = 2;\n}\n</code></pre>"},{"location":"api_design/#4-kafka-message-schemas-future","title":"4. Kafka Message Schemas (Future)","text":""},{"location":"api_design/#41-topic-structure","title":"4.1 Topic Structure","text":"<pre><code>Topics:\n- modelmora.inference.requests    # Inference request submissions\n- modelmora.inference.responses   # Inference results\n- modelmora.lifecycle.events      # Model load/unload events\n- modelmora.audit.events          # Audit trail\n</code></pre>"},{"location":"api_design/#42-message-format-json-schema","title":"4.2 Message Format (JSON Schema)","text":""},{"location":"api_design/#421-inference-request-message","title":"4.2.1 Inference Request Message","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"InferenceRequestMessage\",\n  \"type\": \"object\",\n  \"required\": [\"message_id\", \"model_id\", \"input_data\", \"timestamp\"],\n  \"properties\": {\n    \"message_id\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\",\n      \"description\": \"Unique message identifier\"\n    },\n    \"correlation_id\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\",\n      \"description\": \"Client correlation ID for response matching\"\n    },\n    \"model_id\": {\n      \"type\": \"string\"\n    },\n    \"input_data\": {\n      \"type\": \"object\"\n    },\n    \"parameters\": {\n      \"type\": \"object\"\n    },\n    \"priority\": {\n      \"type\": \"string\",\n      \"enum\": [\"high\", \"medium\", \"low\"]\n    },\n    \"timeout_seconds\": {\n      \"type\": \"integer\"\n    },\n    \"timestamp\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\"\n    }\n  }\n}\n</code></pre>"},{"location":"api_design/#422-inference-response-message","title":"4.2.2 Inference Response Message","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"InferenceResponseMessage\",\n  \"type\": \"object\",\n  \"required\": [\"message_id\", \"correlation_id\", \"status\", \"timestamp\"],\n  \"properties\": {\n    \"message_id\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\"\n    },\n    \"correlation_id\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\",\n      \"description\": \"Matches request correlation_id\"\n    },\n    \"model_id\": {\n      \"type\": \"string\"\n    },\n    \"status\": {\n      \"type\": \"string\",\n      \"enum\": [\"success\", \"failure\"]\n    },\n    \"output_data\": {\n      \"type\": \"object\"\n    },\n    \"metadata\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"inference_time_ms\": {\"type\": \"number\"},\n        \"device\": {\"type\": \"string\"}\n      }\n    },\n    \"error\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"code\": {\"type\": \"string\"},\n        \"message\": {\"type\": \"string\"}\n      }\n    },\n    \"timestamp\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\"\n    }\n  }\n}\n</code></pre>"},{"location":"api_design/#423-lifecycle-event-message","title":"4.2.3 Lifecycle Event Message","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"title\": \"LifecycleEventMessage\",\n  \"type\": \"object\",\n  \"required\": [\"event_id\", \"event_type\", \"model_id\", \"timestamp\"],\n  \"properties\": {\n    \"event_id\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\"\n    },\n    \"event_type\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"model_load_requested\",\n        \"model_loaded\",\n        \"model_load_failed\",\n        \"model_unloaded\",\n        \"model_evicted\"\n      ]\n    },\n    \"model_id\": {\n      \"type\": \"string\"\n    },\n    \"details\": {\n      \"type\": \"object\"\n    },\n    \"timestamp\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\"\n    }\n  }\n}\n</code></pre>"},{"location":"api_design/#5-error-handling","title":"5. Error Handling","text":""},{"location":"api_design/#51-http-status-codes","title":"5.1 HTTP Status Codes","text":"Code Description Usage 200 OK Successful GET/PUT/DELETE 201 Created Successful POST (resource created) 202 Accepted Async operation started 204 No Content Successful DELETE (no body) 400 Bad Request Invalid input/validation error 401 Unauthorized Missing/invalid authentication 403 Forbidden Insufficient permissions 404 Not Found Resource doesn't exist 408 Request Timeout Request exceeded timeout 409 Conflict Resource conflict (e.g., already exists) 422 Unprocessable Entity Semantic validation error 429 Too Many Requests Rate limit exceeded 500 Internal Server Error Unexpected server error 503 Service Unavailable Service temporarily unavailable 507 Insufficient Storage Not enough memory/disk"},{"location":"api_design/#52-grpc-status-codes","title":"5.2 gRPC Status Codes","text":"Code Description Usage OK Success - CANCELLED Request cancelled Client cancelled request INVALID_ARGUMENT Invalid input Validation error DEADLINE_EXCEEDED Timeout Request timeout NOT_FOUND Not found Resource doesn't exist ALREADY_EXISTS Already exists Duplicate resource RESOURCE_EXHAUSTED Resource exhausted Memory/quota limits FAILED_PRECONDITION Precondition failed Invalid state UNAVAILABLE Service unavailable Temporary failure INTERNAL Internal error Unexpected error"},{"location":"api_design/#53-error-codes","title":"5.3 Error Codes","text":"<pre><code># Registry Errors\nMODEL_NOT_FOUND\nMODEL_ALREADY_EXISTS\nINVALID_MODEL_ID\nINVALID_TASK_TYPE\nDOWNLOAD_FAILED\nCHECKSUM_MISMATCH\n\n# Lifecycle Errors\nMODEL_LOAD_FAILED\nMODEL_NOT_LOADED\nINSUFFICIENT_MEMORY\nUNHEALTHY_MODEL\nPROCESS_SPAWN_FAILED\n\n# Inference Errors\nINVALID_INPUT\nINFERENCE_FAILED\nINFERENCE_TIMEOUT\nJOB_NOT_FOUND\nJOB_NOT_READY\nBATCH_SIZE_EXCEEDED\n\n# System Errors\nVALIDATION_ERROR\nINTERNAL_ERROR\nSERVICE_UNAVAILABLE\nRATE_LIMIT_EXCEEDED\n</code></pre>"},{"location":"api_design/#6-client-sdk-examples","title":"6. Client SDK Examples","text":""},{"location":"api_design/#61-python-rest-client","title":"6.1 Python REST Client","text":"<pre><code>from modelmora_client import ModelMoraClient\n\n# Initialize client\nclient = ModelMoraClient(base_url=\"http://localhost:8000\")\n\n# Register model\nmodel = client.registry.register_model(\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    task_type=\"txt2embed\",\n    version=\"v2.2.2\"\n)\n\n# Synchronous inference\nresult = client.inference.infer(\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    input_data={\"text\": \"Hello world\"},\n    parameters={\"normalize\": True}\n)\nprint(result.output_data.embedding)\n\n# Asynchronous inference\njob = client.inference.infer_async(\n    model_id=\"stabilityai/stable-diffusion-2-1\",\n    input_data={\"text\": \"A beautiful sunset\"},\n    priority=\"low\"\n)\nprint(f\"Job ID: {job.job_id}\")\n\n# Poll for result\nstatus = client.jobs.get_status(job.job_id)\nwhile status.status in [\"queued\", \"processing\"]:\n    time.sleep(1)\n    status = client.jobs.get_status(job.job_id)\n\nresult = client.jobs.get_result(job.job_id)\nprint(f\"Image URL: {result.output_data.image_url}\")\n</code></pre>"},{"location":"api_design/#62-python-grpc-client","title":"6.2 Python gRPC Client","text":"<pre><code>import grpc\nfrom modelmora.v1 import inference_pb2, inference_pb2_grpc\n\n# Create channel\nchannel = grpc.insecure_channel('localhost:50051')\nstub = inference_pb2_grpc.InferenceServiceStub(channel)\n\n# Create request\nrequest = inference_pb2.InferenceRequest(\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    input_data=inference_pb2.InputData(text=\"Hello world\"),\n    priority=inference_pb2.PRIORITY_HIGH\n)\n\n# Execute inference\nresponse = stub.Infer(request)\nprint(response.output_data.embedding.values)\n</code></pre>"},{"location":"api_design/#7-revision-history","title":"7. Revision History","text":"Version Date Author Changes 1.0.0 2025-12-04 - Initial API design document"},{"location":"architecture/","title":"ModelMora - System Architecture Document","text":"<p>Version: 1.0.0 Date: 2025-12-04 Status: Draft</p>"},{"location":"architecture/#1-introduction","title":"1. Introduction","text":""},{"location":"architecture/#11-purpose","title":"1.1 Purpose","text":"<p>This document describes the system architecture for ModelMora, a lightweight model serving framework. It defines the bounded contexts, architectural patterns, component interactions, and deployment strategy.</p>"},{"location":"architecture/#12-architectural-principles","title":"1.2 Architectural Principles","text":"<p>ModelMora follows these core principles:</p> <ul> <li>Domain-Driven Design (DDD): Clear bounded contexts with ubiquitous language</li> <li>Hexagonal Architecture (Ports &amp; Adapters): Separation of business logic from infrastructure</li> <li>Test-Driven Development (TDD): Test-first approach with high coverage</li> <li>Single Responsibility: Each component has one clear purpose</li> <li>Dependency Inversion: High-level modules don't depend on low-level modules</li> <li>SOLID Principles: Maintainable, extensible codebase</li> </ul>"},{"location":"architecture/#13-architecture-overview","title":"1.3 Architecture Overview","text":"<p>ModelMora uses a layered hexagonal architecture with the following characteristics:</p> <ul> <li>Domain Layer: Pure business logic, no external dependencies</li> <li>Application Layer: Command handlers and orchestration</li> <li>Infrastructure Layer: External integrations (database, filesystem, HTTP)</li> <li>Presentation Layer: API interfaces (REST, gRPC, CLI)</li> </ul>"},{"location":"architecture/#14-poc-validated-architecture-decisions","title":"1.4 POC-Validated Architecture Decisions","text":"<p>The following critical architecture decisions were validated through production-level POCs (Phase 0.3):</p> <p>1. Multi-Process Worker Architecture \u2705 (POC 1: Memory Isolation)</p> <ul> <li>Decision: One model per subprocess, terminate process to unload</li> <li>Validation: Subprocess achieves 5000x better memory reclamation (0.1MB leak vs 516MB with GC)</li> <li>Impact: Ensures reliable memory cleanup, prevents gradual memory exhaustion</li> <li>Implementation: <code>multiprocessing.Process</code> per model, IPC via queues/pipes</li> </ul> <p>2. Lazy Loading Strategy \u2705 (POC 3: Model Lifecycle)</p> <ul> <li>Decision: Load models on first request, not at startup</li> <li>Validation: Load time 2s for small-medium models (acceptable latency)</li> <li>Impact: Reduces startup time, optimizes memory usage</li> <li>Fallback: Warmup endpoint for pre-loading critical models</li> </ul> <p>3. asyncio.PriorityQueue for Request Scheduling \u2705 (POC 4: Priority Queue)</p> <ul> <li>Decision: Use <code>asyncio.PriorityQueue</code> with 4 priority levels</li> <li>Validation: 730,108 ops/sec throughput, 0.7\u03bcs latency, 100% priority correctness</li> <li>Impact: 730x headroom above target (1,000 req/sec), negligible overhead</li> <li>Priority Levels: CRITICAL(1), HIGH(2), NORMAL(3), LOW(4)</li> </ul> <p>4. gRPC for Streaming Large Payloads \u2705 (POC 2: gRPC Streaming)</p> <ul> <li>Decision: gRPC for image generation, REST for simple queries</li> <li>Validation: 31.92 MB/s single client, 33.31 MB/s concurrent, 23.53ms latency</li> <li>Impact: Efficient streaming, acceptable performance for large payloads</li> <li>Protocol: Protocol Buffers with server-side streaming</li> </ul> <p>Performance Benchmarks:</p> <ul> <li>Model load time: 2s (cached)</li> <li>Queue latency: &lt;1\u03bcs (0.0002% of total request time)</li> <li>Memory per model: 90-500MB (plan for 1.5x = 135-750MB per worker)</li> <li>Memory leak: 0.1MB/unload (subprocess) vs 516MB/unload (GC) \u2192 subprocess mandatory</li> </ul>"},{"location":"architecture/#2-bounded-contexts","title":"2. Bounded Contexts","text":"<p>Following DDD principles, ModelMora is organized into four bounded contexts:</p>"},{"location":"architecture/#21-registry-context","title":"2.1 Registry Context","text":"<p>Responsibility: Model catalog management, versioning, and metadata storage.</p> <p>Core Entities:</p> <ul> <li><code>Model</code>: Represents a machine learning model with metadata</li> <li><code>ModelVersion</code>: Specific version of a model</li> <li><code>ModelLock</code>: Lock file entry for reproducible deployments</li> <li><code>ModelConfig</code>: Curated configuration for model capabilities</li> </ul> <p>Value Objects:</p> <ul> <li><code>ModelId</code>: HuggingFace identifier</li> <li><code>TaskType</code>: Enumeration of supported tasks</li> <li><code>ResourceRequirements</code>: Memory and compute specifications</li> <li><code>Checksum</code>: SHA256 hash for integrity</li> </ul> <p>Aggregates:</p> <ul> <li><code>ModelCatalog</code>: Root aggregate managing model collection</li> </ul> <p>Domain Events:</p> <ul> <li><code>ModelRegistered</code></li> <li><code>ModelVersionChanged</code></li> <li><code>ModelDeleted</code></li> <li><code>ModelDownloadRequested</code></li> </ul> <p>Repositories:</p> <ul> <li><code>IModelRepository</code>: Interface for model persistence</li> <li><code>ILockFileRepository</code>: Interface for lock file operations</li> </ul> <p>Handlers:</p> <ul> <li><code>RegisterModelHandler</code>: Handles model registration</li> <li><code>ListModelsHandler</code>: Handles model listing with filters</li> <li><code>GetModelHandler</code>: Handles retrieving model details</li> <li><code>UpdateModelHandler</code>: Handles model configuration updates</li> <li><code>DeleteModelHandler</code>: Handles model deletion</li> <li><code>GenerateLockHandler</code>: Handles lock file generation</li> <li><code>DownloadModelHandler</code>: Handles model downloading</li> </ul>"},{"location":"architecture/#22-lifecycle-context","title":"2.2 Lifecycle Context","text":"<p>Responsibility: Model loading, unloading, memory management, and health monitoring.</p> <p>Core Entities:</p> <ul> <li><code>LoadedModel</code>: A model instance in memory</li> <li><code>ModelProcess</code>: Worker process managing model execution</li> <li><code>MemoryMonitor</code>: Tracks memory usage</li> </ul> <p>Value Objects:</p> <ul> <li><code>ModelState</code>: Enumeration (UNLOADED, LOADING, LOADED, UNHEALTHY)</li> <li><code>MemoryUsage</code>: Current memory metrics</li> <li><code>HealthStatus</code>: Health check results</li> </ul> <p>Aggregates:</p> <ul> <li><code>ModelLifecycleManager</code>: Root aggregate coordinating model lifecycle</li> </ul> <p>Domain Events:</p> <ul> <li><code>ModelLoadRequested</code></li> <li><code>ModelLoaded</code></li> <li><code>ModelLoadFailed</code></li> <li><code>ModelUnloaded</code></li> <li><code>ModelEvicted</code></li> <li><code>MemoryThresholdExceeded</code></li> </ul> <p>Policies:</p> <ul> <li><code>LRUEvictionPolicy</code>: Determines which model to unload</li> <li><code>WarmupPolicy</code>: Pre-loading strategy</li> </ul> <p>Services:</p> <ul> <li><code>ModelLoader</code>: Loads models into memory (validated: 2s load time for small-medium models)</li> <li><code>ProcessManager</code>: Manages worker processes (POC-validated: subprocess isolation required)</li> <li><code>HealthChecker</code>: Verifies model integrity</li> </ul> <p>Handlers:</p> <ul> <li><code>LoadModelHandler</code>: Handles model loading into memory</li> <li><code>UnloadModelHandler</code>: Handles model unloading from memory (POC-validated: must terminate subprocess, not GC)</li> <li><code>WarmupModelHandler</code>: Handles model warmup/preloading</li> <li><code>CheckHealthHandler</code>: Handles model health verification</li> <li><code>EvictModelHandler</code>: Handles LRU-based model eviction</li> </ul> <p>POC-Validated Memory Management Strategy:</p> <ol> <li>Process Isolation: Each model runs in dedicated subprocess</li> <li>Memory reclamation: 0.1MB leak (subprocess) vs 516MB leak (GC)</li> <li> <p>GPU cleanup: Subprocess termination ensures complete VRAM release</p> </li> <li> <p>LRU Eviction: When memory threshold exceeded:</p> </li> <li>Identify least recently used model</li> <li>Terminate subprocess (kills process, releases all memory)</li> <li> <p>Remove from loaded model registry</p> </li> <li> <p>Memory Budgeting: Reserve 1.5x model footprint per worker</p> </li> <li>Small model (90MB): Reserve 135MB</li> <li>Medium model (420MB): Reserve 630MB</li> <li>10 concurrent models: ~5GB RAM minimum</li> </ol>"},{"location":"architecture/#23-inference-context","title":"2.3 Inference Context","text":"<p>Responsibility: Request handling, batching, execution, and result management.</p> <p>Core Entities:</p> <ul> <li><code>InferenceRequest</code>: A request for model inference</li> <li><code>InferenceJob</code>: Async job tracking</li> <li><code>Batch</code>: Collection of requests for same model</li> <li><code>InferenceResult</code>: Output from model execution</li> </ul> <p>Value Objects:</p> <ul> <li><code>JobId</code>: Unique identifier for async requests</li> <li><code>JobStatus</code>: Enumeration (QUEUED, PROCESSING, COMPLETED, FAILED)</li> <li><code>Priority</code>: Request priority level</li> <li><code>InputData</code>: Task-specific input payload</li> <li><code>OutputData</code>: Task-specific output payload</li> </ul> <p>Aggregates:</p> <ul> <li><code>InferenceQueue</code>: Root aggregate managing request queue</li> </ul> <p>Domain Events:</p> <ul> <li><code>InferenceRequested</code></li> <li><code>RequestQueued</code></li> <li><code>BatchFormed</code></li> <li><code>InferenceStarted</code></li> <li><code>InferenceCompleted</code></li> <li><code>InferenceFailed</code></li> </ul> <p>Services:</p> <ul> <li><code>BatchAccumulator</code>: Groups requests into batches</li> <li><code>InferenceExecutor</code>: Executes model inference</li> <li><code>ResultStore</code>: Stores large results (images)</li> </ul> <p>Policies:</p> <ul> <li><code>PriorityPolicy</code>: Determines request ordering (POC-validated: 100% correctness, 4 priority levels)</li> <li><code>BatchingPolicy</code>: Configures batch parameters</li> <li><code>TimeoutPolicy</code>: Handles request timeouts</li> </ul> <p>Handlers:</p> <ul> <li><code>SubmitRequestHandler</code>: Handles inference request submission (POC-validated: 0.7\u03bcs queue latency)</li> <li><code>ExecuteBatchHandler</code>: Handles batch inference execution</li> <li><code>GetJobStatusHandler</code>: Handles job status retrieval</li> <li><code>GetJobResultHandler</code>: Handles job result retrieval</li> <li><code>CancelRequestHandler</code>: Handles request cancellation</li> </ul> <p>POC-Validated Queue Performance:</p> <ul> <li>Implementation: <code>asyncio.PriorityQueue</code> (async-native, stdlib)</li> <li>Throughput: 730,108 ops/sec (730x above 1,000 req/sec target)</li> <li>Latency: 0.7\u03bcs enqueue, 1.7\u03bcs dequeue (negligible)</li> <li>Priority Ordering: 100% correct under concurrent load</li> <li>Capacity: Massive headroom, queue will never be bottleneck</li> </ul> <p>Priority Levels (Validated):</p> <ol> <li>CRITICAL (1): SLA-bound, real-time inference</li> <li>HIGH (2): User-facing, interactive requests</li> <li>NORMAL (3): Batch processing, standard workload</li> <li>LOW (4): Background tasks, analytics</li> </ol> <p>FIFO within same priority (timestamp-based ordering).</p>"},{"location":"architecture/#24-observability-context","title":"2.4 Observability Context","text":"<p>Responsibility: Metrics collection, logging, tracing, and health reporting.</p> <p>Core Entities:</p> <ul> <li><code>Metric</code>: Time-series data point</li> <li><code>LogEntry</code>: Structured log message</li> <li><code>Trace</code>: Distributed tracing span</li> </ul> <p>Value Objects:</p> <ul> <li><code>MetricType</code>: Enumeration (COUNTER, GAUGE, HISTOGRAM)</li> <li><code>LogLevel</code>: Severity level</li> <li><code>TraceId</code>: Correlation identifier</li> </ul> <p>Services:</p> <ul> <li><code>MetricsCollector</code>: Gathers performance metrics</li> <li><code>Logger</code>: Structured logging</li> <li><code>Tracer</code>: Distributed tracing</li> </ul> <p>Handlers:</p> <ul> <li><code>RecordMetricHandler</code>: Handles metric recording</li> <li><code>LogEventHandler</code>: Handles event logging</li> <li><code>CreateTraceHandler</code>: Handles distributed trace creation</li> <li><code>ExportMetricsHandler</code>: Handles Prometheus metrics export</li> <li><code>HealthCheckHandler</code>: Handles health check requests</li> </ul>"},{"location":"architecture/#3-hexagonal-architecture","title":"3. Hexagonal Architecture","text":""},{"location":"architecture/#31-ports-interfaces","title":"3.1 Ports (Interfaces)","text":""},{"location":"architecture/#primary-ports-driving","title":"Primary Ports (Driving)","text":"<p>Application exposes these interfaces to the outside world:</p> <pre><code># REST API Port\nclass IModelAPI(Protocol):\n    def register_model(request: RegisterModelRequest) -&gt; ModelResponse\n    def list_models(filters: ModelFilters) -&gt; List[ModelResponse]\n    def infer(model_id: str, request: InferenceRequest) -&gt; InferenceResponse\n\n# gRPC Port\nclass IInferenceService(Protocol):\n    def Infer(request: InferenceRequest) -&gt; InferenceResponse\n    def InferAsync(request: InferenceRequest) -&gt; JobReference\n\n# CLI Port\nclass IModelMoraCLI(Protocol):\n    def install(model_id: str, version: Optional[str]) -&gt; None\n    def list_models(task_type: Optional[str]) -&gt; None\n</code></pre>"},{"location":"architecture/#secondary-ports-driven","title":"Secondary Ports (Driven)","text":"<p>Application depends on these interfaces (implemented by adapters):</p> <pre><code># Repository Ports\nclass IModelRepository(Protocol):\n    def save(model: Model) -&gt; None\n    def find_by_id(model_id: ModelId) -&gt; Optional[Model]\n    def find_all(filters: dict) -&gt; List[Model]\n    def delete(model_id: ModelId) -&gt; None\n\n# External Service Ports\nclass IModelDownloader(Protocol):\n    def download(model_id: str, version: str) -&gt; Path\n    def validate_checksum(path: Path, expected: str) -&gt; bool\n\nclass IObjectStorage(Protocol):\n    def store(data: bytes, key: str) -&gt; str\n    def get_presigned_url(key: str, expiration: int) -&gt; str\n\n# Infrastructure Ports\nclass IProcessManager(Protocol):\n    def spawn_worker(model_id: str) -&gt; ProcessHandle\n    def send_request(handle: ProcessHandle, data: dict) -&gt; dict\n    def terminate(handle: ProcessHandle) -&gt; None\n\nclass IMetricsExporter(Protocol):\n    def record_counter(name: str, value: float, labels: dict) -&gt; None\n    def record_histogram(name: str, value: float, labels: dict) -&gt; None\n</code></pre>"},{"location":"architecture/#32-adapters","title":"3.2 Adapters","text":""},{"location":"architecture/#primary-adapters-input","title":"Primary Adapters (Input)","text":"<ul> <li><code>FastAPIAdapter</code>: REST API implementation</li> <li><code>GRPCAdapter</code>: gRPC service implementation</li> <li><code>CLIAdapter</code>: Click-based command-line interface</li> </ul>"},{"location":"architecture/#secondary-adapters-output","title":"Secondary Adapters (Output)","text":"<ul> <li><code>PostgreSQLModelRepository</code>: SQL database for model metadata</li> <li><code>HuggingFaceDownloader</code>: Downloads from HF Hub</li> <li><code>S3ObjectStorage</code>: AWS S3/MinIO for result storage</li> <li><code>MultiprocessingManager</code>: Python multiprocessing for workers</li> <li><code>PrometheusExporter</code>: Metrics export for Prometheus</li> </ul>"},{"location":"architecture/#4-component-architecture","title":"4. Component Architecture","text":""},{"location":"architecture/#41-high-level-component-diagram","title":"4.1 High-Level Component Diagram","text":"<pre>52a83e410a6496d1f8943c9f576afdf7b8944aadc9a25865d5b650416e35930899fc09f6429e7599584c42f930fbe980da4265763240f216ef80776d4951033e</pre><pre>f536929853b5f8ffaaab046e3e7680921d55de58038b82403dab65da909ef55e09b6943238674513b7b2e40b503cb893282795bcdd58cfb6a66c2f669ac5001e</pre>"},{"location":"architecture/#42-directory-structure","title":"4.2 Directory Structure","text":"<pre><code>src/modelmora/\n\u251c\u2500\u2500 registry/                                   # Registry Bounded Context\n\u2502   \u251c\u2500\u2500 domain/                                 # Domain Layer\n\u2502   \u2502   \u251c\u2500\u2500 model.py                            # Model entity\n\u2502   \u2502   \u251c\u2500\u2500 model_version.py                    # ModelVersion entity\n\u2502   \u2502   \u251c\u2500\u2500 model_lock.py                       # ModelLock entity\n\u2502   \u2502   \u251c\u2500\u2500 model_config.py                     # ModelConfig entity\n\u2502   \u2502   \u251c\u2500\u2500 model_catalog.py                    # ModelCatalog aggregate\n\u2502   \u2502   \u251c\u2500\u2500 model_id.py                         # ModelId value object\n\u2502   \u2502   \u251c\u2500\u2500 task_type.py                        # TaskType value object\n\u2502   \u2502   \u251c\u2500\u2500 checksum.py                         # Checksum value object\n\u2502   \u2502   \u251c\u2500\u2500 resource_requirements.py            # ResourceRequirements value object\n\u2502   \u2502   \u251c\u2500\u2500 i_model_repository.py               # IModelRepository interface\n\u2502   \u2502   \u251c\u2500\u2500 i_lock_file_repository.py           # ILockFileRepository interface\n\u2502   \u2502   \u251c\u2500\u2500 events/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_registered.py             # ModelRegistered event\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_version_changed.py        # ModelVersionChanged event\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_deleted.py                # ModelDeleted event\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 model_download_requested.py     # ModelDownloadRequested event\n\u2502   \u2502   \u2514\u2500\u2500 exceptions/\n\u2502   \u2502       \u251c\u2500\u2500 model_not_found_exception.py    # ModelNotFoundException\n\u2502   \u2502       \u251c\u2500\u2500 model_already_exists_exception.py  # ModelAlreadyExistsException\n\u2502   \u2502       \u2514\u2500\u2500 invalid_model_exception.py      # InvalidModelException\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 application/                            # Application Layer\n\u2502   \u2502   \u251c\u2500\u2500 register_model/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 register_model_command.py       # RegisterModelCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 register_model_handler.py       # RegisterModelHandler\n\u2502   \u2502   \u251c\u2500\u2500 list_models/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 list_models_command.py          # ListModelsCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 list_models_handler.py          # ListModelsHandler\n\u2502   \u2502   \u251c\u2500\u2500 get_model/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 get_model_command.py            # GetModelCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 get_model_handler.py            # GetModelHandler\n\u2502   \u2502   \u251c\u2500\u2500 update_model/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 update_model_command.py         # UpdateModelCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 update_model_handler.py         # UpdateModelHandler\n\u2502   \u2502   \u251c\u2500\u2500 delete_model/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 delete_model_command.py         # DeleteModelCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 delete_model_handler.py         # DeleteModelHandler\n\u2502   \u2502   \u251c\u2500\u2500 download_model/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 download_model_command.py       # DownloadModelCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 download_model_handler.py       # DownloadModelHandler\n\u2502   \u2502   \u251c\u2500\u2500 generate_lock/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 generate_lock_command.py        # GenerateLockCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 generate_lock_handler.py        # GenerateLockHandler\n\u2502   \u2502   \u2514\u2500\u2500 dto/\n\u2502   \u2502       \u251c\u2500\u2500 model_response_dto.py           # ModelResponseDTO\n\u2502   \u2502       \u251c\u2500\u2500 model_list_dto.py               # ModelListDTO\n\u2502   \u2502       \u2514\u2500\u2500 lock_file_dto.py                # LockFileDTO\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 infrastructure/                         # Infrastructure Layer\n\u2502       \u251c\u2500\u2500 sql/\n\u2502       \u2502   \u251c\u2500\u2500 sql_model_repository.py         # PostgreSQL IModelRepository impl\n\u2502       \u2502   \u251c\u2500\u2500 sql_lock_file_repository.py     # PostgreSQL ILockFileRepository impl\n\u2502       \u2502   \u251c\u2500\u2500 models/\n\u2502       \u2502   \u2502   \u251c\u2500\u2500 model_orm.py                # SQLAlchemy Model ORM\n\u2502       \u2502   \u2502   \u2514\u2500\u2500 model_version_orm.py        # SQLAlchemy ModelVersion ORM\n\u2502       \u2502   \u2514\u2500\u2500 migrations/\n\u2502       \u251c\u2500\u2500 downloaders/\n\u2502       \u2502   \u251c\u2500\u2500 i_model_downloader.py           # IModelDownloader interface\n\u2502       \u2502   \u251c\u2500\u2500 huggingface_downloader.py       # HuggingFace Hub implementation\n\u2502       \u2502   \u2514\u2500\u2500 local_downloader.py             # Local filesystem implementation\n\u2502       \u2514\u2500\u2500 config/\n\u2502           \u251c\u2500\u2500 settings.py                     # Pydantic settings\n\u2502           \u2514\u2500\u2500 model_configs.yaml              # Curated metadata\n\u2502\n\u251c\u2500\u2500 lifecycle/                                  # Lifecycle Bounded Context\n\u2502   \u251c\u2500\u2500 domain/                                 # Domain Layer\n\u2502   \u2502   \u251c\u2500\u2500 loaded_model.py                     # LoadedModel entity\n\u2502   \u2502   \u251c\u2500\u2500 model_process.py                    # ModelProcess entity\n\u2502   \u2502   \u251c\u2500\u2500 memory_monitor.py                   # MemoryMonitor entity\n\u2502   \u2502   \u251c\u2500\u2500 model_lifecycle_manager.py          # ModelLifecycleManager aggregate\n\u2502   \u2502   \u251c\u2500\u2500 model_state.py                      # ModelState value object\n\u2502   \u2502   \u251c\u2500\u2500 memory_usage.py                     # MemoryUsage value object\n\u2502   \u2502   \u251c\u2500\u2500 health_status.py                    # HealthStatus value object\n\u2502   \u2502   \u251c\u2500\u2500 model_loader.py                     # ModelLoader service\n\u2502   \u2502   \u251c\u2500\u2500 health_checker.py                   # HealthChecker service\n\u2502   \u2502   \u251c\u2500\u2500 i_process_manager.py                # IProcessManager interface\n\u2502   \u2502   \u251c\u2500\u2500 policies/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 lru_eviction_policy.py          # LRUEvictionPolicy\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 warmup_policy.py                # WarmupPolicy\n\u2502   \u2502   \u251c\u2500\u2500 events/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_load_requested.py         # ModelLoadRequested event\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_loaded.py                 # ModelLoaded event\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_load_failed.py            # ModelLoadFailed event\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_unloaded.py               # ModelUnloaded event\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model_evicted.py                # ModelEvicted event\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 memory_threshold_exceeded.py    # MemoryThresholdExceeded event\n\u2502   \u2502   \u2514\u2500\u2500 exceptions/\n\u2502   \u2502       \u251c\u2500\u2500 model_load_failed_exception.py  # ModelLoadFailedException\n\u2502   \u2502       \u251c\u2500\u2500 insufficient_memory_exception.py  # InsufficientMemoryException\n\u2502   \u2502       \u2514\u2500\u2500 model_not_loaded_exception.py   # ModelNotLoadedException\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 application/                            # Application Layer\n\u2502   \u2502   \u251c\u2500\u2500 load_model/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 load_model_command.py           # LoadModelCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 load_model_handler.py           # LoadModelHandler\n\u2502   \u2502   \u251c\u2500\u2500 unload_model/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 unload_model_command.py         # UnloadModelCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 unload_model_handler.py         # UnloadModelHandler\n\u2502   \u2502   \u251c\u2500\u2500 warmup_model/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 warmup_model_command.py         # WarmupModelCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 warmup_model_handler.py         # WarmupModelHandler\n\u2502   \u2502   \u251c\u2500\u2500 check_health/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 check_health_command.py         # CheckHealthCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 check_health_handler.py         # CheckHealthHandler\n\u2502   \u2502   \u251c\u2500\u2500 evict_model/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 evict_model_command.py          # EvictModelCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 evict_model_handler.py          # EvictModelHandler\n\u2502   \u2502   \u2514\u2500\u2500 dto/\n\u2502   \u2502       \u251c\u2500\u2500 model_status_dto.py             # ModelStatusDTO\n\u2502   \u2502       \u2514\u2500\u2500 health_check_dto.py             # HealthCheckDTO\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 infrastructure/                         # Infrastructure Layer\n\u2502       \u251c\u2500\u2500 processing/\n\u2502       \u2502   \u251c\u2500\u2500 multiprocess_manager.py         # IProcessManager implementation\n\u2502       \u2502   \u251c\u2500\u2500 worker_pool.py                  # Worker pool management\n\u2502       \u2502   \u2514\u2500\u2500 process_handle.py               # Process handle wrapper\n\u2502       \u2514\u2500\u2500 monitoring/\n\u2502           \u2514\u2500\u2500 system_memory_monitor.py        # System memory monitoring\n\u2502\n\u251c\u2500\u2500 inference/                                  # Inference Bounded Context\n\u2502   \u251c\u2500\u2500 domain/                                 # Domain Layer\n\u2502   \u2502   \u251c\u2500\u2500 inference_request.py                # InferenceRequest entity\n\u2502   \u2502   \u251c\u2500\u2500 inference_job.py                    # InferenceJob entity\n\u2502   \u2502   \u251c\u2500\u2500 batch.py                            # Batch entity\n\u2502   \u2502   \u251c\u2500\u2500 inference_result.py                 # InferenceResult entity\n\u2502   \u2502   \u251c\u2500\u2500 inference_queue.py                  # InferenceQueue aggregate\n\u2502   \u2502   \u251c\u2500\u2500 job_id.py                           # JobId value object\n\u2502   \u2502   \u251c\u2500\u2500 job_status.py                       # JobStatus value object\n\u2502   \u2502   \u251c\u2500\u2500 priority.py                         # Priority value object\n\u2502   \u2502   \u251c\u2500\u2500 input_data.py                       # InputData value object\n\u2502   \u2502   \u251c\u2500\u2500 output_data.py                      # OutputData value object\n\u2502   \u2502   \u251c\u2500\u2500 batch_accumulator.py                # BatchAccumulator service\n\u2502   \u2502   \u251c\u2500\u2500 inference_executor.py               # InferenceExecutor service\n\u2502   \u2502   \u251c\u2500\u2500 i_result_store.py                   # IResultStore interface\n\u2502   \u2502   \u251c\u2500\u2500 policies/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 priority_policy.py              # PriorityPolicy\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 batching_policy.py              # BatchingPolicy\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 timeout_policy.py               # TimeoutPolicy\n\u2502   \u2502   \u251c\u2500\u2500 events/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 inference_requested.py          # InferenceRequested event\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 request_queued.py               # RequestQueued event\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 batch_formed.py                 # BatchFormed event\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 inference_started.py            # InferenceStarted event\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 inference_completed.py          # InferenceCompleted event\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 inference_failed.py             # InferenceFailed event\n\u2502   \u2502   \u2514\u2500\u2500 exceptions/\n\u2502   \u2502       \u251c\u2500\u2500 job_not_found_exception.py      # JobNotFoundException\n\u2502   \u2502       \u251c\u2500\u2500 inference_timeout_exception.py  # InferenceTimeoutException\n\u2502   \u2502       \u2514\u2500\u2500 invalid_input_exception.py      # InvalidInputException\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 application/                            # Application Layer\n\u2502   \u2502   \u251c\u2500\u2500 submit_request/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 submit_request_command.py       # SubmitRequestCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 submit_request_handler.py       # SubmitRequestHandler\n\u2502   \u2502   \u251c\u2500\u2500 execute_batch/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 execute_batch_command.py        # ExecuteBatchCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 execute_batch_handler.py        # ExecuteBatchHandler\n\u2502   \u2502   \u251c\u2500\u2500 get_job_status/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 get_job_status_command.py       # GetJobStatusCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 get_job_status_handler.py       # GetJobStatusHandler\n\u2502   \u2502   \u251c\u2500\u2500 get_job_result/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 get_job_result_command.py       # GetJobResultCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 get_job_result_handler.py       # GetJobResultHandler\n\u2502   \u2502   \u251c\u2500\u2500 cancel_request/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 cancel_request_command.py       # CancelRequestCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 cancel_request_handler.py       # CancelRequestHandler\n\u2502   \u2502   \u2514\u2500\u2500 dto/\n\u2502   \u2502       \u251c\u2500\u2500 inference_response_dto.py       # InferenceResponseDTO\n\u2502   \u2502       \u251c\u2500\u2500 job_reference_dto.py            # JobReferenceDTO\n\u2502   \u2502       \u2514\u2500\u2500 job_result_dto.py               # JobResultDTO\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 infrastructure/                         # Infrastructure Layer\n\u2502       \u251c\u2500\u2500 storage/\n\u2502       \u2502   \u251c\u2500\u2500 s3_result_store.py              # S3/MinIO IResultStore impl\n\u2502       \u2502   \u2514\u2500\u2500 filesystem_result_store.py      # Local filesystem IResultStore impl\n\u2502       \u2514\u2500\u2500 queue/\n\u2502           \u251c\u2500\u2500 memory_queue.py                 # In-memory queue (MVP)\n\u2502           \u2514\u2500\u2500 redis_queue.py                  # Redis queue (production)\n\u2502\n\u251c\u2500\u2500 observability/                              # Observability Bounded Context\n\u2502   \u251c\u2500\u2500 domain/                                 # Domain Layer\n\u2502   \u2502   \u251c\u2500\u2500 metric.py                           # Metric entity\n\u2502   \u2502   \u251c\u2500\u2500 log_entry.py                        # LogEntry entity\n\u2502   \u2502   \u251c\u2500\u2500 trace.py                            # Trace entity\n\u2502   \u2502   \u251c\u2500\u2500 metric_type.py                      # MetricType value object\n\u2502   \u2502   \u251c\u2500\u2500 log_level.py                        # LogLevel value object\n\u2502   \u2502   \u251c\u2500\u2500 trace_id.py                         # TraceId value object\n\u2502   \u2502   \u251c\u2500\u2500 metrics_collector.py                # MetricsCollector service\n\u2502   \u2502   \u251c\u2500\u2500 logger.py                           # Logger service\n\u2502   \u2502   \u2514\u2500\u2500 tracer.py                           # Tracer service\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 application/                            # Application Layer\n\u2502   \u2502   \u251c\u2500\u2500 record_metric/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 record_metric_command.py        # RecordMetricCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 record_metric_handler.py        # RecordMetricHandler\n\u2502   \u2502   \u251c\u2500\u2500 log_event/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 log_event_command.py            # LogEventCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 log_event_handler.py            # LogEventHandler\n\u2502   \u2502   \u251c\u2500\u2500 create_trace/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 create_trace_command.py         # CreateTraceCommand\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 create_trace_handler.py         # CreateTraceHandler\n\u2502   \u2502   \u2514\u2500\u2500 dto/\n\u2502   \u2502       \u251c\u2500\u2500 metric_dto.py                   # MetricDTO\n\u2502   \u2502       \u2514\u2500\u2500 trace_dto.py                    # TraceDTO\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 infrastructure/                         # Infrastructure Layer\n\u2502       \u251c\u2500\u2500 metrics/\n\u2502       \u2502   \u251c\u2500\u2500 i_metrics_exporter.py           # IMetricsExporter interface\n\u2502       \u2502   \u2514\u2500\u2500 prometheus_exporter.py          # Prometheus implementation\n\u2502       \u2514\u2500\u2500 logging/\n\u2502           \u2514\u2500\u2500 structured_logger.py            # Structured logging implementation\n\u2502\n\u251c\u2500\u2500 presentation/                       # Presentation Layer (Cross-cutting)\n\u2502   \u251c\u2500\u2500 rest/\n\u2502   \u2502   \u251c\u2500\u2500 main.py                     # FastAPI app\n\u2502   \u2502   \u251c\u2500\u2500 routes/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 models.py               # Model registry endpoints\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 inference.py            # Inference endpoints\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 management.py           # Lifecycle management endpoints\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 health.py               # Health check endpoints\n\u2502   \u2502   \u251c\u2500\u2500 schemas.py                  # Pydantic request/response models\n\u2502   \u2502   \u2514\u2500\u2500 dependencies.py             # FastAPI dependencies\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 grpc/\n\u2502   \u2502   \u251c\u2500\u2500 server.py                   # gRPC server\n\u2502   \u2502   \u251c\u2500\u2500 inference_service_impl.py   # InferenceService implementation\n\u2502   \u2502   \u251c\u2500\u2500 registry_service_impl.py    # RegistryService implementation\n\u2502   \u2502   \u251c\u2500\u2500 management_service_impl.py  # ManagementService implementation\n\u2502   \u2502   \u2514\u2500\u2500 protos/\n\u2502   \u2502       \u251c\u2500\u2500 inference.proto\n\u2502   \u2502       \u251c\u2500\u2500 registry.proto\n\u2502   \u2502       \u2514\u2500\u2500 management.proto\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 cli/\n\u2502       \u251c\u2500\u2500 main.py                     # Click app\n\u2502       \u2514\u2500\u2500 commands/\n\u2502           \u251c\u2500\u2500 install.py              # modelmora install\n\u2502           \u251c\u2500\u2500 list.py                 # modelmora list\n\u2502           \u251c\u2500\u2500 info.py                 # modelmora info\n\u2502           \u251c\u2500\u2500 lock.py                 # modelmora lock\n\u2502           \u251c\u2500\u2500 serve.py                # modelmora serve\n\u2502           \u251c\u2500\u2500 health.py               # modelmora health\n\u2502           \u2514\u2500\u2500 uninstall.py            # modelmora uninstall\n\u2502\n\u251c\u2500\u2500 worker/                             # Worker Process (Cross-cutting)\n\u2502   \u251c\u2500\u2500 main.py                         # Worker entry point\n\u2502   \u251c\u2500\u2500 model_executor.py               # Model inference execution\n\u2502   \u251c\u2500\u2500 ipc.py                          # Inter-process communication\n\u2502   \u2514\u2500\u2500 handlers/\n\u2502       \u251c\u2500\u2500 txt2embed_handler.py        # Text embedding handler\n\u2502       \u251c\u2500\u2500 img2embed_handler.py        # Image embedding handler\n\u2502       \u251c\u2500\u2500 txt2img_handler.py          # Text-to-image handler\n\u2502       \u251c\u2500\u2500 img2txt_handler.py          # Image-to-text handler\n\u2502       \u2514\u2500\u2500 txt2txt_handler.py          # Text-to-text handler\n\u2502\n\u2514\u2500\u2500 shared/                             # Shared Kernel (Cross-cutting)\n    \u251c\u2500\u2500 logging.py                      # Logging utilities\n    \u251c\u2500\u2500 types.py                        # Common types\n    \u251c\u2500\u2500 constants.py                    # System constants\n    \u251c\u2500\u2500 exceptions.py                   # Base exceptions\n    \u2514\u2500\u2500 event_bus.py                    # Event bus implementation\n</code></pre>"},{"location":"architecture/#5-key-workflows","title":"5. Key Workflows","text":""},{"location":"architecture/#51-model-registration-flow","title":"5.1 Model Registration Flow","text":"<pre>2bf0164fdc9f3f1831c79afb54e0ed3f70ea6e7e4be74c0d284e338bd6d278c8c2da75d76629e7f5e331955b3248ffc9bb492d60e1105386148685f146725966</pre><pre>77608c112dcf2822b8801dc65cbdd6e2eb0af9979f009e9cf6783acf2d69af345e59ee26ccf681d5c918e6edecbb8205650221d53a60781c45ad780cfe5fb9c7</pre>"},{"location":"architecture/#52-lazy-model-loading-flow","title":"5.2 Lazy Model Loading Flow","text":"<pre>29d6f0f1465acaa675020c6e4fa533e3de18c8c165713c81fc14fea004e25debafb4f60d56c948e93241523dfc58763a7291a455e3aa85371c11125668fb8716</pre><pre>5a5170fbb9834c51b46e5412052af5e4bac79908c69e51a681cea1fc4fe02f8c2b087458a112636fa2fd35919341a19c8f6cd1c166cf5a80b792d3841c4712be</pre>"},{"location":"architecture/#53-asynchronous-inference-flow","title":"5.3 Asynchronous Inference Flow","text":"<pre>861b4eb681de506146e9d76daf1dd39b21c739297991fba12394e6cd4ba65791eba3bc2d1e2eac8c19fa3fee7bd8e48c74e9832b9a6f82868c03775521491bd0</pre><pre>f12c403513775bcde2a6d3f301f004ff6f4e7485d9406b8d9ee8da83d6b9e8bd3913bb588246573c427c4d16a093f118ad6ee722f3689cf6bb4397d5b0c18de7</pre>"},{"location":"architecture/#54-batch-accumulation-flow","title":"5.4 Batch Accumulation Flow","text":"<pre>6cff391602511fe7588c7f3eb9481b2fef3d2d614f33c7ec68c7871e54cb7437f09d443984d2f430456934923b4a950553222b5b6e87ebe3dacd2c4a75c9d725</pre><pre>7e59f98123d4a05a70d68c689ee47848195b8ba6622c9b6bc07d0fdc82b15dcd484c2e2c6c4170067f3bef47042bb2a98a34c0997896bd24df2b7b892ef75376</pre>"},{"location":"architecture/#55-lru-eviction-flow","title":"5.5 LRU Eviction Flow","text":"<pre>0a6109e14175dcf44134b80b5ae575b1deaa415ae94e49ea75de4f1bbb249c322db2e9fe4a79e78376cfc56c9718862356b2a2412311a6a990f744a2e23d1354</pre><pre>3e787847ffa4500fd567bc8e8516b02df870a898499fea2048716dd89ac0cfd0a5ef8b74e650838067331df03ee66c42e2d18871e7ec837de5dc99a56ff698d1</pre>"},{"location":"architecture/#6-data-flow-architecture","title":"6. Data Flow Architecture","text":""},{"location":"architecture/#61-request-processing-data-flow","title":"6.1 Request Processing Data Flow","text":"<pre>f6141c499f3b69f4b34024206250017014462a5d5ddf9f806dd44db8e265be590230d537121ae9925042b31bc27d5a51b9f11ad0d808710ad2d8d93d10be07f6</pre><pre>22716ac39765fb22e5c788ba0d8dd4f6519909fc25afef7414aacbdf9ef8687c0eac070fd0a7a63d08c2d49c55007c32ad05c7db14cfd74e57a7ef4c4e3423c8</pre>"},{"location":"architecture/#62-model-download-data-flow","title":"6.2 Model Download Data Flow","text":"<pre>6e053290fdc550b502bb232a33fae04c10544e22d0e39a13fd3b4452cdf4538b36394bf889eaf29186634690c8e09dbec172fc0bb2ede388b8f341c409caf6ce</pre><pre>a31f2c190f43a8ec093babe54312b74a173d171299e69f1d027c7ae5772ec5da0134864a81f01879364421dbe502e94fbbe89cf5448dc15956f1d3f7e33bbba3</pre>"},{"location":"architecture/#7-deployment-architecture","title":"7. Deployment Architecture","text":""},{"location":"architecture/#71-single-node-deployment-mvp","title":"7.1 Single-Node Deployment (MVP)","text":"<pre>7b418aba11860191f30729d4b6fe943fd5004f35b2d211b321640ecea74c1f374ad01b0d025f7d64cbc747bc56dd23367d3c2a6bf11c656672c6f1338971f524</pre><pre>3a3da032535282808e186cfc1f660fb44c62ba7f94ba926c8f5a7850a5afa8b53138e94c326ee269dee51c20c73224804079699bbad236b43484e6b561275a0d</pre>"},{"location":"architecture/#72-kubernetes-deployment-production","title":"7.2 Kubernetes Deployment (Production)","text":"<pre>8994bc4c08231f79b2215d0d8098e348dd800c685198fa8be15433adac878fdecf28b873db99d3d903bd371caf8ff32e5aa425e99b7f42f96dfbb083cd50b07d</pre><pre>403793ef33d89c5bc4dc5360628c3dc726fd3dec4bdd42645c2c9ca494b568ced2bf8c352063c4344c03205f69ee5c033ec909b08544a36bbf63f7d4fdf37490</pre>"},{"location":"architecture/#73-container-architecture","title":"7.3 Container Architecture","text":"<pre>7f2d8a0258afdf277ce273c77ab47a227246c6bbaa1c5a96533ce865782ea7b9ded5b1113f60eaf032329458589360747f660783e0ba0c2bcd883e21bb9b4422</pre><pre>4cf421a6cc57ba285f85cb766b40a06ab488f31da9daee0bf50aa3a35cb8f935e5430f5a424ac8351c0d0747783b95f75cdf80c14a7cc67fe14521511b0b1a1a</pre>"},{"location":"architecture/#8-testing-strategy","title":"8. Testing Strategy","text":""},{"location":"architecture/#81-test-pyramid","title":"8.1 Test Pyramid","text":"<p>Following TDD principles, ModelMora employs a comprehensive testing strategy:</p> <pre><code>                 /\\\n                /  \\\n               /E2E \\           (~5% - End-to-End Tests)\n              /------\\\n             /        \\\n            /Integration\\      (~20% - Integration Tests)\n           /------------\\\n          /              \\\n         /  Unit Tests    \\   (~75% - Unit Tests)\n        /------------------\\\n</code></pre>"},{"location":"architecture/#82-unit-tests-75","title":"8.2 Unit Tests (75%)","text":"<p>Domain Layer Tests (100% coverage required):</p> <ul> <li>Entity behavior and invariants</li> <li>Value object validation</li> <li>Policy logic (LRU, priority, batching)</li> <li>Domain service operations</li> </ul> <p>Example:</p> <pre><code># tests/unit/domain/lifecycle/test_lru_policy.py\ndef test_lru_policy_selects_least_recently_used_model():\n    policy = LRUEvictionPolicy()\n    models = [\n        LoadedModel(\"model-a\", last_accessed=now - timedelta(minutes=10)),\n        LoadedModel(\"model-b\", last_accessed=now - timedelta(minutes=5)),\n        LoadedModel(\"model-c\", last_accessed=now - timedelta(minutes=2)),\n    ]\n\n    to_evict = policy.select_model_to_evict(models)\n\n    assert to_evict.model_id == \"model-a\"\n</code></pre>"},{"location":"architecture/#83-integration-tests-20","title":"8.3 Integration Tests (20%)","text":"<p>Application Layer Tests:</p> <ul> <li>Handler orchestration with mock repositories</li> <li>Event publishing and handling</li> <li>Cross-context interactions</li> </ul> <p>Infrastructure Tests:</p> <ul> <li>Database operations (using TestContainers)</li> <li>API endpoint integration</li> <li>Worker process communication</li> </ul> <p>Example:</p> <pre><code># tests/integration/lifecycle/application/test_load_model_handler.py\n@pytest.mark.integration\ndef test_load_model_downloads_and_spawns_worker(\n    model_repository: IModelRepository,\n    downloader: IModelDownloader,\n    process_manager: IProcessManager\n):\n    handler = LoadModelHandler(model_repository, downloader, process_manager)\n\n    result = handler.handle(LoadModelCommand(model_id=\"model-x\"))\n\n    assert result.status == \"LOADED\"\n    assert process_manager.is_running(\"model-x\")\n</code></pre>"},{"location":"architecture/#84-end-to-end-tests-5","title":"8.4 End-to-End Tests (5%)","text":"<p>System Tests:</p> <ul> <li>Full API workflows (REST and gRPC)</li> <li>Multi-model concurrent inference</li> <li>Performance benchmarks</li> <li>Failure scenarios</li> </ul> <p>Example:</p> <pre><code># tests/e2e/test_inference_workflow.py\n@pytest.mark.e2e\ndef test_complete_inference_workflow(api_client, test_model):\n    # Register model\n    response = api_client.post(\"/api/v1/models\", json=test_model)\n    assert response.status_code == 201\n\n    # Submit inference\n    response = api_client.post(\n        f\"/api/v1/infer/{test_model['model_id']}\",\n        json={\"input_data\": {\"text\": \"test\"}}\n    )\n    assert response.status_code == 200\n    assert \"embedding\" in response.json()\n</code></pre>"},{"location":"architecture/#9-deployment-configuration","title":"9. Deployment Configuration","text":""},{"location":"architecture/#91-docker-compose-development","title":"9.1 Docker Compose (Development)","text":"<pre><code>services:\n  modelmora:\n    build: .\n    ports:\n      - \"8000:8000\"   # REST API\n      - \"50051:50051\" # gRPC\n    environment:\n      - DATABASE_URL=postgresql://user:pass@db:5432/modelmora\n      - MODEL_CACHE_DIR=/models\n      - RESULT_STORAGE_TYPE=filesystem\n      - RESULT_STORAGE_PATH=/results\n      - LOG_LEVEL=INFO\n    volumes:\n      - model_cache:/models\n      - results:/results\n      - ./config:/app/config:ro\n    depends_on:\n      - db\n    deploy:\n      resources:\n        limits:\n          memory: 16G\n          cpus: '8'\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\n  db:\n    image: postgres:15\n    environment:\n      - POSTGRES_USER=user\n      - POSTGRES_PASSWORD=pass\n      - POSTGRES_DB=modelmora\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml\n\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n    volumes:\n      - grafana_data:/var/lib/grafana\n\nvolumes:\n  model_cache:\n  results:\n  postgres_data:\n  grafana_data:\n</code></pre>"},{"location":"architecture/#92-kubernetes-helm-values","title":"9.2 Kubernetes Helm Values","text":"<pre><code># values.yaml\nreplicaCount: 3\n\nimage:\n  repository: modelmora/modelmora\n  tag: \"1.0.0\"\n  pullPolicy: IfNotPresent\n\nservice:\n  type: ClusterIP\n  restPort: 8000\n  grpcPort: 50051\n\ningress:\n  enabled: true\n  className: nginx\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n  hosts:\n    - host: modelmora.example.com\n      paths:\n        - path: /\n          pathType: Prefix\n  tls:\n    - secretName: modelmora-tls\n      hosts:\n        - modelmora.example.com\n\nresources:\n  limits:\n    memory: 16Gi\n    cpu: 8\n    nvidia.com/gpu: 1\n  requests:\n    memory: 8Gi\n    cpu: 4\n    nvidia.com/gpu: 1\n\nautoscaling:\n  enabled: true\n  minReplicas: 2\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 70\n  targetMemoryUtilizationPercentage: 80\n\npersistence:\n  modelCache:\n    enabled: true\n    storageClass: \"efs-sc\"\n    accessMode: ReadWriteMany\n    size: 500Gi\n\npostgresql:\n  enabled: true\n  auth:\n    database: modelmora\n    username: modelmora\n  primary:\n    persistence:\n      enabled: true\n      size: 50Gi\n\nredis:\n  enabled: true\n  architecture: standalone\n  auth:\n    enabled: false\n\ns3:\n  enabled: true\n  bucket: modelmora-results\n  region: us-east-1\n  endpoint: https://s3.amazonaws.com\n\nconfig:\n  logLevel: INFO\n  maxLoadedModels: 5\n  memoryThresholdPercent: 90\n  batchMaxSize: 32\n  batchMaxWaitMs: 100\n  resultExpirationDays: 7\n</code></pre>"},{"location":"architecture/#10-security-architecture","title":"10. Security Architecture","text":""},{"location":"architecture/#101-security-layers","title":"10.1 Security Layers","text":"<ol> <li>Network Security:</li> <li>TLS 1.3 for all external communications</li> <li>gRPC with mutual TLS (mTLS) option</li> <li> <p>Network policies in Kubernetes</p> </li> <li> <p>Authentication &amp; Authorization:</p> </li> <li>API key authentication for REST endpoints</li> <li>JWT tokens for user sessions (future)</li> <li> <p>Role-based access control (RBAC)</p> </li> <li> <p>Data Security:</p> </li> <li>Model checksum verification</li> <li>Input validation and sanitization</li> <li> <p>Result encryption at rest (S3 SSE)</p> </li> <li> <p>Process Isolation:</p> </li> <li>Separate processes per model</li> <li>Container security contexts (non-root user)</li> <li>Resource limits and quotas</li> </ol>"},{"location":"architecture/#102-security-sequence","title":"10.2 Security Sequence","text":"<pre>d39d85323f653c3ead2de6c7272eef6ee5b4504007080d41b76d8bbe1add29171ef4f78cebbd01dce1e91a19b7b59a32886cb14fb248bd05e4cc5257c6e7e429</pre><pre>a08d77ec345673a7038cfeb521bc0b0f291ef196cdd70676f0620d8705dae3feb793526b1d973d34fb8f2a439601b88a5d70a2615e7e8fa60cfffde261cf2de6</pre>"},{"location":"architecture/#11-performance-considerations","title":"11. Performance Considerations","text":""},{"location":"architecture/#111-optimization-strategies","title":"11.1 Optimization Strategies","text":"<ol> <li>Batching: Accumulate requests to maximize GPU utilization</li> <li>Lazy Loading: Load models only when needed</li> <li>Connection Pooling: Reuse database connections</li> <li>Result Caching: Cache frequently requested results (future)</li> <li>Async Processing: Non-blocking request handling</li> <li>Process Pooling: Reuse worker processes when possible</li> </ol>"},{"location":"architecture/#112-resource-management","title":"11.2 Resource Management","text":"<pre><code>Memory Budget (16GB container):\n- Base System: 2GB\n- FastAPI/gRPC: 1GB\n- PostgreSQL Client: 0.5GB\n- Model Cache Overhead: 0.5GB\n- Available for Models: 12GB\n\nGPU Budget (24GB VRAM):\n- Model A (7B params): ~14GB\n- Model B (embedding): ~2GB\n- Model C (small): ~1GB\n- Buffer: 7GB\n</code></pre>"},{"location":"architecture/#12-monitoring-observability","title":"12. Monitoring &amp; Observability","text":""},{"location":"architecture/#121-key-metrics","title":"12.1 Key Metrics","text":"<p>System Metrics:</p> <ul> <li>CPU usage per pod</li> <li>Memory usage per pod</li> <li>GPU utilization and VRAM</li> <li>Disk I/O for model cache</li> </ul> <p>Application Metrics:</p> <ul> <li>Request latency (p50, p95, p99)</li> <li>Throughput (requests/second)</li> <li>Error rate by endpoint</li> <li>Queue depth by priority</li> </ul> <p>Business Metrics:</p> <ul> <li>Models loaded count</li> <li>Total inference requests</li> <li>Average batch size</li> <li>Model load/unload frequency</li> </ul>"},{"location":"architecture/#122-alerting-rules","title":"12.2 Alerting Rules","text":"<pre><code>groups:\n  - name: modelmora_alerts\n    rules:\n      - alert: HighErrorRate\n        expr: rate(modelmora_requests_failed_total[5m]) &gt; 0.05\n        annotations:\n          summary: \"High error rate detected\"\n\n      - alert: HighMemoryUsage\n        expr: modelmora_memory_usage_percent &gt; 90\n        annotations:\n          summary: \"Memory usage above 90%\"\n\n      - alert: ModelLoadFailure\n        expr: increase(modelmora_model_load_failed_total[5m]) &gt; 3\n        annotations:\n          summary: \"Multiple model load failures\"\n</code></pre>"},{"location":"architecture/#13-evolution-future-architecture","title":"13. Evolution &amp; Future Architecture","text":""},{"location":"architecture/#131-planned-enhancements","title":"13.1 Planned Enhancements","text":"<p>Phase 1 (Current):</p> <ul> <li>Single-node deployment</li> <li>Basic multi-process isolation</li> <li>REST + gRPC APIs</li> </ul> <p>Phase 2 (6 months):</p> <ul> <li>Kubernetes multi-node</li> <li>Redis-based distributed queue</li> <li>Horizontal scaling</li> </ul> <p>Phase 3 (12 months):</p> <ul> <li>Model A/B testing</li> <li>Advanced caching layer</li> <li>ONNX Runtime support</li> <li>TensorRT optimization</li> </ul> <p>Phase 4 (18 months):</p> <ul> <li>Multi-tenant isolation</li> <li>Fine-tuning pipeline</li> <li>Model marketplace</li> <li>Advanced monitoring</li> </ul>"},{"location":"architecture/#132-architectural-flexibility","title":"13.2 Architectural Flexibility","text":"<p>The hexagonal architecture allows easy evolution:</p> <ul> <li>Swap PostgreSQL \u2192 MongoDB (implement new adapter)</li> <li>Add Kafka \u2192 Create new infrastructure adapter</li> <li>Support ONNX \u2192 Add new domain service</li> <li>New API protocol \u2192 Add presentation adapter</li> </ul>"},{"location":"architecture/#14-revision-history","title":"14. Revision History","text":"Version Date Author Changes 1.0.0 2025-12-04 - Initial architecture document with DDD, Hexagonal, and deployment diagrams"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for considering contributing to ModelMora!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork and clone the repository</li> <li>Install dependencies: <code>poetry install --with dev,docs</code></li> <li>Install pre-commit hooks: <code>poetry run pre-commit install</code></li> <li>Create a feature branch: <code>git checkout -b feature/my-feature</code></li> </ol>"},{"location":"contributing/#code-standards","title":"Code Standards","text":"<ul> <li>Formatting: Black (120 char line length)</li> <li>Linting: Pylint, isort</li> <li>Type Checking: MyPy</li> <li>Testing: pytest (&gt;70% coverage required)</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Write tests for new features</li> <li>Ensure all tests pass: <code>poetry run pytest</code></li> <li>Format code: <code>poetry run black . &amp;&amp; poetry run isort .</code></li> <li>Run linters: <code>poetry run pylint src/modelmora</code></li> <li>Update documentation as needed</li> <li>Submit PR with clear description</li> </ol>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits:</p> <pre><code>feat: add model warmup feature\nfix: resolve memory leak in worker cleanup\ndocs: update API reference\ntest: add integration test for lifecycle\n</code></pre>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Open an issue or discussion on GitHub.</p>"},{"location":"data_models/","title":"ModelMora - Data Models Documentation","text":"<p>Version: 1.0.0 Date: 2025-12-04 Status: Draft</p>"},{"location":"data_models/#1-introduction","title":"1. Introduction","text":""},{"location":"data_models/#11-purpose","title":"1.1 Purpose","text":"<p>This document defines the data models used throughout ModelMora, providing language-agnostic specifications for entities, value objects, aggregates, and data transfer objects. Each model includes structural definitions, constraints, validation rules, and relationships.</p>"},{"location":"data_models/#12-model-categories","title":"1.2 Model Categories","text":"<p>ModelMora data models are organized into the following categories:</p> <ul> <li>Domain Entities: Core business objects with identity and lifecycle</li> <li>Value Objects: Immutable objects defined by their attributes</li> <li>Aggregates: Clusters of entities with a root and consistency boundary</li> <li>Data Transfer Objects (DTOs): API request/response representations</li> <li>Event Models: Domain events for event-driven architecture</li> <li>Persistence Models: Database schema representations</li> </ul>"},{"location":"data_models/#13-notation-conventions","title":"1.3 Notation Conventions","text":"<ul> <li>MUST: Required field, cannot be null/empty</li> <li>SHOULD: Recommended field, may be optional</li> <li>MAY: Optional field</li> <li>Immutable: Cannot be modified after creation</li> <li>Derived: Calculated from other fields</li> </ul>"},{"location":"data_models/#14-validation-levels","title":"1.4 Validation Levels","text":"<ul> <li>Syntax: Format and type validation</li> <li>Semantic: Business rule validation</li> <li>Cross-Entity: Referential integrity validation</li> </ul>"},{"location":"data_models/#2-model-organization","title":"2. Model Organization","text":"<p>Data models are organized by bounded context and detailed in separate files:</p>"},{"location":"data_models/#21-registry-context-models","title":"2.1 Registry Context Models","text":"<ul> <li>Model Entity - Core model representation</li> <li>Model Version - Version tracking</li> <li>Model Lock - Reproducible deployment</li> <li>Model Catalog Aggregate - Collection management</li> <li>Task Type - Model capability enumeration</li> <li>Resource Requirements - Compute specifications</li> </ul>"},{"location":"data_models/#22-lifecycle-context-models","title":"2.2 Lifecycle Context Models","text":"<ul> <li>Loaded Model - In-memory model instance</li> <li>Model Process - Worker process representation</li> <li>Model Lifecycle Manager - Lifecycle coordination</li> <li>Model State - State enumeration</li> <li>Memory Usage - Memory metrics</li> <li>Health Status - Health check results</li> </ul>"},{"location":"data_models/#23-inference-context-models","title":"2.3 Inference Context Models","text":"<ul> <li>Inference Request - Request representation</li> <li>Inference Job - Async job tracking</li> <li>Batch - Request batching</li> <li>Inference Queue Aggregate - Queue management</li> <li>Job Status - Status enumeration</li> <li>Priority - Priority levels</li> <li>Input Data - Task-specific input</li> <li>Output Data - Task-specific output</li> </ul>"},{"location":"data_models/#24-observability-context-models","title":"2.4 Observability Context Models","text":"<ul> <li>Metric - Time-series data point</li> <li>Log Entry - Structured log message</li> <li>Trace - Distributed tracing span</li> </ul>"},{"location":"data_models/#25-cross-cutting-models","title":"2.5 Cross-Cutting Models","text":"<ul> <li>API Request/Response DTOs - REST API contracts</li> <li>Event Models - Domain event definitions</li> <li>Error Models - Error representations</li> </ul>"},{"location":"data_models/#3-common-patterns","title":"3. Common Patterns","text":""},{"location":"data_models/#31-entity-pattern","title":"3.1 Entity Pattern","text":"<p>All entities follow this pattern:</p> <pre>1086b99689203f3e1184f4bb09bbb42342a57c057377b91e544ca5285f61347d4df60074f7d3a7dd7fac59e05d3e5777bbbe3a96b5c78193187a03d213963ee4</pre><pre>522f002b910c3aed2b4f399693629d09c09240c117e3f301349b87a0b1f0e614f959e710aa1f171e01eebb7923163d1c7afda628c28dce6692409871fd010b21</pre> <p>Constraints:</p> <ul> <li><code>id</code> MUST be unique within entity type</li> <li><code>created_at</code> MUST NOT be modified after creation</li> <li><code>updated_at</code> MUST be updated on every modification</li> <li><code>version</code> MUST increment on every modification (optimistic locking)</li> </ul>"},{"location":"data_models/#32-value-object-pattern","title":"3.2 Value Object Pattern","text":"<p>All value objects follow this pattern:</p> <pre>d080729b56f735bbc55b3cfed48033cd848b6b624e413b99b0f59590095b9f0fd8703b44f3f0ea57658a61e9fcc0a487ebd074de65eab992723f3530b7f52210</pre><pre>a10870867cd4b95dbff909b905d520c826f00ec9a23bfe487fd8772c470d2c19eac1d4ae03ff63c068f3d57a683ebce7dda8c7689f5dbc2afebbcb4f4e1cafa1</pre> <p>Constraints:</p> <ul> <li>All fields MUST be immutable</li> <li>Equality based on structural comparison</li> <li>No identity field required</li> </ul>"},{"location":"data_models/#33-aggregate-pattern","title":"3.3 Aggregate Pattern","text":"<p>All aggregates follow this pattern:</p> <pre>9fb3f7f993767bdcfd4bc6d92e7cae3b6cd95d3689587e13de110b2287741bc222c12ffd4d722c8afc67064e6ac94d41887db7c28248dc43ba7af2e40307585d</pre><pre>1d2a5b04fdda0f8681d66d5931c47fc406922f187f97493304e98aa707b0d11452f23934d7f6bf4715f540e7fcdd3134451d018ead341de682ed678254da868a</pre> <p>Constraints:</p> <ul> <li>External references MUST only point to aggregate root</li> <li>Child entities MUST NOT be modified outside aggregate</li> <li>All changes MUST go through aggregate root methods</li> <li>Aggregate MUST maintain invariants</li> </ul>"},{"location":"data_models/#4-data-type-definitions","title":"4. Data Type Definitions","text":""},{"location":"data_models/#41-primitive-types","title":"4.1 Primitive Types","text":"Type Description Format Example String UTF-8 text - <code>\"hello\"</code> Integer Whole number - <code>42</code> Float Decimal number IEEE 754 <code>3.14</code> Boolean True/false - <code>true</code> Bytes Binary data Base64 (JSON) <code>b\"\\x89PNG\"</code>"},{"location":"data_models/#42-semantic-types","title":"4.2 Semantic Types","text":"Type Description Format Example Timestamp Point in time ISO 8601 <code>2025-12-04T10:30:00Z</code> Duration Time span ISO 8601 <code>PT30S</code> (30 seconds) UUID Unique identifier RFC 4122 <code>123e4567-e89b-12d3-a456-426614174000</code> URL Web resource RFC 3986 <code>https://example.com/model</code> Email Email address RFC 5322 <code>user@example.com</code> Checksum Hash digest Hex string <code>sha256:abc123...</code>"},{"location":"data_models/#43-domain-specific-types","title":"4.3 Domain-Specific Types","text":"Type Description Pattern Example ModelId HuggingFace identifier <code>{org}/{repo}</code> <code>sentence-transformers/all-MiniLM-L6-v2</code> Version Semantic version <code>{major}.{minor}.{patch}</code> or branch <code>v2.2.2</code>, <code>main</code> TaskType ML task category Enumeration <code>txt2embed</code> Priority Request priority Enumeration <code>high</code>, <code>medium</code>, <code>low</code> ModelState Lifecycle state Enumeration <code>loaded</code>, <code>unloaded</code>"},{"location":"data_models/#5-relationship-diagrams","title":"5. Relationship Diagrams","text":""},{"location":"data_models/#51-registry-context-relationships","title":"5.1 Registry Context Relationships","text":"<pre>6f33a0682fd4a67540d1afedcad9ae0bba65e4e2702b1121c0084c95b50d27c57cb594b8eed031167997b7bfbaa9fb65a588b6ab99e06c248a2ed8c8c39ec92b</pre><pre>7efa00a34bb4e220707ffa83a092ba9f58a24a1de2c4b717d3795cbcb4bd73ee0bf1f2b4673eaacfc3b2114a8048821ecac80ce0b3515e30c6087eea08f76146</pre>"},{"location":"data_models/#52-lifecycle-context-relationships","title":"5.2 Lifecycle Context Relationships","text":"<pre>9d4a10ded548a8fd381bba8f90d7b67136094e77f409fec6001133a28e34a43b4d03c89d963ee88594fe26785fa51be4dd826ce4c8b5a02bbafb35f564920c71</pre><pre>97836b90918cb9a271cfc085906868a10b757f5dd260a2e5045f22b3d8793add2a80ed08f5fad4d686b8a662c15e020c0fdda47d09956232490f321c98067f72</pre>"},{"location":"data_models/#53-inference-context-relationships","title":"5.3 Inference Context Relationships","text":"<pre>90e0e1bd8a5ad0ec4631ca2689f0e67eb8eb97af8a80e6580ae9ad9fd70073c557cb303d26ca88bdf7cfdce5f9923172bba73aca052153aee6eee09a4d367e34</pre><pre>8f67ad105c1b6290e470ae41f0f83d6fa11f5bbbb9aa9f1211adf1972348a4edacbb3659bd2dc2baef274310e8973d8c09187dd517010566f93263194fad996e</pre>"},{"location":"data_models/#6-validation-rules","title":"6. Validation Rules","text":""},{"location":"data_models/#61-global-validation-rules","title":"6.1 Global Validation Rules","text":"<p>These rules apply to all models across bounded contexts:</p> <ol> <li>Required Fields: All fields marked <code>MUST</code> cannot be null, empty, or whitespace-only</li> <li>String Length:</li> <li>Short strings (names, ids): 1-255 characters</li> <li>Medium strings (descriptions): 1-1000 characters</li> <li>Long strings (text content): 1-100,000 characters</li> <li>Numeric Ranges:</li> <li>Positive integers: &gt; 0</li> <li>Non-negative integers: &gt;= 0</li> <li>Percentages: 0.0 - 100.0</li> <li>Memory values: &gt; 0 (in MB)</li> <li>Timestamps:</li> <li>MUST be in UTC</li> <li>MUST follow ISO 8601 format</li> <li>Future timestamps only allowed for scheduled operations</li> <li>Identifiers:</li> <li>MUST be unique within their scope</li> <li>MUST NOT contain special characters (except hyphens and underscores)</li> <li>MUST be case-sensitive</li> </ol>"},{"location":"data_models/#62-business-rule-validation","title":"6.2 Business Rule Validation","text":""},{"location":"data_models/#registry-context-rules","title":"Registry Context Rules","text":"<ol> <li>Model Registration:</li> <li>ModelId MUST match pattern: <code>{org}/{repo}</code></li> <li>TaskType MUST be one of the supported types</li> <li>Version MUST be semantic version or branch name</li> <li> <p>Checksum MUST be SHA256 format</p> </li> <li> <p>Model Versioning:</p> </li> <li>Multiple versions allowed for same ModelId</li> <li>Version strings MUST be unique per ModelId</li> <li> <p>Cannot delete version if it's marked as current</p> </li> <li> <p>Resource Requirements:</p> </li> <li>memory_mb MUST be &gt; 0</li> <li>gpu_vram_mb MAY be 0 (CPU-only models)</li> <li>cpu_threads MUST be &gt; 0</li> </ol>"},{"location":"data_models/#lifecycle-context-rules","title":"Lifecycle Context Rules","text":"<ol> <li>Model Loading:</li> <li>Cannot load model if memory threshold exceeded</li> <li>Cannot load model if already in LOADING or LOADED state</li> <li> <p>MUST verify health after loading</p> </li> <li> <p>Model Eviction:</p> </li> <li>Can only evict models in LOADED state</li> <li>Cannot evict if processing active requests</li> <li> <p>MUST follow LRU policy</p> </li> <li> <p>State Transitions:</p> </li> <li>Valid transitions: UNLOADED \u2192 LOADING \u2192 LOADED \u2192 UNHEALTHY</li> <li>Valid transitions: LOADED \u2192 UNLOADED</li> <li>Invalid: LOADING \u2192 UNLOADED (must complete or fail)</li> </ol>"},{"location":"data_models/#inference-context-rules","title":"Inference Context Rules","text":"<ol> <li>Request Submission:</li> <li>ModelId MUST exist in registry</li> <li>InputData MUST match TaskType requirements</li> <li>Priority MUST be valid enum value</li> <li> <p>Timeout MUST be &gt; 0 and &lt;= 600 seconds</p> </li> <li> <p>Batch Formation:</p> </li> <li>All requests in batch MUST target same ModelId</li> <li>Batch size MUST be &gt; 0 and &lt;= configured maximum</li> <li> <p>Requests MUST have compatible input shapes</p> </li> <li> <p>Job Management:</p> </li> <li>JobId MUST be unique UUID</li> <li>Cannot cancel job in COMPLETED or FAILED state</li> <li>Cannot retrieve result if job not COMPLETED</li> </ol>"},{"location":"data_models/#7-serialization-formats","title":"7. Serialization Formats","text":""},{"location":"data_models/#71-json-representation","title":"7.1 JSON Representation","text":"<p>Default format for REST API and configuration files:</p> <pre><code>{\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"version\": \"v2.2.2\",\n  \"task_type\": \"txt2embed\",\n  \"resource_requirements\": {\n    \"memory_mb\": 2048,\n    \"gpu_vram_mb\": 0,\n    \"cpu_threads\": 4\n  },\n  \"created_at\": \"2025-12-04T10:30:00Z\",\n  \"updated_at\": \"2025-12-04T10:30:00Z\"\n}\n</code></pre> <p>Conventions:</p> <ul> <li>Field names: snake_case</li> <li>Timestamps: ISO 8601 with UTC timezone</li> <li>Enums: lowercase strings</li> <li>Null values: excluded from JSON (omitempty)</li> </ul>"},{"location":"data_models/#72-protocol-buffers-representation","title":"7.2 Protocol Buffers Representation","text":"<p>Format for gRPC communication:</p> <pre><code>message Model {\n  string model_id = 1;\n  string version = 2;\n  TaskType task_type = 3;\n  ResourceRequirements resource_requirements = 4;\n  google.protobuf.Timestamp created_at = 5;\n  google.protobuf.Timestamp updated_at = 6;\n}\n</code></pre> <p>Conventions:</p> <ul> <li>Field names: snake_case</li> <li>Field numbers: sequential, never reuse</li> <li>Optional fields: use wrapper types or <code>optional</code> keyword</li> <li>Enums: UPPERCASE with prefix</li> </ul>"},{"location":"data_models/#73-yaml-representation","title":"7.3 YAML Representation","text":"<p>Format for configuration and lock files:</p> <pre><code>model_id: sentence-transformers/all-MiniLM-L6-v2\nversion: v2.2.2\ntask_type: txt2embed\nresource_requirements:\n  memory_mb: 2048\n  gpu_vram_mb: 0\n  cpu_threads: 4\ncreated_at: 2025-12-04T10:30:00Z\nupdated_at: 2025-12-04T10:30:00Z\n</code></pre> <p>Conventions:</p> <ul> <li>Field names: snake_case</li> <li>Indentation: 2 spaces</li> <li>List items: dash prefix</li> <li>Comments: allowed with <code>#</code></li> </ul>"},{"location":"data_models/#8-database-schema-mapping","title":"8. Database Schema Mapping","text":""},{"location":"data_models/#81-persistence-strategy","title":"8.1 Persistence Strategy","text":"<ul> <li>Entities: Map to database tables with primary key</li> <li>Value Objects: Embedded in parent table or separate table with foreign key</li> <li>Aggregates: Root table with cascade relationships to child tables</li> <li>Events: Event store table with JSON payload</li> </ul>"},{"location":"data_models/#82-table-naming-conventions","title":"8.2 Table Naming Conventions","text":"<ul> <li>Singular entity name</li> <li>Lowercase with underscores: <code>model_version</code></li> <li>Foreign key: <code>{entity}_id</code></li> <li>Junction tables: <code>{entity1}_{entity2}</code></li> </ul>"},{"location":"data_models/#83-index-strategy","title":"8.3 Index Strategy","text":"<p>Primary indexes:</p> <ul> <li>Primary key (clustered): <code>id</code></li> <li>Unique constraints: <code>model_id</code>, <code>version</code> combinations</li> <li>Foreign key indexes: All foreign key columns</li> </ul> <p>Secondary indexes:</p> <ul> <li>Frequently queried fields: <code>task_type</code>, <code>status</code>, <code>created_at</code></li> <li>Composite indexes: <code>(model_id, version)</code>, <code>(task_type, status)</code></li> </ul>"},{"location":"data_models/#9-model-evolution-strategy","title":"9. Model Evolution Strategy","text":""},{"location":"data_models/#91-versioning-approach","title":"9.1 Versioning Approach","text":"<ul> <li>Minor Changes: Add optional fields, maintain backward compatibility</li> <li>Major Changes: Require version increment, migration path</li> <li>Deprecation: Mark fields as deprecated, maintain for 2 versions</li> </ul>"},{"location":"data_models/#92-migration-guidelines","title":"9.2 Migration Guidelines","text":"<ol> <li>Adding Fields:</li> <li>Always add as optional</li> <li>Provide default values</li> <li> <p>Document in changelog</p> </li> <li> <p>Removing Fields:</p> </li> <li>Deprecate first (1-2 versions)</li> <li>Provide migration script</li> <li> <p>Update API documentation</p> </li> <li> <p>Changing Types:</p> </li> <li>Create new field with new type</li> <li>Migrate data</li> <li>Deprecate old field</li> <li>Remove in next major version</li> </ol>"},{"location":"data_models/#93-backward-compatibility","title":"9.3 Backward Compatibility","text":"<ul> <li>REST API: Support previous version URLs</li> <li>gRPC: Use field numbers, never reuse</li> <li>Database: Use migrations with up/down scripts</li> <li>Events: Version field in event payload</li> </ul>"},{"location":"data_models/#10-cross-references","title":"10. Cross-References","text":""},{"location":"data_models/#101-related-documents","title":"10.1 Related Documents","text":"<ul> <li>System Architecture - Component interactions</li> <li>API Design - API contracts and schemas</li> <li>Requirements - Functional requirements</li> </ul>"},{"location":"data_models/#102-implementation-guidelines","title":"10.2 Implementation Guidelines","text":"<ul> <li>All models defined in <code>src/modelmora/{context}/domain/</code></li> <li>DTOs defined in <code>src/modelmora/{context}/application/dto/</code></li> <li>ORM models in <code>src/modelmora/{context}/infrastructure/sql/models/</code></li> <li>Proto definitions in <code>protos/modelmora/v1/</code></li> </ul>"},{"location":"data_models/#11-model-documentation-index","title":"11. Model Documentation Index","text":"<p>Below is the complete index of all data model documentation files:</p>"},{"location":"data_models/#registry-context","title":"Registry Context","text":"<ul> <li>Model Entity</li> <li>Model Version</li> <li>Model Lock</li> <li>Model Catalog Aggregate</li> <li>Task Type</li> <li>Resource Requirements</li> </ul>"},{"location":"data_models/#lifecycle-context","title":"Lifecycle Context","text":"<ul> <li>Loaded Model</li> <li>Model Process</li> <li>Model Lifecycle Manager</li> <li>Model State</li> <li>Memory Usage</li> <li>Health Status</li> </ul>"},{"location":"data_models/#inference-context","title":"Inference Context","text":"<ul> <li>Inference Request</li> <li>Inference Job</li> <li>Batch</li> <li>Inference Queue Aggregate</li> <li>Job Status</li> <li>Priority</li> <li>Input Data</li> <li>Output Data</li> </ul>"},{"location":"data_models/#observability-context","title":"Observability Context","text":"<ul> <li>Metric</li> <li>Log Entry</li> <li>Trace</li> </ul>"},{"location":"data_models/#cross-cutting","title":"Cross-Cutting","text":"<ul> <li>API DTOs</li> <li>Domain Events</li> <li>Error Models</li> </ul>"},{"location":"data_models/#12-revision-history","title":"12. Revision History","text":"Version Date Author Changes 1.0.0 2025-12-04 - Initial data models documentation"},{"location":"design-decisions/","title":"Design Decisions","text":"<p>Key architectural and technical decisions made during ModelMora development.</p>"},{"location":"design-decisions/#poc-validated-decisions","title":"POC-Validated Decisions","text":""},{"location":"design-decisions/#multi-process-worker-architecture","title":"Multi-Process Worker Architecture","text":"<p>Decision: One model per subprocess with process termination for cleanup.</p> <p>Rationale: POC 1 demonstrated subprocess approach provides 5000x better memory reclamation than Python GC (0.1MB vs 516MB leak).</p> <p>Trade-offs:</p> <ul> <li>\u2705 Excellent memory isolation</li> <li>\u2705 GPU memory fully reclaimed</li> <li>\u2705 Crash isolation per model</li> <li>\u274c Higher process spawn overhead (~2s load time)</li> <li>\u274c IPC complexity</li> </ul>"},{"location":"design-decisions/#asynciopriorityqueue","title":"asyncio.PriorityQueue","text":"<p>Decision: Use <code>asyncio.PriorityQueue</code> for request scheduling.</p> <p>Rationale: POC 4 showed 730,108 ops/sec throughput (730x headroom above 1,000 req/sec target) with 0.7\u03bcs enqueue latency.</p> <p>Trade-offs:</p> <ul> <li>\u2705 Native async/await integration</li> <li>\u2705 Excellent performance (730k ops/sec)</li> <li>\u2705 Priority ordering guarantee</li> <li>\u2705 No external dependencies</li> <li>\u274c Not distributed (single-node only for MVP)</li> </ul>"},{"location":"design-decisions/#lazy-loading-strategy","title":"Lazy Loading Strategy","text":"<p>Decision: Load models on first request, not at startup.</p> <p>Rationale: POC 3 showed 2s cached load time is acceptable, allowing dynamic model management without upfront memory cost.</p> <p>Trade-offs:</p> <ul> <li>\u2705 Lower memory footprint at startup</li> <li>\u2705 Flexible model catalog</li> <li>\u2705 Pay-per-use resource allocation</li> <li>\u274c First request latency penalty</li> <li>\u274c Requires warmup for latency-sensitive workloads</li> </ul>"},{"location":"design-decisions/#grpc-and-rest-dual-protocol","title":"gRPC and REST Dual Protocol","text":"<p>Decision: Support both gRPC (streaming) and REST (simple queries).</p> <p>Rationale: POC 2 showed gRPC 31.92 MB/s throughput suitable for large payloads, while REST simpler for basic requests.</p> <p>Trade-offs:</p> <ul> <li>\u2705 Flexibility for different clients</li> <li>\u2705 Streaming support for large outputs</li> <li>\u2705 REST simplicity for testing</li> <li>\u274c Dual API surface to maintain</li> <li>\u274c Protocol translation overhead</li> </ul>"},{"location":"design-decisions/#technology-choices","title":"Technology Choices","text":""},{"location":"design-decisions/#fastapi","title":"FastAPI","text":"<p>Why: Async-first, automatic OpenAPI docs, Pydantic integration, excellent performance.</p> <p>Alternatives Considered: Flask (sync), Starlette (lower-level).</p>"},{"location":"design-decisions/#sqlalchemy-alembic","title":"SQLAlchemy + Alembic","text":"<p>Why: Mature ORM, migration support, PostgreSQL compatibility.</p> <p>Alternatives Considered: Raw SQL, Tortoise ORM, Django ORM.</p>"},{"location":"design-decisions/#poetry","title":"Poetry","text":"<p>Why: Modern dependency management, deterministic builds, PEP 517/621 compliance.</p> <p>Alternatives Considered: pip + requirements.txt, pipenv, PDM.</p>"},{"location":"design-decisions/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Overview</li> <li>Requirements</li> <li>Roadmap</li> </ul>"},{"location":"requirements/","title":"ModelMora - Requirements Documentation","text":"<p>Version: 1.0.0 Date: 2025-12-04 Status: Draft</p>"},{"location":"requirements/#1-introduction","title":"1. Introduction","text":""},{"location":"requirements/#11-purpose","title":"1.1 Purpose","text":"<p>This document specifies the functional and non-functional requirements for ModelMora, a lightweight model serving framework designed to efficiently deploy and manage machine learning models in a containerized environment.</p>"},{"location":"requirements/#12-scope","title":"1.2 Scope","text":"<p>ModelMora provides:</p> <ul> <li>Centralized model registry with version management</li> <li>Dynamic model lifecycle orchestration</li> <li>High-performance inference via REST and gRPC APIs</li> <li>Priority-based request scheduling with batching</li> <li>Multi-process memory isolation for Python memory management</li> </ul>"},{"location":"requirements/#13-context","title":"1.3 Context","text":"<p>ModelMora is part of the MiraVeja ecosystem, serving as a dedicated node for neural network operations including image processing, embedding generation, and text/image generation tasks.</p>"},{"location":"requirements/#14-definitions-acronyms","title":"1.4 Definitions &amp; Acronyms","text":"<ul> <li>LRU: Least Recently Used (eviction policy)</li> <li>gRPC: Google Remote Procedure Call</li> <li>HF: HuggingFace</li> <li>MVP: Minimum Viable Product</li> <li>IPC: Inter-Process Communication</li> </ul>"},{"location":"requirements/#2-user-personas","title":"2. User Personas","text":""},{"location":"requirements/#21-data-scientist","title":"2.1 Data Scientist","text":"<ul> <li>Downloads and registers models for experimentation</li> <li>Tests model inference via CLI or API</li> <li>Monitors model performance metrics</li> </ul>"},{"location":"requirements/#22-application-developer-miraveja-team","title":"2.2 Application Developer (MiraVeja Team)","text":"<ul> <li>Integrates ModelMora into application architecture</li> <li>Calls gRPC APIs for inference requests</li> <li>Handles async responses and error cases</li> </ul>"},{"location":"requirements/#23-devops-engineer","title":"2.3 DevOps Engineer","text":"<ul> <li>Deploys ModelMora in containerized environments</li> <li>Configures resource limits and scaling policies</li> <li>Monitors system health and performance</li> </ul>"},{"location":"requirements/#3-functional-requirements","title":"3. Functional Requirements","text":""},{"location":"requirements/#31-model-registry","title":"3.1 Model Registry","text":""},{"location":"requirements/#311-model-metadata-management","title":"3.1.1 Model Metadata Management","text":"<ul> <li>REQ-REG-001: The system SHALL maintain a centralized registry of available models with metadata including:</li> <li>Model identifier (HuggingFace ID)</li> <li>Version string (semantic versioning)</li> <li>Task type (txt2embed, img2embed, txt2img, img2txt, txt2txt)</li> <li>Resource requirements (memory, GPU VRAM)</li> <li>Download source URL</li> <li>Model file checksums</li> <li> <p>Custom configuration parameters</p> </li> <li> <p>REQ-REG-002: The system SHALL support PyTorch models exclusively in the initial release.</p> </li> <li> <p>REQ-REG-003: The system SHALL validate model metadata against a predefined schema during registration.</p> </li> </ul>"},{"location":"requirements/#312-model-versioning","title":"3.1.2 Model Versioning","text":"<ul> <li> <p>REQ-REG-004: The system SHALL support multiple versions of the same model simultaneously.</p> </li> <li> <p>REQ-REG-005: The system SHALL generate a lock file (similar to <code>poetry.lock</code>) containing:</p> </li> <li>Pinned model versions</li> <li>Download URLs</li> <li>Checksums for integrity verification</li> <li> <p>Last updated timestamp</p> </li> <li> <p>REQ-REG-006: Users SHALL be able to upgrade, downgrade, or pin model versions.</p> </li> </ul>"},{"location":"requirements/#313-model-discovery-downloading","title":"3.1.3 Model Discovery &amp; Downloading","text":"<ul> <li> <p>REQ-REG-007: The system SHALL download models on-demand from HuggingFace Hub when requested.</p> </li> <li> <p>REQ-REG-008: The system SHALL cache downloaded models in a persistent volume to avoid redundant downloads.</p> </li> <li> <p>REQ-REG-009: The system SHALL support custom model sources (local filesystem, S3, custom HTTP endpoints) in addition to HuggingFace.</p> </li> <li> <p>REQ-REG-010: The system SHALL provide a curated model configuration file to supplement missing HuggingFace metadata (task type, vector dimensions, etc.).</p> </li> </ul>"},{"location":"requirements/#314-model-crud-operations","title":"3.1.4 Model CRUD Operations","text":"<ul> <li>REQ-REG-011: Users SHALL be able to:</li> <li>Register new models</li> <li>List available models with filtering by task type</li> <li>Retrieve detailed model information</li> <li>Update model configurations</li> <li>Delete models and their cached files</li> </ul>"},{"location":"requirements/#32-model-lifecycle-management","title":"3.2 Model Lifecycle Management","text":""},{"location":"requirements/#321-lazy-loading","title":"3.2.1 Lazy Loading","text":"<ul> <li>REQ-LIFE-001: The system SHALL implement lazy loading, loading models into memory only when the first inference request is received.</li> <li>Validated: Load time 2s for small-medium models (90-420MB) from cache - acceptable for lazy loading</li> <li> <p>Performance: First inference adds 150ms warmup, subsequent inferences &lt;10ms</p> </li> <li> <p>REQ-LIFE-002: The system SHALL provide a warmup endpoint to pre-load frequently used models.</p> </li> <li> <p>Use case: Pre-warm models at startup to eliminate first-request latency</p> </li> <li> <p>REQ-LIFE-003: The system SHALL support configurable model preloading on service startup.</p> </li> <li>Guidance: Pre-load if load time &gt;5s or first inference &gt;500ms</li> </ul>"},{"location":"requirements/#322-memory-management-eviction","title":"3.2.2 Memory Management &amp; Eviction","text":"<ul> <li> <p>REQ-LIFE-004: The system SHALL implement an LRU (Least Recently Used) eviction policy when memory limits are reached.</p> </li> <li> <p>REQ-LIFE-005: The system SHALL use multi-process architecture where each model runs in a separate Python process to ensure complete memory cleanup on unload.</p> </li> <li>Rationale: POC demonstrated subprocess achieves 5000x better memory reclamation (0.1MB leak vs 516MB with GC)</li> <li> <p>Implementation: Kill subprocess to unload, not <code>del</code> + <code>gc.collect()</code></p> </li> <li> <p>REQ-LIFE-006: The system SHALL monitor:</p> </li> <li>Total system memory usage</li> <li>Per-model memory consumption (typical: 90-500MB per model)</li> <li>GPU VRAM utilization</li> <li> <p>Model access timestamps</p> </li> <li> <p>REQ-LIFE-007: The system SHALL support configurable memory thresholds to trigger model eviction.</p> </li> <li> <p>Guidance: Reserve 1.5x model footprint per worker (e.g., 500MB model \u2192 750MB limit)</p> </li> <li> <p>REQ-LIFE-008: The system SHALL explicitly call <code>torch.cuda.empty_cache()</code> after model unloading on GPU instances.</p> </li> <li>Note: Even with GPU cache clearing, subprocess termination is required for complete cleanup</li> </ul>"},{"location":"requirements/#323-health-checks","title":"3.2.3 Health Checks","text":"<ul> <li> <p>REQ-LIFE-009: The system SHALL verify model integrity after loading by running a test inference.</p> </li> <li> <p>REQ-LIFE-010: The system SHALL mark models as unhealthy if loading or test inference fails, and SHALL NOT route requests to unhealthy models.</p> </li> <li> <p>REQ-LIFE-011: The system SHALL support automatic retry of failed model loads with exponential backoff.</p> </li> </ul>"},{"location":"requirements/#33-request-queue-scheduling","title":"3.3 Request Queue &amp; Scheduling","text":""},{"location":"requirements/#331-priority-queue","title":"3.3.1 Priority Queue","text":"<ul> <li>REQ-QUEUE-001: The system SHALL implement a priority queue for inference requests with the following priority levels:</li> <li>CRITICAL (1): SLA-bound, real-time inference</li> <li>HIGH (2): User-facing, interactive requests</li> <li>NORMAL (3): Batch processing, standard workload</li> <li>LOW (4): Background tasks, analytics</li> <li> <p>Validated: asyncio.PriorityQueue provides 730,108 ops/sec with 0.7\u03bcs latency and 100% priority correctness</p> </li> <li> <p>REQ-QUEUE-002: The system SHALL group requests by target model to minimize load/unload cycles.</p> </li> <li> <p>Rationale: Model loading takes 2s, grouping amortizes this cost</p> </li> <li> <p>REQ-QUEUE-003: The system SHALL support configurable request timeout values per task type.</p> </li> <li> <p>REQ-QUEUE-004: The system SHALL implement request cancellation when timeouts are exceeded.</p> </li> <li>Performance: Queue overhead (0.7\u03bcs) negligible vs model execution (50-500ms)</li> </ul>"},{"location":"requirements/#332-batching","title":"3.3.2 Batching","text":"<ul> <li> <p>REQ-QUEUE-005: The system SHALL accumulate requests for the same model into batches to improve throughput.</p> </li> <li> <p>REQ-QUEUE-006: The system SHALL support configurable batch parameters:</p> </li> <li>Maximum batch size</li> <li>Maximum batch wait time</li> <li> <p>Per-model batch size overrides</p> </li> <li> <p>REQ-QUEUE-007: The system SHALL preprocess batches (padding, normalization) before inference execution.</p> </li> </ul>"},{"location":"requirements/#333-load-balancing","title":"3.3.3 Load Balancing","text":"<ul> <li>REQ-QUEUE-008: In multi-node deployments, the system SHALL distribute requests across available ModelMora instances.</li> </ul>"},{"location":"requirements/#34-inference-engine","title":"3.4 Inference Engine","text":""},{"location":"requirements/#341-execution","title":"3.4.1 Execution","text":"<ul> <li>REQ-INFER-001: The system SHALL execute model inference using the appropriate task-specific pipeline:</li> <li>Text embedding generation</li> <li>Image embedding generation</li> <li>Text-to-image generation</li> <li>Image-to-text generation (captioning)</li> <li> <p>Text-to-text generation</p> </li> <li> <p>REQ-INFER-002: The system SHALL support both CPU and GPU execution, with automatic device selection based on availability.</p> </li> <li> <p>REQ-INFER-003: The system SHALL handle inference errors gracefully and return meaningful error messages to clients.</p> </li> </ul>"},{"location":"requirements/#342-asynchronous-processing","title":"3.4.2 Asynchronous Processing","text":"<ul> <li>REQ-INFER-004: The system SHALL support asynchronous inference requests with:</li> <li>Unique job ID generation</li> <li>Job status tracking (queued, processing, completed, failed)</li> <li> <p>Result retrieval by job ID</p> </li> <li> <p>REQ-INFER-005: The system SHALL support synchronous inference for low-latency use cases.</p> </li> </ul>"},{"location":"requirements/#343-result-storage","title":"3.4.3 Result Storage","text":"<ul> <li> <p>REQ-INFER-006: For small results (embeddings, short text), the system SHALL return results inline in the API response.</p> </li> <li> <p>REQ-INFER-007: For large results (generated images), the system SHALL:</p> </li> <li>Store results in object storage (S3/MinIO)</li> <li>Return a URI or presigned URL to the client</li> <li> <p>Support configurable result expiration</p> </li> <li> <p>REQ-INFER-008: The system SHALL support both storage strategies via configuration.</p> </li> </ul>"},{"location":"requirements/#35-observability","title":"3.5 Observability","text":""},{"location":"requirements/#351-metrics","title":"3.5.1 Metrics","text":"<ul> <li>REQ-OBS-001: The system SHALL expose Prometheus-compatible metrics including:</li> <li>Request latency (histogram) by model and task type</li> <li>Request throughput (counter) by status (success, error)</li> <li>Queue depth (gauge) by priority level</li> <li>Model load/unload events (counter)</li> <li>Active models count (gauge)</li> <li>Memory usage (gauge) - system and per-model</li> <li>GPU utilization (gauge)</li> <li>Batch size distribution (histogram)</li> </ul>"},{"location":"requirements/#352-logging","title":"3.5.2 Logging","text":"<ul> <li> <p>REQ-OBS-002: The system SHALL implement structured logging (JSON format) with configurable log levels.</p> </li> <li> <p>REQ-OBS-003: The system SHALL log:</p> </li> <li>Model lifecycle events (load, unload, eviction)</li> <li>Request processing (received, queued, started, completed)</li> <li>Errors with stack traces</li> <li> <p>Performance warnings (slow requests, queue saturation)</p> </li> <li> <p>REQ-OBS-004: The system SHALL support distributed tracing with trace ID propagation.</p> </li> </ul>"},{"location":"requirements/#353-health-endpoints","title":"3.5.3 Health Endpoints","text":"<ul> <li>REQ-OBS-005: The system SHALL provide:</li> <li><code>/health/live</code> - Liveness probe (process running)</li> <li><code>/health/ready</code> - Readiness probe (can accept requests)</li> <li><code>/health/models</code> - Status of each loaded model</li> </ul>"},{"location":"requirements/#4-api-requirements","title":"4. API Requirements","text":""},{"location":"requirements/#41-rest-api-fastapi","title":"4.1 REST API (FastAPI)","text":""},{"location":"requirements/#411-model-registry-endpoints","title":"4.1.1 Model Registry Endpoints","text":"<ul> <li>POST <code>/api/v1/models</code> - Register a new model</li> <li>GET <code>/api/v1/models</code> - List all models (with filtering)</li> <li>GET <code>/api/v1/models/{model_id}</code> - Get model details</li> <li>PUT <code>/api/v1/models/{model_id}</code> - Update model configuration</li> <li>DELETE <code>/api/v1/models/{model_id}</code> - Delete model</li> <li>POST <code>/api/v1/models/{model_id}/download</code> - Trigger model download</li> <li>POST <code>/api/v1/models/{model_id}/versions/{version}</code> - Set active version</li> </ul>"},{"location":"requirements/#412-inference-endpoints","title":"4.1.2 Inference Endpoints","text":"<ul> <li>POST <code>/api/v1/infer/{model_id}</code> - Synchronous inference</li> <li>POST <code>/api/v1/infer/{model_id}/async</code> - Asynchronous inference (returns job ID)</li> <li>GET <code>/api/v1/jobs/{job_id}</code> - Get job status</li> <li>GET <code>/api/v1/jobs/{job_id}/result</code> - Retrieve job result</li> </ul>"},{"location":"requirements/#413-management-endpoints","title":"4.1.3 Management Endpoints","text":"<ul> <li>POST <code>/api/v1/models/{model_id}/load</code> - Force model load</li> <li>POST <code>/api/v1/models/{model_id}/unload</code> - Force model unload</li> <li>GET <code>/api/v1/status</code> - System status and loaded models</li> <li>GET <code>/api/v1/metrics</code> - Prometheus metrics endpoint</li> </ul>"},{"location":"requirements/#414-health-endpoints","title":"4.1.4 Health Endpoints","text":"<ul> <li>GET <code>/health/live</code> - Liveness probe</li> <li>GET <code>/health/ready</code> - Readiness probe</li> <li>GET <code>/health/models</code> - Model health status</li> </ul>"},{"location":"requirements/#42-grpc-service","title":"4.2 gRPC Service","text":""},{"location":"requirements/#421-service-definition","title":"4.2.1 Service Definition","text":"<ul> <li>REQ-GRPC-001: The system SHALL provide a gRPC service with Protocol Buffer definitions for:</li> <li><code>InferenceService</code> - Model inference operations</li> <li><code>RegistryService</code> - Model registry operations</li> <li><code>ManagementService</code> - Lifecycle management operations</li> </ul>"},{"location":"requirements/#422-inference-methods","title":"4.2.2 Inference Methods","text":"<ul> <li><code>Infer(InferenceRequest) returns (InferenceResponse)</code> - Unary inference</li> <li><code>InferAsync(InferenceRequest) returns (JobReference)</code> - Async inference</li> <li><code>InferStream(stream InferenceRequest) returns (stream InferenceResponse)</code> - Streaming inference</li> </ul>"},{"location":"requirements/#423-client-sdk","title":"4.2.3 Client SDK","text":"<ul> <li>REQ-GRPC-002: The system SHALL provide a Python client SDK for gRPC communication.</li> </ul>"},{"location":"requirements/#43-cli-tool","title":"4.3 CLI Tool","text":""},{"location":"requirements/#431-commands","title":"4.3.1 Commands","text":"<ul> <li><code>modelmora init [--config CONFIG_PATH]</code> - Initialize project with config template</li> <li><code>modelmora install &lt;model_id&gt;[@version]</code> - Download and register model</li> <li><code>modelmora list [--task TASK_TYPE]</code> - List installed models</li> <li><code>modelmora info &lt;model_id&gt;</code> - Show model details</li> <li><code>modelmora lock</code> - Generate/update lock file</li> <li><code>modelmora serve [--config CONFIG_PATH]</code> - Start ModelMora server</li> <li><code>modelmora health</code> - Check server health</li> <li><code>modelmora uninstall &lt;model_id&gt;</code> - Remove model</li> </ul>"},{"location":"requirements/#5-non-functional-requirements","title":"5. Non-Functional Requirements","text":""},{"location":"requirements/#51-performance","title":"5.1 Performance","text":"<ul> <li> <p>REQ-PERF-001: The system SHALL add &lt;100ms overhead to model inference time (excluding model loading).</p> </li> <li> <p>REQ-PERF-002: The system SHALL support at least 10 concurrent requests in single-node deployment.</p> </li> <li> <p>REQ-PERF-003: Model loading time SHALL be &lt;30 seconds for models up to 5GB in size.</p> </li> <li> <p>REQ-PERF-004: The system SHALL achieve &gt;80% GPU utilization during peak load when batching is enabled.</p> </li> </ul>"},{"location":"requirements/#52-scalability","title":"5.2 Scalability","text":"<ul> <li> <p>REQ-SCALE-001: The system SHALL support at least 20 different models registered in the catalog.</p> </li> <li> <p>REQ-SCALE-002: The system SHALL support at least 5 models loaded simultaneously (memory permitting).</p> </li> <li> <p>REQ-SCALE-003: The system SHALL scale horizontally to multiple nodes behind a load balancer.</p> </li> </ul>"},{"location":"requirements/#53-reliability","title":"5.3 Reliability","text":"<ul> <li> <p>REQ-REL-001: The system SHALL achieve 99.9% uptime in production environments.</p> </li> <li> <p>REQ-REL-002: The system SHALL gracefully handle model loading failures without crashing.</p> </li> <li> <p>REQ-REL-003: The system SHALL implement request retry logic with exponential backoff for transient failures.</p> </li> <li> <p>REQ-REL-004: The system SHALL persist request queue state to survive process restarts (for async requests).</p> </li> </ul>"},{"location":"requirements/#54-resource-constraints","title":"5.4 Resource Constraints","text":"<ul> <li> <p>REQ-RES-001: The system SHALL run with &lt;4GB base memory (excluding loaded models).</p> </li> <li> <p>REQ-RES-002: The system SHALL respect configurable memory limits and SHALL NOT exceed them.</p> </li> <li> <p>REQ-RES-003: The system SHALL support GPU sharing between multiple models when VRAM permits.</p> </li> </ul>"},{"location":"requirements/#55-security","title":"5.5 Security","text":"<ul> <li> <p>REQ-SEC-001: The system SHALL validate all input data against expected schemas to prevent injection attacks.</p> </li> <li> <p>REQ-SEC-002: The system SHALL verify model file checksums after download to prevent tampering.</p> </li> <li> <p>REQ-SEC-003: The system SHALL support TLS/SSL for gRPC and HTTPS for REST APIs.</p> </li> <li> <p>REQ-SEC-004: The system SHALL support API authentication (token-based) for production deployments.</p> </li> </ul>"},{"location":"requirements/#56-maintainability","title":"5.6 Maintainability","text":"<ul> <li> <p>REQ-MAINT-001: The codebase SHALL maintain &gt;90% test coverage.</p> </li> <li> <p>REQ-MAINT-002: The system SHALL use type hints throughout the codebase for static analysis.</p> </li> <li> <p>REQ-MAINT-003: The system SHALL provide comprehensive API documentation using OpenAPI/Swagger.</p> </li> <li> <p>REQ-MAINT-004: The system SHALL follow PEP 8 coding standards with automated linting.</p> </li> </ul>"},{"location":"requirements/#57-deployability","title":"5.7 Deployability","text":"<ul> <li> <p>REQ-DEPLOY-001: The system SHALL be packaged as a Docker image.</p> </li> <li> <p>REQ-DEPLOY-002: The system SHALL provide Kubernetes deployment manifests (YAML/Helm chart).</p> </li> <li> <p>REQ-DEPLOY-003: The system SHALL support configuration via environment variables and config files.</p> </li> <li> <p>REQ-DEPLOY-004: The system SHALL support volume mounts for model cache persistence.</p> </li> </ul>"},{"location":"requirements/#6-technical-constraints","title":"6. Technical Constraints","text":""},{"location":"requirements/#61-technology-stack","title":"6.1 Technology Stack","text":"<ul> <li>Python: 3.10 or higher</li> <li>Web Framework: FastAPI</li> <li>RPC Framework: gRPC (grpcio)</li> <li>Database: PostgreSQL</li> <li>Task Queue: In-memory (MVP), Redis (production)</li> <li>Object Storage: S3/MinIO</li> <li>Containerization: Docker, Kubernetes</li> </ul>"},{"location":"requirements/#62-model-support","title":"6.2 Model Support","text":"<ul> <li>Framework: PyTorch only (initial release)</li> <li>Source: HuggingFace Hub (primary), local files, custom URLs</li> <li>Task Types: txt2embed, img2embed, txt2img, img2txt, txt2txt</li> </ul>"},{"location":"requirements/#63-dependencies","title":"6.3 Dependencies","text":"<ul> <li>Core: torch, transformers, diffusers, sentence-transformers</li> <li>API: fastapi, grpcio, pydantic</li> <li>Storage: sqlalchemy, boto3 (S3), redis (optional)</li> <li>Observability: prometheus-client, structlog</li> </ul>"},{"location":"requirements/#7-data-models","title":"7. Data Models","text":""},{"location":"requirements/#71-model-metadata-schema","title":"7.1 Model Metadata Schema","text":"<pre><code>model_id: str  # e.g., \"sentence-transformers/all-MiniLM-L6-v2\"\nversion: str  # semantic version\ntask_type: enum  # txt2embed | img2embed | txt2img | img2txt | txt2txt\ndisplay_name: str\ndescription: str\nsource_url: str\nfile_checksum: str  # SHA256\nresource_requirements:\n  memory_mb: int\n  gpu_vram_mb: int\n  cpu_threads: int\nconfig:\n  vector_size: int  # for embedding models\n  max_length: int  # for text models\n  batch_size: int\n  device: enum  # cpu | cuda | auto\ncustom_params: dict  # arbitrary key-value pairs\ncreated_at: datetime\nupdated_at: datetime\n</code></pre>"},{"location":"requirements/#72-inference-request-schema","title":"7.2 Inference Request Schema","text":"<pre><code>model_id: str\ninput_data:\n  text: str | list[str]\n  image: str  # base64 or URL\n  # task-specific fields\nparameters:\n  temperature: float\n  max_tokens: int\n  # task-specific parameters\npriority: enum  # high | medium | low\ntimeout_seconds: int\n</code></pre>"},{"location":"requirements/#73-lock-file-format","title":"7.3 Lock File Format","text":"<pre><code># modelmora.lock\nversion: \"1.0\"\ngenerated_at: datetime\nmodels:\n  - model_id: str\n    version: str\n    source_url: str\n    checksum: str\n    task_type: str\n</code></pre>"},{"location":"requirements/#8-integration-requirements","title":"8. Integration Requirements","text":""},{"location":"requirements/#81-miraveja-integration","title":"8.1 MiraVeja Integration","text":"<ul> <li> <p>REQ-INT-001: ModelMora SHALL be callable from MiraVeja via gRPC.</p> </li> <li> <p>REQ-INT-002: MiraVeja SHALL handle async job polling with configurable retry intervals.</p> </li> <li> <p>REQ-INT-003: ModelMora SHALL return embeddings inline for MiraVeja image search features.</p> </li> </ul>"},{"location":"requirements/#82-kafka-integration-future","title":"8.2 Kafka Integration (Future)","text":"<ul> <li> <p>REQ-INT-004: The system SHALL optionally consume inference requests from Kafka topics.</p> </li> <li> <p>REQ-INT-005: The system SHALL publish inference results to Kafka response topics.</p> </li> <li> <p>REQ-INT-006: The system SHALL publish model lifecycle events to an audit topic.</p> </li> </ul>"},{"location":"requirements/#9-out-of-scope-future-releases","title":"9. Out of Scope (Future Releases)","text":"<ul> <li>ONNX Runtime support</li> <li>TensorRT optimization</li> <li>Model A/B testing</li> <li>Model fine-tuning capabilities</li> <li>Multi-tenant isolation</li> <li>Advanced authentication (OAuth, JWT)</li> <li>Web UI for management</li> <li>Model quantization</li> <li>Distributed training integration</li> </ul>"},{"location":"requirements/#10-acceptance-criteria","title":"10. Acceptance Criteria","text":""},{"location":"requirements/#101-mvp-v010","title":"10.1 MVP (v0.1.0)","text":"<ul> <li>[ ] Register at least 3 different models (embedding, text gen, image gen)</li> <li>[ ] Execute inference via REST API with &lt;100ms overhead</li> <li>[ ] Handle 10 concurrent requests without errors</li> <li>[ ] Models load lazily on first request</li> <li>[ ] Memory usage &lt;4GB excluding models</li> <li>[ ] Docker image builds successfully</li> <li>[ ] Unit test coverage &gt;90%</li> <li>[ ] API documentation available</li> </ul>"},{"location":"requirements/#102-production-ready-v100","title":"10.2 Production Ready (v1.0.0)","text":"<ul> <li>[ ] All requirements implemented</li> <li>[ ] gRPC service operational</li> <li>[ ] CLI tool with all commands</li> <li>[ ] Prometheus metrics exposed</li> <li>[ ] Kubernetes deployment successful</li> <li>[ ] Integration with MiraVeja complete</li> <li>[ ] 99.9% uptime in staging environment</li> <li>[ ] Load testing passed (100 req/s)</li> <li>[ ] Complete documentation published</li> </ul>"},{"location":"requirements/#11-revision-history","title":"11. Revision History","text":"Version Date Author Changes 1.0.0 2025-12-04 - Initial comprehensive requirements document"},{"location":"roadmap/","title":"ModelMora Development Roadmap","text":""},{"location":"roadmap/#phase-0-requirements-planning-2-3-weeks","title":"Phase 0: Requirements &amp; Planning (2-3 weeks)","text":""},{"location":"roadmap/#01-requirements-gathering","title":"0.1 Requirements Gathering","text":"<ul> <li>[x] Functional Requirements Document</li> <li>Define supported model types (embedding, text generation, image generation, etc.)</li> <li>Specify API contracts (REST endpoints, gRPC services)</li> <li>Define user personas (data scientists, application developers, DevOps)</li> <li> <p>List CLI commands and workflows</p> </li> <li> <p>[x] Non-Functional Requirements</p> </li> <li>Performance targets (latency, throughput)</li> <li>Resource constraints (memory limits, GPU sharing)</li> <li>Scalability requirements (concurrent requests, model count)</li> <li> <p>Reliability (uptime, error handling)</p> </li> <li> <p>[x] Technical Constraints</p> </li> <li>Python 3.10+</li> <li>GPU/CPU support matrix</li> <li>Container resource allocation</li> <li>Network bandwidth considerations</li> </ul>"},{"location":"roadmap/#02-architecture-design","title":"0.2 Architecture Design","text":"<ul> <li>[x] System Architecture Document</li> <li>Component diagram (Registry, Scheduler, Lifecycle Manager, Workers)</li> <li>Sequence diagrams for key workflows</li> <li>Data flow diagrams</li> <li> <p>Deployment architecture</p> </li> <li> <p>[x] API Design</p> </li> <li>OpenAPI specification for REST endpoints</li> <li>Protocol Buffer definitions for gRPC</li> <li> <p>Message schemas for Kafka (if used)</p> </li> <li> <p>[x] Data Models</p> </li> <li>Model metadata schema</li> <li>Request/response schemas</li> <li>Queue message format</li> <li>Lock file format</li> </ul>"},{"location":"roadmap/#03-technology-evaluation","title":"0.3 Technology Evaluation","text":"<ul> <li>[x] Proof of Concepts</li> <li>[x] Multi-process memory isolation test</li> <li>[x] gRPC streaming performance test</li> <li>[x] Model loading/unloading benchmark</li> <li>[x] Priority queue implementation comparison</li> </ul>"},{"location":"roadmap/#poc-results-summary","title":"POC Results Summary","text":"<p>POC 1: Multi-Process Memory Isolation \u2705</p> <ul> <li>Subprocess approach: 0.1MB RAM leak, 0MB GPU leak</li> <li>In-process GC: 516MB RAM leak, 9MB GPU leak</li> <li>Verdict: Multi-process architecture MANDATORY (5000x better memory reclamation)</li> <li>Decision: One model per subprocess, kill process to unload</li> </ul> <p>POC 2: gRPC Streaming Performance \u2705</p> <ul> <li>gRPC throughput: 31.92 MB/s single client, 33.31 MB/s concurrent (10 clients)</li> <li>Average latency: 23.53ms per chunk</li> <li>Verdict: gRPC performs well, both gRPC and REST viable</li> <li>Decision: Use gRPC for streaming large payloads (image generation), REST for simple queries</li> </ul> <p>POC 3: Model Loading/Unloading \u2705</p> <ul> <li>Load time (cached): 2s for small models (90MB), 2s for medium models (420MB)</li> <li>Memory reclamation (GC): 0.2% - essentially ineffective</li> <li>Memory leak: 12.4MB over 3 cycles</li> <li>Verdict: GC-based cleanup FAILS, subprocess isolation required</li> <li>Decision: Lazy loading viable (2s acceptable), subprocess mandatory for cleanup</li> </ul> <p>POC 4: Priority Queue Implementation \u2705</p> <ul> <li>asyncio.PriorityQueue: 730,108 ops/sec, 0.7\u03bcs enqueue latency</li> <li>Throughput headroom: 730x above target (1,000 req/sec)</li> <li>Priority correctness: 100% (0 violations)</li> <li>Verdict: Queue will never be bottleneck</li> <li>Decision: Use asyncio.PriorityQueue for MVP (async-native, excellent performance)</li> </ul> <p>Key Architecture Decisions Validated:</p> <ol> <li>\u2705 Multi-process worker architecture (one model per process)</li> <li>\u2705 Lazy loading on first request (2s load time acceptable)</li> <li>\u2705 asyncio.PriorityQueue for request scheduling</li> <li>\u2705 gRPC streaming for large inference results</li> <li>\u2705 Process termination for model cleanup (not GC)</li> </ol>"},{"location":"roadmap/#phase-1-mvp-core-4-6-weeks","title":"Phase 1: MVP Core (4-6 weeks)","text":""},{"location":"roadmap/#11-project-foundation-week-1","title":"1.1 Project Foundation (Week 1)","text":"<ul> <li>[x] Repository setup with Poetry</li> <li>Create <code>pyproject.toml</code> with metadata, dependencies, dev dependencies, scripts</li> <li>Initialize Poetry virtualenv and generate <code>poetry.lock</code></li> <li>Add <code>README.md</code>, license file, <code>.gitignore</code></li> <li>Configure <code>poetry run</code> scripts for common tasks (test, lint, serve)</li> <li> <p>Acceptance: <code>poetry install</code> reproduces environment; <code>poetry run pytest</code> executes</p> </li> <li> <p>[X] Project structure scaffolding</p> </li> <li>Create directory layout: <code>src/modelmora/</code>, <code>tests/unit/</code>, <code>tests/integration/</code>, <code>docs/</code>, <code>config/</code>, <code>examples/</code></li> <li>Add <code>__init__.py</code> to package and submodules (<code>registry</code>, <code>lifecycle</code>, <code>inference</code>, <code>observability</code>, <code>presentation</code>, <code>worker</code>, <code>shared</code>)</li> <li>Configure <code>src/</code> layout in <code>pyproject.toml</code> for proper imports</li> <li> <p>Acceptance: <code>from modelmora.{...} import Registry</code> works; structure matches conventions</p> </li> <li> <p>[X] CI/CD pipeline (GitHub Actions)</p> </li> <li>Create <code>.github/workflows/ci.yml</code> (lint + test on PR/push)</li> <li>Create <code>.github/workflows/docker-build.yml</code> (build and push on release)</li> <li>Add dependency caching for Poetry in workflows</li> <li>Create <code>deploy-docs.yml</code> for MkDocs to GitHub Pages</li> <li> <p>Acceptance: PR checks run and report status; Docker build completes successfully</p> </li> <li> <p>[X] Code quality tools (pylint, black, mypy)</p> </li> <li>Configure <code>black</code> in <code>pyproject.toml</code> (line length, target version)</li> <li>Configure <code>pylint</code> rules (select/ignore, extend-select) in <code>pyproject.toml</code></li> <li>Configure <code>mypy</code> strictness (disallow-untyped-defs, plugins) in <code>pyproject.toml</code> or <code>mypy.ini</code></li> <li>Add <code>.pre-commit-config.yaml</code> with black, pylint, mypy hooks</li> <li>Add lint job to CI workflow</li> <li> <p>Acceptance: <code>pre-commit run --all-files</code> passes; CI enforces checks</p> </li> <li> <p>[X] Testing framework (pytest)</p> </li> <li>Add <code>pytest.ini</code> or <code>[tool.pytest.ini_options]</code> in <code>pyproject.toml</code></li> <li>Create <code>conftest.py</code> with common fixtures (temp dirs, mock models, DB fixtures)</li> <li>Add sample unit tests for core modules</li> <li>Configure coverage measurement (<code>pytest-cov</code>) with &gt;90% threshold</li> <li> <p>Acceptance: <code>pytest</code> runs locally and in CI; coverage enforced in CI</p> </li> <li> <p>[X] Documentation site (MkDocs)</p> </li> <li>Create <code>mkdocs.yml</code> with site metadata and navigation structure</li> <li>Bootstrap initial docs pages: <code>getting-started.md</code>, <code>architecture.md</code>, <code>api-reference.md</code>, <code>deployment.md</code></li> <li>Add GitHub Pages deployment workflow (<code>.github/workflows/deploy-docs.yml</code>)</li> <li>Acceptance: <code>mkdocs serve</code> renders site locally; docs deployed on push to main</li> </ul>"},{"location":"roadmap/#12-model-registry-week-2","title":"1.2 Model Registry (Week 2)","text":"<ul> <li>[ ] Data Layer - SQLite database schema</li> <li>Design <code>models</code> table: id, name, version, path/url, config (JSON), created_by, created_at, state</li> <li>Create SQL schema file or migration script (simple SQL or <code>alembic</code> for versioning)</li> <li>Add indexes for common queries (name, version, state)</li> <li> <p>Acceptance: Schema created; DB operations persist and retrieve model records</p> </li> <li> <p>[ ] Data Layer - Model metadata CRUD operations</p> </li> <li>Implement <code>ModelRepository</code> class with methods: <code>create()</code>, <code>get()</code>, <code>update()</code>, <code>delete()</code>, <code>list()</code></li> <li>Add transactional safety and connection pooling</li> <li>Write unit tests for repository using in-memory SQLite</li> <li> <p>Acceptance: Repository tested in isolation; supports filtering by name/version/state</p> </li> <li> <p>[ ] Data Layer - Version tracking logic</p> </li> <li>Add DB columns and logic for version comparisons (<code>is_latest</code> flag or ordering by <code>created_at</code>)</li> <li>Implement API to promote versions or deprecate older ones</li> <li>Add query methods: <code>get_latest()</code>, <code>get_by_version()</code></li> <li> <p>Acceptance: Registry returns correct version based on query (explicit version vs latest)</p> </li> <li> <p>[ ] Registry Service - Model registration API</p> </li> <li>Implement POST <code>/models</code> endpoint to register models (local path or remote URL)</li> <li>Validate input and persist metadata using <code>ModelRepository</code></li> <li>Return 201 with created model metadata or 400 with validation errors</li> <li> <p>Acceptance: POST returns persisted record; validation errors return helpful messages</p> </li> <li> <p>[ ] Registry Service - Model discovery/listing</p> </li> <li>Implement GET <code>/models</code> endpoint with pagination and filters (tag, task, device)</li> <li>Implement GET <code>/models/{name}</code> to fetch specific model metadata</li> <li>Support query parameters for filtering and sorting</li> <li> <p>Acceptance: Clients can list and fetch model metadata; basic queries work</p> </li> <li> <p>[ ] Registry Service - Basic validation</p> </li> <li>Add Pydantic validators for required metadata fields</li> <li>Optional: Light probe to verify path/URL accessibility</li> <li>Return structured error responses for invalid registrations</li> <li> <p>Acceptance: Invalid registrations rejected with clear, actionable error messages</p> </li> <li> <p>[ ] Configuration Parser - YAML model definitions</p> </li> <li>Implement parser to read <code>config/models/*.yml</code> into Pydantic model objects</li> <li>Define example YAML schema with fields: name, version, source, task, device, config</li> <li>Add CLI command or boot-time loader to populate registry from YAML</li> <li> <p>Acceptance: Parser converts YAML to model metadata; batch registration works</p> </li> <li> <p>[ ] Configuration Parser - Environment variable support</p> </li> <li>Support <code>${ENV_VAR}</code> syntax for variable interpolation in YAML</li> <li>Add fallback/default value syntax (e.g., <code>${VAR:-default}</code>)</li> <li>Raise clear errors for missing required environment variables</li> <li>Acceptance: Configs load with variables replaced; missing vars produce helpful errors</li> </ul>"},{"location":"roadmap/#13-basic-inference-engine-week-3-4","title":"1.3 Basic Inference Engine (Week 3-4)","text":"<ul> <li>[ ] Model Loader - HuggingFace integration</li> <li>Implement <code>ModelLoader</code> interface with <code>load(model_meta)</code> method</li> <li>Support HuggingFace <code>transformers</code> and <code>sentence-transformers</code> using <code>from_pretrained()</code></li> <li>Configure <code>cache_dir</code> for downloaded models</li> <li>Return <code>ModelHandle</code> exposing <code>.infer()</code> or <code>.encode()</code> based on model type</li> <li>Add mock loader for unit tests</li> <li> <p>Acceptance: Loader instantiates small model in dev; logs memory and timing metrics</p> </li> <li> <p>[ ] Model Loader - Local file support</p> </li> <li>Support <code>file://</code> paths or direct local directory paths</li> <li>Verify file structure and provide informative errors for missing files</li> <li>Add tests for loading from local directories</li> <li> <p>Acceptance: Local models load successfully; test coverage for file validation</p> </li> <li> <p>[ ] Model Loader - Basic caching mechanism</p> </li> <li>Implement in-process cache keyed by model id/version with TTL or LRU policy</li> <li>Avoid redundant loads when model already in memory</li> <li>Expose metrics on cache hits/misses</li> <li> <p>Acceptance: Repeated load requests hit cache; metrics logged or exposed</p> </li> <li> <p>[ ] Worker Process - Single model worker implementation</p> </li> <li>Implement worker process entrypoint accepting control messages (load/unload/health)</li> <li>Accept inference requests over IPC (<code>multiprocessing.Connection</code>, HTTP, or socket)</li> <li>Implement message protocol for request/response serialization</li> <li> <p>Acceptance: Worker loads model and serves requests; main process controls lifecycle</p> </li> <li> <p>[ ] Worker Process - Process spawning/cleanup</p> </li> <li>Implement supervisor logic to spawn worker subprocess with <code>multiprocessing.Process</code></li> <li>Monitor worker heartbeat and collect exit codes</li> <li>Implement graceful shutdown with timeout and forced kill for stuck processes</li> <li>Add tests for clean shutdown and forced termination scenarios</li> <li> <p>Acceptance: Worker termination frees memory (validated by POC); supervisor handles failures</p> </li> <li> <p>[ ] Worker Process - Basic inference execution</p> </li> <li>Implement worker API method: <code>infer(payload)</code> returning serializable outputs</li> <li>Add request queueing within worker and basic timeout handling</li> <li>Support synchronous inference for MVP</li> <li> <p>Acceptance: <code>/infer/{model_name}</code> calls worker and returns results within expected latency</p> </li> <li> <p>[ ] Memory Management - Process isolation verification</p> </li> <li>Add integration tests measuring RSS before/after load/unload with <code>psutil</code></li> <li>Verify memory reclaimed to baseline after worker termination</li> <li>Document test results and compare against POC benchmarks</li> <li> <p>Acceptance: Memory reclamation meets POC targets (subprocess ~0MB leak vs GC ~500MB leak)</p> </li> <li> <p>[ ] Memory Management - GPU memory cleanup</p> </li> <li>Add hooks to clear <code>torch.cuda</code> state before worker exit</li> <li>Force process termination to release GPU contexts</li> <li>Optional: Add <code>nvidia-smi</code> checks in integration tests for dev verification</li> <li> <p>Acceptance: GPU memory freed after worker exit (observed in integration test or manual run)</p> </li> <li> <p>[ ] Memory Management - Resource monitoring</p> </li> <li>Implement lightweight monitor using <code>psutil</code> for CPU/RAM and <code>pynvml</code> for GPU</li> <li>Expose metrics via Prometheus client or structured logs</li> <li>Add alerts/thresholds for memory pressure</li> <li>Acceptance: Supervisor logs per-worker resource usage; alerts trigger on threshold breach</li> </ul>"},{"location":"roadmap/#14-api-layer-week-5","title":"1.4 API Layer (Week 5)","text":"<ul> <li>[ ] REST API (FastAPI) - <code>/health</code> endpoint</li> <li>Implement <code>/health</code> returning app status, DB connectivity, worker status</li> <li>Add optional <code>/ready</code> for Kubernetes-style readiness checks</li> <li>Ensure health checks are fast (&lt;50ms) and safe (no side effects)</li> <li> <p>Acceptance: Health endpoints return correct status; suitable for liveness/readiness probes</p> </li> <li> <p>[ ] REST API (FastAPI) - <code>/models</code> - list available models</p> </li> <li>Implement GET <code>/models</code> returning paginated list with optional filters</li> <li>Implement GET <code>/models/{name}</code> returning specific model metadata</li> <li>Support query parameters: <code>task</code>, <code>tag</code>, <code>device</code>, <code>page</code>, <code>limit</code></li> <li> <p>Acceptance: Endpoints documented in OpenAPI; tests verify JSON schema</p> </li> <li> <p>[ ] REST API (FastAPI) - <code>/infer/{model_name}</code> - synchronous inference</p> </li> <li>Implement POST <code>/infer/{model_name}</code> accepting input payload</li> <li>Forward request to appropriate worker (or load if not cached)</li> <li>Return inference output with metadata (timing, model version)</li> <li>Handle errors: model not found (404), timeout (504), internal error (500)</li> <li> <p>Acceptance: Endpoint returns inference JSON; appropriate HTTP codes for errors</p> </li> <li> <p>[ ] Request Validation - Pydantic models for inputs</p> </li> <li>Define Pydantic schemas for common request types: <code>TextRequest</code>, <code>EmbeddingRequest</code>, <code>ImageRequest</code></li> <li>Define response schemas with fields: <code>result</code>, <code>metadata</code>, <code>timing</code></li> <li>Add validators for input constraints (max length, format checks)</li> <li> <p>Acceptance: Invalid payloads return 422 with detailed validation errors</p> </li> <li> <p>[ ] Request Validation - Error handling</p> </li> <li>Implement centralized exception handlers for validation, not-found, timeout, internal errors</li> <li>Map exceptions to appropriate HTTP status codes</li> <li>Return structured JSON error bodies: <code>{\"error\": \"...\", \"detail\": \"...\", \"code\": \"...\"}</code></li> <li> <p>Acceptance: Clients receive consistent, actionable error messages</p> </li> <li> <p>[ ] Request Validation - Response serialization</p> </li> <li>Implement serializers for numeric arrays, base64-encoded images, large payloads</li> <li>Add API version field or header to responses</li> <li>Ensure all responses are JSON-safe and documented in OpenAPI</li> <li>Acceptance: API outputs stable, versioned, and match OpenAPI spec</li> </ul>"},{"location":"roadmap/#15-mvp-testing-documentation-week-6","title":"1.5 MVP Testing &amp; Documentation (Week 6)","text":"<ul> <li>[ ] Unit tests (&gt;70% coverage)</li> <li>Write unit tests for registry, loader (with mocks), worker supervisor, API endpoints</li> <li>Use <code>TestClient</code> from FastAPI for endpoint tests</li> <li>Add tests for utility modules and helper functions</li> <li>Configure <code>pytest-cov</code> with coverage threshold enforcement in CI</li> <li> <p>Acceptance: Coverage &gt;70%; failing tests block CI build</p> </li> <li> <p>[ ] Integration tests for key workflows</p> </li> <li>Test end-to-end flows: register model \u2192 load \u2192 infer \u2192 unload</li> <li>Use local SQLite and spawn real worker process with small/mock model</li> <li>Assert result correctness and resource cleanup (memory, processes)</li> <li>Run integration tests in separate CI job (slower tests)</li> <li> <p>Acceptance: Integration tests pass in CI on merge to main</p> </li> <li> <p>[ ] Basic deployment guide</p> </li> <li>Write <code>docs/deployment.md</code> with local setup and Docker instructions</li> <li>Document <code>docker build</code> and <code>docker-compose</code> examples</li> <li>List required environment variables and resource recommendations</li> <li>Add troubleshooting section for common issues</li> <li> <p>Acceptance: Following guide results in running single-node instance</p> </li> <li> <p>[ ] API documentation</p> </li> <li>Leverage FastAPI automatic OpenAPI docs at <code>/docs</code> and <code>/redoc</code></li> <li>Add MkDocs page summarizing API use cases with examples</li> <li>Include curl and Python client examples for each endpoint</li> <li>Document request/response schemas and error codes</li> <li> <p>Acceptance: Docs contain runnable examples for <code>/models</code> and <code>/infer</code></p> </li> <li> <p>[ ] Example usage scripts</p> </li> <li>Create <code>examples/register_and_infer.py</code> demonstrating full workflow</li> <li>Create <code>examples/local_infer.py</code> for local testing</li> <li>Add <code>examples/README.md</code> with setup instructions and expected outputs</li> <li>Ensure examples excluded from linting (already configured in <code>.vscode/settings.json</code>)</li> <li>Acceptance: Scripts run with documented commands; produce expected outputs</li> </ul> <p>MVP Deliverable: Single-node ModelMora that can:</p> <ul> <li>Register models from config file: YAML import or POST to <code>/models</code> populates registry</li> <li>Load model on first request: Lazy loading with 2s load time (POC validated)</li> <li>Execute inference synchronously: POST to <code>/infer/{model_name}</code> returns results</li> <li>Return results via REST API: JSON responses with OpenAPI documentation</li> <li>Run in Docker container: <code>docker-compose up</code> starts service with minimal configuration</li> </ul> <p>MVP Acceptance Checklist:</p> <ul> <li>\u2705 YAML config imports models into SQLite registry</li> <li>\u2705 First inference request triggers model load in subprocess worker</li> <li>\u2705 Subsequent requests reuse loaded model (cache hit)</li> <li>\u2705 Inference completes with &lt;100ms overhead (queue: &lt;1\u03bcs, validated by POC)</li> <li>\u2705 Worker termination reclaims memory (subprocess: ~0MB leak, validated by POC)</li> <li>\u2705 Docker image builds and runs with documented environment variables</li> <li>\u2705 API endpoints documented in OpenAPI and tested</li> <li>\u2705 Unit test coverage &gt;70%; integration tests pass</li> <li>\u2705 Deployment guide produces working instance</li> </ul>"},{"location":"roadmap/#phase-2-production-ready-6-8-weeks","title":"Phase 2: Production Ready (6-8 weeks)","text":""},{"location":"roadmap/#21-queue-scheduler-week-7-8","title":"2.1 Queue &amp; Scheduler (Week 7-8)","text":"<ul> <li>[ ] Priority Queue Implementation</li> <li>Task priority system</li> <li>Model-based grouping</li> <li> <p>Timeout handling</p> </li> <li> <p>[ ] Async Request Handling</p> </li> <li>Job ID generation</li> <li>Status polling endpoints</li> <li> <p>Result retrieval</p> </li> <li> <p>[ ] Batching Engine</p> </li> <li>Dynamic batch accumulation</li> <li>Configurable batch size/timeout</li> <li>Batch preprocessing</li> </ul>"},{"location":"roadmap/#22-lifecycle-management-week-9-10","title":"2.2 Lifecycle Management (Week 9-10)","text":"<ul> <li>[ ] Model Orchestrator</li> <li>Lazy loading strategy</li> <li>LRU unloading policy</li> <li>Warmup mechanism</li> <li> <p>Health checks per model</p> </li> <li> <p>[ ] Resource Manager</p> </li> <li>Memory pressure monitoring</li> <li>GPU utilization tracking</li> <li>Automatic scaling decisions</li> </ul>"},{"location":"roadmap/#23-grpc-service-week-11","title":"2.3 gRPC Service (Week 11)","text":"<ul> <li>[ ] Protocol Buffer definitions</li> <li>[ ] gRPC server implementation</li> <li>[ ] Streaming support for large responses</li> <li>[ ] Client SDK (Python)</li> </ul>"},{"location":"roadmap/#24-cli-tool-week-12","title":"2.4 CLI Tool (Week 12)","text":"<ul> <li>[ ] <code>modelmora init</code> - Initialize project</li> <li>[ ] <code>modelmora install &lt;model&gt;</code> - Download model</li> <li>[ ] <code>modelmora list</code> - Show installed models</li> <li>[ ] <code>modelmora lock</code> - Generate lock file</li> <li>[ ] <code>modelmora serve</code> - Start server</li> </ul>"},{"location":"roadmap/#25-enhanced-storage-week-13","title":"2.5 Enhanced Storage (Week 13)","text":"<ul> <li>[ ] Result Storage Options</li> <li>Inline response for small data</li> <li>S3/MinIO integration for large outputs</li> <li> <p>Presigned URL generation</p> </li> <li> <p>[ ] Model Cache</p> </li> <li>Persistent volume management</li> <li>Cache invalidation strategy</li> <li>Shared cache for multi-instance</li> </ul>"},{"location":"roadmap/#26-observability-week-14","title":"2.6 Observability (Week 14)","text":"<ul> <li>[ ] Metrics (Prometheus)</li> <li>Request latency histograms</li> <li>Model load/unload counters</li> <li>Memory/GPU usage gauges</li> <li> <p>Queue depth metrics</p> </li> <li> <p>[ ] Logging</p> </li> <li>Structured logging (JSON)</li> <li>Log levels configuration</li> <li> <p>Request tracing</p> </li> <li> <p>[ ] Health Checks</p> </li> <li>Liveness probe</li> <li>Readiness probe</li> <li>Dependency checks</li> </ul>"},{"location":"roadmap/#phase-3-kafka-integration-optional-3-4-weeks","title":"Phase 3: Kafka Integration (Optional - 3-4 weeks)","text":""},{"location":"roadmap/#31-kafka-consumerproducer-week-15-16","title":"3.1 Kafka Consumer/Producer (Week 15-16)","text":"<ul> <li>[ ] Request consumption from topics</li> <li>[ ] Result publishing to response topics</li> <li>[ ] Dead letter queue handling</li> <li>[ ] Consumer group management</li> </ul>"},{"location":"roadmap/#32-event-streaming-week-17-18","title":"3.2 Event Streaming (Week 17-18)","text":"<ul> <li>[ ] Model lifecycle events</li> <li>[ ] Performance metrics streaming</li> <li>[ ] Audit log events</li> </ul>"},{"location":"roadmap/#phase-4-scale-polish-4-6-weeks","title":"Phase 4: Scale &amp; Polish (4-6 weeks)","text":""},{"location":"roadmap/#41-kubernetes-deployment-week-19-20","title":"4.1 Kubernetes Deployment (Week 19-20)","text":"<ul> <li>[ ] Helm chart creation</li> <li>[ ] ConfigMap/Secret management</li> <li>[ ] StatefulSet for model cache</li> <li>[ ] HPA (Horizontal Pod Autoscaler)</li> <li>[ ] Service mesh integration (optional)</li> </ul>"},{"location":"roadmap/#42-multi-node-coordination-week-21-22","title":"4.2 Multi-Node Coordination (Week 21-22)","text":"<ul> <li>[ ] Redis-based distributed queue</li> <li>[ ] Distributed locking (model loading)</li> <li>[ ] Service discovery</li> <li>[ ] Load balancing strategies</li> </ul>"},{"location":"roadmap/#43-advanced-features-week-23-24","title":"4.3 Advanced Features (Week 23-24)","text":"<ul> <li>[ ] A/B testing support (model versions)</li> <li>[ ] Canary deployments</li> <li>[ ] Request shadowing</li> <li>[ ] Rate limiting</li> <li>[ ] Authentication/Authorization</li> </ul>"},{"location":"roadmap/#phase-5-ecosystem-integration-ongoing","title":"Phase 5: Ecosystem Integration (Ongoing)","text":""},{"location":"roadmap/#51-miraveja-integration","title":"5.1 MiraVeja Integration","text":"<ul> <li>[ ] Custom gRPC client in MiraVeja</li> <li>[ ] Error handling patterns</li> <li>[ ] Retry logic</li> <li>[ ] Circuit breaker implementation</li> </ul>"},{"location":"roadmap/#52-model-support-expansion","title":"5.2 Model Support Expansion","text":"<ul> <li>[ ] ONNX Runtime support</li> <li>[ ] TensorRT optimization</li> <li>[ ] Custom model loaders</li> <li>[ ] Fine-tuned model versioning</li> </ul>"},{"location":"roadmap/#53-developer-experience","title":"5.3 Developer Experience","text":"<ul> <li>[ ] Web UI for model management</li> <li>[ ] Interactive API documentation</li> <li>[ ] Performance profiling tools</li> <li>[ ] Debugging utilities</li> </ul>"},{"location":"roadmap/#milestones-releases","title":"Milestones &amp; Releases","text":"Version Milestone Timeline Key Features v0.1.0 MVP Week 6 Single model inference via REST v0.5.0 Alpha Week 14 Queue, gRPC, CLI, Observability v0.8.0 Beta Week 18 Kafka, K8s ready v1.0.0 Production Week 24 Multi-node, full features v1.x Enhancements Ongoing Performance, new models"},{"location":"roadmap/#risk-management","title":"Risk Management","text":"Risk Impact Mitigation Status Python memory leaks High Multi-process isolation (Phase 1) \u2705 Validated (POC 1: 5000x better cleanup) HF metadata inconsistency Medium Curated model configs (Phase 1) Planned Complex K8s orchestration Medium Start single-node, scale later (Phase 4) Planned Performance bottlenecks High Early benchmarking POCs (Phase 0) \u2705 Validated (730x queue capacity, 2s load time) Scope creep Medium Strict MVP definition, phased approach Ongoing"},{"location":"roadmap/#success-metrics","title":"Success Metrics","text":"<p>MVP Success Criteria:</p> <ul> <li>Serve inference requests with &lt;100ms overhead \u2705 Validated (Queue: 0.7\u03bcs, Load: 2s)</li> <li>Support 3+ model types (embedding, text gen, image gen)</li> <li>Handle 10 concurrent requests \u2705 Validated (Capacity: 730k req/sec)</li> <li>Run in &lt;4GB RAM (excluding models) \u2705 Validated (Per model: ~500MB)</li> </ul> <p>v1.0 Success Criteria:</p> <ul> <li>Support 20+ concurrent models \u2705 Feasible (10 models = ~5GB RAM)</li> <li>&lt;50ms queue latency \u2705 Validated (Actual: &lt;0.001ms)</li> <li>99.9% uptime in production</li> <li>Complete API documentation</li> <li>Integration with MiraVeja</li> </ul> <p>Validated Performance Benchmarks (POC Results):</p> <ul> <li>Model load time: 2s (cached, small-medium models)</li> <li>Queue throughput: 730,108 ops/sec (asyncio.PriorityQueue)</li> <li>Queue latency: 0.7\u03bcs enqueue, 1.7\u03bcs dequeue</li> <li>gRPC streaming: 31.92 MB/s single client, 33.31 MB/s concurrent</li> <li>Memory isolation: Subprocess 5000x better than GC (0.1MB vs 516MB leak)</li> <li>Priority ordering: 100% correct under load</li> </ul>"},{"location":"roadmap/#next-immediate-steps","title":"Next Immediate Steps","text":"<ol> <li>Create requirements document (this week)</li> <li>Design system architecture (next week)</li> <li>POC: Multi-process memory isolation (parallel task)</li> <li>Set up project structure (start Phase 1)</li> </ol> <p>Should I proceed with drafting the detailed requirements document?</p>"},{"location":"api-reference/grpc/","title":"gRPC API Reference","text":"<p>Documentation for ModelMora's gRPC API.</p>"},{"location":"api-reference/grpc/#connection","title":"Connection","text":"<pre><code>localhost:50051\n</code></pre>"},{"location":"api-reference/grpc/#service-definitions","title":"Service Definitions","text":""},{"location":"api-reference/grpc/#inferenceservice","title":"InferenceService","text":"<p>Protocol Buffer definition (<code>inference.proto</code>):</p> <pre><code>syntax = \"proto3\";\n\npackage modelmora.inference;\n\nservice InferenceService {\n  rpc Infer(InferenceRequest) returns (InferenceResponse);\n  rpc InferStream(InferenceRequest) returns (stream InferenceChunk);\n}\n\nmessage InferenceRequest {\n  string model_name = 1;\n  oneof input {\n    string text = 2;\n    bytes image = 3;\n  }\n  map&lt;string, string&gt; options = 4;\n}\n\nmessage InferenceResponse {\n  repeated float embeddings = 1;\n  string text = 2;\n  bytes image = 3;\n  map&lt;string, string&gt; metadata = 4;\n}\n\nmessage InferenceChunk {\n  bytes data = 1;\n  int32 sequence = 2;\n  bool final = 3;\n}\n</code></pre>"},{"location":"api-reference/grpc/#python-client-example","title":"Python Client Example","text":"<pre><code>import grpc\nfrom modelmora.protos import inference_pb2, inference_pb2_grpc\n\n# Create channel\nchannel = grpc.insecure_channel('localhost:50051')\nstub = inference_pb2_grpc.InferenceServiceStub(channel)\n\n# Text embedding\nrequest = inference_pb2.InferenceRequest(\n    model_name=\"all-MiniLM-L6-v2\",\n    text=\"Hello, ModelMora!\"\n)\n\nresponse = stub.Infer(request)\nprint(f\"Embeddings: {response.embeddings}\")\n\n# Streaming inference (for large outputs)\nfor chunk in stub.InferStream(request):\n    print(f\"Chunk {chunk.sequence}: {len(chunk.data)} bytes\")\n    if chunk.final:\n        break\n</code></pre>"},{"location":"api-reference/grpc/#next-steps","title":"Next Steps","text":"<ul> <li>REST API Reference</li> <li>Architecture Overview</li> </ul>"},{"location":"api-reference/rest/","title":"REST API Reference","text":"<p>Documentation for ModelMora's REST API endpoints.</p>"},{"location":"api-reference/rest/#base-url","title":"Base URL","text":"<pre><code>http://localhost:8000\n</code></pre>"},{"location":"api-reference/rest/#authentication","title":"Authentication","text":"<p>Currently, ModelMora MVP does not require authentication. Authentication will be added in Phase 2.</p>"},{"location":"api-reference/rest/#endpoints","title":"Endpoints","text":""},{"location":"api-reference/rest/#health-check","title":"Health Check","text":""},{"location":"api-reference/rest/#get-health","title":"<code>GET /health</code>","text":"<p>Check service health status.</p> <p>Response:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"version\": \"0.1.0\",\n  \"uptime_seconds\": 3600,\n  \"models_loaded\": 2,\n  \"workers\": {\n    \"active\": 2,\n    \"idle\": 1\n  }\n}\n</code></pre>"},{"location":"api-reference/rest/#model-registry","title":"Model Registry","text":""},{"location":"api-reference/rest/#get-models","title":"<code>GET /models</code>","text":"<p>List all registered models.</p> <p>Query Parameters:</p> <ul> <li><code>task</code> (optional): Filter by task type</li> <li><code>device</code> (optional): Filter by device</li> <li><code>page</code> (optional): Page number (default: 1)</li> <li><code>limit</code> (optional): Items per page (default: 20)</li> </ul> <p>Response:</p> <pre><code>{\n  \"models\": [\n    {\n      \"name\": \"all-MiniLM-L6-v2\",\n      \"version\": \"1.0.0\",\n      \"task\": \"text-embedding\",\n      \"device\": \"cuda\",\n      \"state\": \"loaded\",\n      \"created_at\": \"2025-12-04T10:00:00Z\"\n    }\n  ],\n  \"total\": 1,\n  \"page\": 1,\n  \"pages\": 1\n}\n</code></pre>"},{"location":"api-reference/rest/#get-modelsname","title":"<code>GET /models/{name}</code>","text":"<p>Get specific model details.</p> <p>Response:</p> <pre><code>{\n  \"name\": \"all-MiniLM-L6-v2\",\n  \"version\": \"1.0.0\",\n  \"source\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"task\": \"text-embedding\",\n  \"device\": \"cuda\",\n  \"state\": \"loaded\",\n  \"config\": {\n    \"max_seq_length\": 512,\n    \"batch_size\": 32\n  },\n  \"stats\": {\n    \"requests_total\": 1523,\n    \"requests_failed\": 2,\n    \"avg_latency_ms\": 15.2\n  }\n}\n</code></pre>"},{"location":"api-reference/rest/#post-models","title":"<code>POST /models</code>","text":"<p>Register a new model.</p> <p>Request:</p> <pre><code>{\n  \"name\": \"all-MiniLM-L6-v2\",\n  \"source\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"task\": \"text-embedding\",\n  \"device\": \"cuda\",\n  \"config\": {\n    \"max_seq_length\": 512,\n    \"batch_size\": 32\n  }\n}\n</code></pre> <p>Response: <code>201 Created</code></p>"},{"location":"api-reference/rest/#inference","title":"Inference","text":""},{"location":"api-reference/rest/#post-infermodel_name","title":"<code>POST /infer/{model_name}</code>","text":"<p>Execute synchronous inference.</p> <p>Request:</p> <pre><code>{\n  \"text\": \"ModelMora is awesome\",\n  \"options\": {\n    \"normalize\": true\n  }\n}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"result\": {\n    \"embeddings\": [0.123, -0.456, ...]\n  },\n  \"metadata\": {\n    \"model\": \"all-MiniLM-L6-v2\",\n    \"version\": \"1.0.0\",\n    \"device\": \"cuda:0\"\n  },\n  \"timing\": {\n    \"queue_time_ms\": 0.7,\n    \"inference_time_ms\": 15.2,\n    \"total_time_ms\": 15.9\n  }\n}\n</code></pre>"},{"location":"api-reference/rest/#error-responses","title":"Error Responses","text":""},{"location":"api-reference/rest/#400-bad-request","title":"400 Bad Request","text":"<pre><code>{\n  \"error\": \"ValidationError\",\n  \"detail\": \"Invalid input format\",\n  \"code\": \"INVALID_INPUT\"\n}\n</code></pre>"},{"location":"api-reference/rest/#404-not-found","title":"404 Not Found","text":"<pre><code>{\n  \"error\": \"ModelNotFound\",\n  \"detail\": \"Model 'unknown-model' not found\",\n  \"code\": \"MODEL_NOT_FOUND\"\n}\n</code></pre>"},{"location":"api-reference/rest/#504-gateway-timeout","title":"504 Gateway Timeout","text":"<pre><code>{\n  \"error\": \"InferenceTimeout\",\n  \"detail\": \"Request exceeded timeout of 30s\",\n  \"code\": \"INFERENCE_TIMEOUT\"\n}\n</code></pre> <p>For interactive API documentation, visit <code>/docs</code> or <code>/redoc</code> when the service is running.</p>"},{"location":"data_models/common/error_models/","title":"Error Models","text":"<p>Context: Cross-Cutting Type: Documentation Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/common/error_models/#1-overview","title":"1. Overview","text":"<p>Error models define standardized error representations across ModelMora APIs, providing consistent error handling and client experience.</p>"},{"location":"data_models/common/error_models/#2-error-response-structure","title":"2. Error Response Structure","text":"<pre>4d236649f26666ea498b83aaeeb040d2cbe5d52b46d6d2c39a7b53fb302877e5eb175afa3acc3c7b0a4bb0d4826d1045fa59d1f3bbf0af07a5e6783115f96828</pre><pre>c957a7b6ebbdd31d314bf80fcf01500248a69d4bc8736744241b1eb57b9067781e8c547b28d0fc190df8d46b058c4e88892bdc32175b65b791c25e07adea0248</pre>"},{"location":"data_models/common/error_models/#3-standard-error-response","title":"3. Standard Error Response","text":"<pre><code>{\n  \"error\": {\n    \"code\": \"MODEL_NOT_FOUND\",\n    \"message\": \"Model not found in registry\",\n    \"details\": {\n      \"model_id\": \"invalid/model\"\n    },\n    \"trace_id\": \"55000000-e29b-41d4-a716-446655440017\"\n  }\n}\n</code></pre>"},{"location":"data_models/common/error_models/#4-error-codes","title":"4. Error Codes","text":""},{"location":"data_models/common/error_models/#41-registry-context-errors","title":"4.1 Registry Context Errors","text":"Code HTTP Status Description <code>MODEL_NOT_FOUND</code> 404 Model does not exist in registry <code>MODEL_ALREADY_EXISTS</code> 409 Model already registered <code>VERSION_NOT_FOUND</code> 404 Model version does not exist <code>VERSION_ALREADY_EXISTS</code> 409 Version already registered for model <code>INVALID_MODEL_ID</code> 400 Model ID format invalid <code>INVALID_CHECKSUM</code> 400 Checksum format invalid <code>ARTIFACT_NOT_ACCESSIBLE</code> 400 Artifact URI not accessible"},{"location":"data_models/common/error_models/#42-lifecycle-context-errors","title":"4.2 Lifecycle Context Errors","text":"Code HTTP Status Description <code>MODEL_NOT_LOADED</code> 404 Model not currently loaded <code>MODEL_ALREADY_LOADED</code> 409 Model already in memory <code>INSUFFICIENT_MEMORY</code> 503 Not enough memory to load model <code>LOAD_TIMEOUT</code> 504 Model loading timed out <code>LOAD_FAILED</code> 500 Model loading failed <code>MODEL_UNHEALTHY</code> 503 Model failed health check <code>EVICTION_FAILED</code> 500 Failed to evict model"},{"location":"data_models/common/error_models/#43-inference-context-errors","title":"4.3 Inference Context Errors","text":"Code HTTP Status Description <code>JOB_NOT_FOUND</code> 404 Inference job does not exist <code>QUEUE_FULL</code> 503 Inference queue at capacity <code>INVALID_INPUT</code> 400 Input data invalid for task type <code>INFERENCE_TIMEOUT</code> 504 Inference processing timed out <code>INFERENCE_FAILED</code> 500 Inference processing failed <code>JOB_CANCELLED</code> 409 Job was cancelled <code>CANNOT_CANCEL</code> 409 Job cannot be cancelled (already processing)"},{"location":"data_models/common/error_models/#44-common-errors","title":"4.4 Common Errors","text":"Code HTTP Status Description <code>VALIDATION_ERROR</code> 400 Request validation failed <code>UNAUTHORIZED</code> 401 Authentication required <code>FORBIDDEN</code> 403 Insufficient permissions <code>INTERNAL_ERROR</code> 500 Unexpected internal error <code>SERVICE_UNAVAILABLE</code> 503 Service temporarily unavailable"},{"location":"data_models/common/error_models/#5-validation-error-details","title":"5. Validation Error Details","text":"<p>For validation errors, <code>details</code> includes field-specific errors:</p> <pre><code>{\n  \"error\": {\n    \"code\": \"VALIDATION_ERROR\",\n    \"message\": \"Request validation failed\",\n    \"details\": {\n      \"fields\": {\n        \"model_id\": [\"Model ID cannot be empty\"],\n        \"version\": [\"Version must match pattern: v{major}.{minor}.{patch}\"],\n        \"resource_requirements.memory_mb\": [\"Must be greater than 0\"]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"data_models/common/error_models/#6-error-exceptions","title":"6. Error Exceptions","text":"<p>Domain exceptions map to error codes:</p> <pre><code># Registry exceptions\nclass ModelNotFoundException(Exception):\n    code = \"MODEL_NOT_FOUND\"\n    status_code = 404\n\nclass ModelAlreadyExistsException(Exception):\n    code = \"MODEL_ALREADY_EXISTS\"\n    status_code = 409\n\n# Lifecycle exceptions\nclass InsufficientMemoryException(Exception):\n    code = \"INSUFFICIENT_MEMORY\"\n    status_code = 503\n\nclass ModelUnhealthyException(Exception):\n    code = \"MODEL_UNHEALTHY\"\n    status_code = 503\n\n# Inference exceptions\nclass QueueFullException(Exception):\n    code = \"QUEUE_FULL\"\n    status_code = 503\n\nclass InferenceTimeoutException(Exception):\n    code = \"INFERENCE_TIMEOUT\"\n    status_code = 504\n</code></pre>"},{"location":"data_models/common/error_models/#7-error-handler","title":"7. Error Handler","text":"<p>API layer converts exceptions to error responses:</p> <pre><code>def exception_to_error_response(exc: Exception, trace_id: UUID = None) -&gt; ErrorResponse:\n    \"\"\"Convert exception to standardized error response\"\"\"\n\n    # Domain exception with code\n    if hasattr(exc, 'code'):\n        return ErrorResponse(\n            error=Error(\n                code=exc.code,\n                message=str(exc),\n                details=getattr(exc, 'details', None),\n                trace_id=trace_id\n            )\n        )\n\n    # Validation error\n    if isinstance(exc, ValidationError):\n        return ErrorResponse(\n            error=Error(\n                code=\"VALIDATION_ERROR\",\n                message=\"Request validation failed\",\n                details={\"fields\": exc.errors},\n                trace_id=trace_id\n            )\n        )\n\n    # Unexpected error\n    logger.exception(\"Unexpected error\", exc_info=exc)\n    return ErrorResponse(\n        error=Error(\n            code=\"INTERNAL_ERROR\",\n            message=\"An unexpected error occurred\",\n            details=None,\n            trace_id=trace_id\n        )\n    )\n</code></pre>"},{"location":"data_models/common/error_models/#8-protocol-buffers","title":"8. Protocol Buffers","text":"<p>Error representation in gRPC:</p> <pre><code>message Error {\n  string code = 1;\n  string message = 2;\n  google.protobuf.Struct details = 3;\n  string trace_id = 4;\n}\n\nmessage ErrorResponse {\n  Error error = 1;\n}\n</code></pre> <p>gRPC status codes:</p> <pre><code>ERROR_TO_GRPC_STATUS = {\n    \"MODEL_NOT_FOUND\": grpc.StatusCode.NOT_FOUND,\n    \"MODEL_ALREADY_EXISTS\": grpc.StatusCode.ALREADY_EXISTS,\n    \"VALIDATION_ERROR\": grpc.StatusCode.INVALID_ARGUMENT,\n    \"UNAUTHORIZED\": grpc.StatusCode.UNAUTHENTICATED,\n    \"FORBIDDEN\": grpc.StatusCode.PERMISSION_DENIED,\n    \"INTERNAL_ERROR\": grpc.StatusCode.INTERNAL,\n    \"SERVICE_UNAVAILABLE\": grpc.StatusCode.UNAVAILABLE,\n    \"INFERENCE_TIMEOUT\": grpc.StatusCode.DEADLINE_EXCEEDED,\n    \"QUEUE_FULL\": grpc.StatusCode.RESOURCE_EXHAUSTED,\n}\n</code></pre>"},{"location":"data_models/common/error_models/#9-client-error-handling","title":"9. Client Error Handling","text":"<p>Example client error handling:</p> <pre><code># Python client\ntry:\n    response = client.register_model(request)\nexcept ModelmoraException as e:\n    if e.code == \"MODEL_ALREADY_EXISTS\":\n        # Handle duplicate\n        logger.warning(f\"Model already exists: {e.message}\")\n    elif e.code == \"VALIDATION_ERROR\":\n        # Handle validation errors\n        for field, errors in e.details[\"fields\"].items():\n            logger.error(f\"{field}: {errors}\")\n    else:\n        # Handle other errors\n        logger.error(f\"Error: {e.code} - {e.message}\")\n        raise\n</code></pre> <pre><code>// TypeScript client\ntry {\n  const response = await client.registerModel(request);\n} catch (error) {\n  if (error.code === 'MODEL_ALREADY_EXISTS') {\n    // Handle duplicate\n    console.warn(`Model already exists: ${error.message}`);\n  } else if (error.code === 'VALIDATION_ERROR') {\n    // Handle validation errors\n    Object.entries(error.details.fields).forEach(([field, errors]) =&gt; {\n      console.error(`${field}: ${errors}`);\n    });\n  } else {\n    // Handle other errors\n    console.error(`Error: ${error.code} - ${error.message}`);\n    throw error;\n  }\n}\n</code></pre>"},{"location":"data_models/common/error_models/#10-error-logging","title":"10. Error Logging","text":"<p>Errors should be logged with context:</p> <pre><code>def log_error(error: Error, context: dict = None):\n    \"\"\"Log error with context\"\"\"\n    logger.error(\n        f\"Error: {error.code} - {error.message}\",\n        extra={\n            \"error_code\": error.code,\n            \"error_details\": error.details,\n            \"trace_id\": str(error.trace_id),\n            **(context or {})\n        }\n    )\n</code></pre>"},{"location":"data_models/common/error_models/#11-related-documentation","title":"11. Related Documentation","text":"<ul> <li>API Design - API error responses</li> <li>Data Models - Domain model validation</li> <li>Domain Events - Error event publishing</li> </ul>"},{"location":"data_models/dto/api_models/","title":"API DTOs (Data Transfer Objects)","text":"<p>Context: Cross-Cutting Type: Documentation Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/dto/api_models/#1-overview","title":"1. Overview","text":"<p>API DTOs are language-agnostic data structures for REST API request/response serialization. They provide a stable API contract separate from internal domain models.</p>"},{"location":"data_models/dto/api_models/#2-dto-pattern","title":"2. DTO Pattern","text":"<p>DTOs follow these conventions:</p> <ul> <li>Flat structure - No deep nesting</li> <li>snake_case - JSON field naming</li> <li>Optional versioning - API version in URL</li> <li>Validation - Schema validation at API boundary</li> </ul>"},{"location":"data_models/dto/api_models/#3-registry-api-dtos","title":"3. Registry API DTOs","text":""},{"location":"data_models/dto/api_models/#31-registermodelrequest","title":"3.1 RegisterModelRequest","text":"<pre><code>{\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"version\": \"v2.2.2\",\n  \"task_type\": \"txt2embed\",\n  \"checksum\": \"sha256:abc123...\",\n  \"artifact_uri\": \"https://...\",\n  \"resource_requirements\": {\n    \"memory_mb\": 512,\n    \"gpu_vram_mb\": 0,\n    \"cpu_threads\": 2\n  },\n  \"framework\": \"pytorch\",\n  \"framework_version\": \"2.0.1\"\n}\n</code></pre>"},{"location":"data_models/dto/api_models/#32-modelresponse","title":"3.2 ModelResponse","text":"<pre><code>{\n  \"id\": \"880e8400-e29b-41d4-a716-446655440004\",\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"task_type\": \"txt2embed\",\n  \"current_version\": \"v2.2.2\",\n  \"versions\": [\"v2.2.2\", \"v2.2.1\"],\n  \"created_at\": \"2025-12-04T10:30:00Z\"\n}\n</code></pre>"},{"location":"data_models/dto/api_models/#4-lifecycle-api-dtos","title":"4. Lifecycle API DTOs","text":""},{"location":"data_models/dto/api_models/#41-loadmodelrequest","title":"4.1 LoadModelRequest","text":"<pre><code>{\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"version\": \"v2.2.2\"\n}\n</code></pre>"},{"location":"data_models/dto/api_models/#42-loadedmodelresponse","title":"4.2 LoadedModelResponse","text":"<pre><code>{\n  \"id\": \"bb0e8400-e29b-41d4-a716-446655440007\",\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"version\": \"v2.2.2\",\n  \"state\": \"loaded\",\n  \"memory_usage_mb\": 512.5,\n  \"healthy\": true,\n  \"loaded_at\": \"2025-12-04T10:25:00Z\"\n}\n</code></pre>"},{"location":"data_models/dto/api_models/#5-inference-api-dtos","title":"5. Inference API DTOs","text":""},{"location":"data_models/dto/api_models/#51-inferencerequest","title":"5.1 InferenceRequest","text":"<pre><code>{\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"input\": {\n    \"text\": \"Hello world\"\n  },\n  \"priority\": \"high\",\n  \"timeout_seconds\": 60\n}\n</code></pre>"},{"location":"data_models/dto/api_models/#52-inferenceresponse","title":"5.2 InferenceResponse","text":"<pre><code>{\n  \"job_id\": \"ee0e8400-e29b-41d4-a716-446655440010\",\n  \"status\": \"completed\",\n  \"result\": {\n    \"embedding\": [0.123, -0.456, ...]\n  },\n  \"processing_time_ms\": 45.3,\n  \"created_at\": \"2025-12-04T10:30:00Z\",\n  \"completed_at\": \"2025-12-04T10:30:00.045Z\"\n}\n</code></pre>"},{"location":"data_models/dto/api_models/#6-common-dtos","title":"6. Common DTOs","text":""},{"location":"data_models/dto/api_models/#61-errorresponse","title":"6.1 ErrorResponse","text":"<pre><code>{\n  \"error\": {\n    \"code\": \"MODEL_NOT_FOUND\",\n    \"message\": \"Model not found in registry\",\n    \"details\": {\n      \"model_id\": \"invalid/model\"\n    }\n  }\n}\n</code></pre>"},{"location":"data_models/dto/api_models/#62-paginatedresponse","title":"6.2 PaginatedResponse","text":"<pre><code>{\n  \"data\": [...],\n  \"pagination\": {\n    \"page\": 1,\n    \"page_size\": 20,\n    \"total_items\": 100,\n    \"total_pages\": 5\n  }\n}\n</code></pre>"},{"location":"data_models/dto/api_models/#7-dto-mapping","title":"7. DTO Mapping","text":"<p>DTOs map to domain models:</p> DTO Domain Model Direction RegisterModelRequest Model + ModelVersion Request \u2192 Domain ModelResponse Model Domain \u2192 Response LoadModelRequest ModelId + Version Request \u2192 Domain LoadedModelResponse LoadedModel Domain \u2192 Response InferenceRequest InferenceRequest Request \u2192 Domain InferenceResponse InferenceJob Domain \u2192 Response"},{"location":"data_models/dto/api_models/#8-validation","title":"8. Validation","text":"<p>DTOs undergo validation at API boundary:</p> <pre><code># JSON Schema validation\ndef validate_dto(dto_data: dict, schema: dict) -&gt; ValidationResult:\n    \"\"\"Validate DTO against JSON Schema\"\"\"\n    try:\n        jsonschema.validate(dto_data, schema)\n        return ValidationResult(valid=True)\n    except jsonschema.ValidationError as e:\n        return ValidationResult(\n            valid=False,\n            errors=[str(e)]\n        )\n</code></pre>"},{"location":"data_models/dto/api_models/#9-protocol-buffers-grpc","title":"9. Protocol Buffers (gRPC)","text":"<p>DTOs also defined as Protocol Buffer messages:</p> <pre><code>message RegisterModelRequest {\n  string model_id = 1;\n  string version = 2;\n  string task_type = 3;\n  string checksum = 4;\n  string artifact_uri = 5;\n  ResourceRequirements resource_requirements = 6;\n  string framework = 7;\n  string framework_version = 8;\n}\n\nmessage ModelResponse {\n  string id = 1;\n  string model_id = 2;\n  string task_type = 3;\n  string current_version = 4;\n  repeated string versions = 5;\n  google.protobuf.Timestamp created_at = 6;\n}\n</code></pre>"},{"location":"data_models/dto/api_models/#10-related-documentation","title":"10. Related Documentation","text":"<ul> <li>API Design - Complete API specifications</li> <li>Data Models - Domain model definitions</li> <li>Error Models - Error response structures</li> </ul>"},{"location":"data_models/events/domain_events/","title":"Domain Events","text":"<p>Context: Cross-Cutting Type: Documentation Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/events/domain_events/#1-overview","title":"1. Overview","text":"<p>Domain events represent significant state changes in the system. They enable event-driven architecture, loose coupling, and auditability.</p>"},{"location":"data_models/events/domain_events/#2-event-pattern","title":"2. Event Pattern","text":"<p>All domain events follow this structure:</p> <pre>5fd749e6e61d4a6b2a9a797f0f1ed70262ad4f3b79d70242f91d1a05b3d890d1ae48bf35be3bc859cdfce54cac9db2dae5a83c6b20b1d87a04ee465850b0d8c7</pre><pre>ff26cf849043824492c2db01c4357392dcd5f8ea2e918a9c30add4342d2698dd0f9c9c71be6cd4a117494b4d4bc391972d149c714dd686e1fa5c090b84653e3a</pre>"},{"location":"data_models/events/domain_events/#3-registry-context-events","title":"3. Registry Context Events","text":""},{"location":"data_models/events/domain_events/#31-modelregisteredevent","title":"3.1 ModelRegisteredEvent","text":"<pre><code>{\n  \"event_id\": \"66000000-e29b-41d4-a716-446655440018\",\n  \"event_type\": \"model.registered\",\n  \"aggregate_id\": \"770e8400-e29b-41d4-a716-446655440003\",\n  \"aggregate_type\": \"model_catalog\",\n  \"payload\": {\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"task_type\": \"txt2embed\",\n    \"version\": \"v2.2.2\"\n  },\n  \"timestamp\": \"2025-12-04T10:30:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/events/domain_events/#32-modelversionaddedevent","title":"3.2 ModelVersionAddedEvent","text":"<pre><code>{\n  \"event_id\": \"77000000-e29b-41d4-a716-446655440019\",\n  \"event_type\": \"model.version.added\",\n  \"aggregate_id\": \"770e8400-e29b-41d4-a716-446655440003\",\n  \"aggregate_type\": \"model_catalog\",\n  \"payload\": {\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"version\": \"v2.3.0\",\n    \"checksum\": \"sha256:new123...\"\n  },\n  \"timestamp\": \"2025-12-04T11:00:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/events/domain_events/#33-modelunregisteredevent","title":"3.3 ModelUnregisteredEvent","text":"<pre><code>{\n  \"event_id\": \"88000000-e29b-41d4-a716-446655440020\",\n  \"event_type\": \"model.unregistered\",\n  \"aggregate_id\": \"770e8400-e29b-41d4-a716-446655440003\",\n  \"aggregate_type\": \"model_catalog\",\n  \"payload\": {\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\"\n  },\n  \"timestamp\": \"2025-12-04T12:00:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/events/domain_events/#4-lifecycle-context-events","title":"4. Lifecycle Context Events","text":""},{"location":"data_models/events/domain_events/#41-modelloadedevent","title":"4.1 ModelLoadedEvent","text":"<pre><code>{\n  \"event_id\": \"99000000-e29b-41d4-a716-446655440021\",\n  \"event_type\": \"model.loaded\",\n  \"aggregate_id\": \"cc0e8400-e29b-41d4-a716-446655440008\",\n  \"aggregate_type\": \"model_lifecycle_manager\",\n  \"payload\": {\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"version\": \"v2.2.2\",\n    \"memory_mb\": 512.5,\n    \"process_id\": 12345\n  },\n  \"timestamp\": \"2025-12-04T10:25:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/events/domain_events/#42-modelunloadedevent","title":"4.2 ModelUnloadedEvent","text":"<pre><code>{\n  \"event_id\": \"aa100000-e29b-41d4-a716-446655440022\",\n  \"event_type\": \"model.unloaded\",\n  \"aggregate_id\": \"cc0e8400-e29b-41d4-a716-446655440008\",\n  \"aggregate_type\": \"model_lifecycle_manager\",\n  \"payload\": {\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"reason\": \"lru_eviction\",\n    \"uptime_seconds\": 3600\n  },\n  \"timestamp\": \"2025-12-04T11:25:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/events/domain_events/#43-modelhealthchangedevent","title":"4.3 ModelHealthChangedEvent","text":"<pre><code>{\n  \"event_id\": \"bb100000-e29b-41d4-a716-446655440023\",\n  \"event_type\": \"model.health.changed\",\n  \"aggregate_id\": \"cc0e8400-e29b-41d4-a716-446655440008\",\n  \"aggregate_type\": \"model_lifecycle_manager\",\n  \"payload\": {\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"previous_state\": \"loaded\",\n    \"new_state\": \"unhealthy\",\n    \"error_message\": \"Health check timeout\"\n  },\n  \"timestamp\": \"2025-12-04T10:35:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/events/domain_events/#5-inference-context-events","title":"5. Inference Context Events","text":""},{"location":"data_models/events/domain_events/#51-jobsubmittedevent","title":"5.1 JobSubmittedEvent","text":"<pre><code>{\n  \"event_id\": \"cc100000-e29b-41d4-a716-446655440024\",\n  \"event_type\": \"job.submitted\",\n  \"aggregate_id\": \"11000000-e29b-41d4-a716-446655440013\",\n  \"aggregate_type\": \"inference_queue\",\n  \"payload\": {\n    \"job_id\": \"ee0e8400-e29b-41d4-a716-446655440010\",\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"priority\": \"high\"\n  },\n  \"timestamp\": \"2025-12-04T10:30:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/events/domain_events/#52-jobcompletedevent","title":"5.2 JobCompletedEvent","text":"<pre><code>{\n  \"event_id\": \"dd100000-e29b-41d4-a716-446655440025\",\n  \"event_type\": \"job.completed\",\n  \"aggregate_id\": \"11000000-e29b-41d4-a716-446655440013\",\n  \"aggregate_type\": \"inference_queue\",\n  \"payload\": {\n    \"job_id\": \"ee0e8400-e29b-41d4-a716-446655440010\",\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"processing_time_ms\": 45.3,\n    \"success\": true\n  },\n  \"timestamp\": \"2025-12-04T10:30:00.045Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/events/domain_events/#53-jobfailedevent","title":"5.3 JobFailedEvent","text":"<pre><code>{\n  \"event_id\": \"ee100000-e29b-41d4-a716-446655440026\",\n  \"event_type\": \"job.failed\",\n  \"aggregate_id\": \"11000000-e29b-41d4-a716-446655440013\",\n  \"aggregate_type\": \"inference_queue\",\n  \"payload\": {\n    \"job_id\": \"ee0e8400-e29b-41d4-a716-446655440010\",\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"error_message\": \"Inference timeout\",\n    \"processing_time_ms\": 60000.0\n  },\n  \"timestamp\": \"2025-12-04T10:31:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/events/domain_events/#6-event-publishing","title":"6. Event Publishing","text":""},{"location":"data_models/events/domain_events/#61-event-bus-pattern","title":"6.1 Event Bus Pattern","text":"<pre><code>class EventBus:\n    def __init__(self):\n        self.handlers = {}\n\n    def subscribe(self, event_type: str, handler: Callable):\n        \"\"\"Subscribe to event type\"\"\"\n        if event_type not in self.handlers:\n            self.handlers[event_type] = []\n        self.handlers[event_type].append(handler)\n\n    def publish(self, event: DomainEvent):\n        \"\"\"Publish event to subscribers\"\"\"\n        if event.event_type in self.handlers:\n            for handler in self.handlers[event.event_type]:\n                handler(event)\n</code></pre>"},{"location":"data_models/events/domain_events/#62-aggregate-event-generation","title":"6.2 Aggregate Event Generation","text":"<pre><code># In aggregate methods\ndef register_model(self, model: Model) -&gt; None:\n    \"\"\"Register model and generate event\"\"\"\n    self.models[model.model_id] = model\n\n    # Generate event\n    self.addEvent(DomainEvent(\n        event_id=generate_uuid(),\n        event_type=\"model.registered\",\n        aggregate_id=self.id,\n        aggregate_type=\"model_catalog\",\n        payload={\n            \"model_id\": str(model.model_id),\n            \"task_type\": model.task_type.toString()\n        },\n        timestamp=current_timestamp(),\n        version=1\n    ))\n</code></pre>"},{"location":"data_models/events/domain_events/#7-event-store","title":"7. Event Store","text":"<p>Events can be persisted for event sourcing:</p> <pre><code>CREATE TABLE event_store (\n    event_id UUID PRIMARY KEY,\n    event_type VARCHAR(255) NOT NULL,\n    aggregate_id UUID NOT NULL,\n    aggregate_type VARCHAR(255) NOT NULL,\n    payload JSONB NOT NULL,\n    timestamp TIMESTAMP NOT NULL,\n    version INTEGER NOT NULL\n);\n\nCREATE INDEX idx_event_store_aggregate ON event_store(aggregate_id);\nCREATE INDEX idx_event_store_type ON event_store(event_type);\nCREATE INDEX idx_event_store_timestamp ON event_store(timestamp);\n</code></pre>"},{"location":"data_models/events/domain_events/#8-kafka-integration","title":"8. Kafka Integration","text":"<p>Events can be published to Kafka topics:</p> <pre><code># Publish to Kafka\ndef publish_to_kafka(event: DomainEvent):\n    \"\"\"Publish event to Kafka topic\"\"\"\n    topic = f\"modelmora.{event.aggregate_type}.{event.event_type}\"\n\n    kafka_producer.send(\n        topic=topic,\n        key=str(event.aggregate_id),\n        value=event.to_json()\n    )\n</code></pre>"},{"location":"data_models/events/domain_events/#9-event-handlers","title":"9. Event Handlers","text":"<p>Example event handlers:</p> <pre><code># Audit logging\ndef audit_log_handler(event: DomainEvent):\n    \"\"\"Log all events for audit trail\"\"\"\n    logger.info(f\"Event: {event.event_type}\", extra={\n        \"event_id\": str(event.event_id),\n        \"aggregate_id\": str(event.aggregate_id),\n        \"payload\": event.payload\n    })\n\n# Metrics\ndef metrics_handler(event: DomainEvent):\n    \"\"\"Emit metrics for events\"\"\"\n    if event.event_type == \"job.completed\":\n        emit_metric(\n            name=\"job.processing_time\",\n            value=event.payload[\"processing_time_ms\"],\n            tags={\"model_id\": event.payload[\"model_id\"]}\n        )\n\n# Notifications\ndef notification_handler(event: DomainEvent):\n    \"\"\"Send notifications for critical events\"\"\"\n    if event.event_type == \"model.health.changed\":\n        if event.payload[\"new_state\"] == \"unhealthy\":\n            send_alert(f\"Model {event.payload['model_id']} unhealthy\")\n</code></pre>"},{"location":"data_models/events/domain_events/#10-related-documentation","title":"10. Related Documentation","text":"<ul> <li>Data Models - Domain model specifications</li> <li>API Design - Event-driven API patterns</li> </ul>"},{"location":"data_models/inference/batch/","title":"Batch Entity","text":"<p>Context: Inference Type: Entity Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/inference/batch/#1-overview","title":"1. Overview","text":"<p>The <code>Batch</code> entity groups multiple inference requests for the same model to enable efficient batch processing.</p>"},{"location":"data_models/inference/batch/#2-structure","title":"2. Structure","text":"<pre>ae454b2edca04a3e10679f6c7ed6aa88ab01402c2fc942fa398c7a07b91b4e9fbd5a86383096ee0104f365e739b4a4499befc61e1bae55e30b65ca55ed74043b</pre><pre>46b1c0aa597f1e35aba747a65e80e4dbd40df257c2105767901d601d904d5e04c8790f9d8b21ab604e8f72c7b608eaac9724b833ccef328552c45394b81aae14</pre>"},{"location":"data_models/inference/batch/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/inference/batch/#31-id","title":"3.1 id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique batch identifier</li> </ul>"},{"location":"data_models/inference/batch/#32-model_id","title":"3.2 model_id","text":"<ul> <li>Type: ModelId</li> <li>Constraint: MUST</li> <li>Description: Target model (all requests must match)</li> </ul>"},{"location":"data_models/inference/batch/#33-requests","title":"3.3 requests","text":"<ul> <li>Type: Collection\\ <li>Constraint: MUST</li> <li>Description: Requests in this batch</li>"},{"location":"data_models/inference/batch/#34-max_batch_size","title":"3.4 max_batch_size","text":"<ul> <li>Type: Integer</li> <li>Constraint: MUST</li> <li>Default: 32</li> <li>Description: Maximum requests per batch</li> </ul>"},{"location":"data_models/inference/batch/#35-max_wait_ms","title":"3.5 max_wait_ms","text":"<ul> <li>Type: Integer</li> <li>Constraint: MUST</li> <li>Default: 100</li> <li>Description: Maximum time to wait before processing</li> </ul>"},{"location":"data_models/inference/batch/#4-behavior","title":"4. Behavior","text":"<pre><code>def is_full(self) -&gt; bool:\n    \"\"\"Check if batch is at capacity\"\"\"\n    return len(self.requests) &gt;= self.max_batch_size\n\ndef is_ready(self) -&gt; bool:\n    \"\"\"Check if batch should be processed\"\"\"\n    # Full or wait time exceeded\n    if self.is_full():\n        return True\n\n    age_ms = (current_timestamp() - self.created_at).total_seconds() * 1000\n    return age_ms &gt;= self.max_wait_ms\n\ndef add_request(self, request: InferenceRequest) -&gt; bool:\n    \"\"\"Add request to batch\"\"\"\n    if self.is_full():\n        return False\n\n    if request.model_id != self.model_id:\n        raise ValueError(\"Request model_id doesn't match batch\")\n\n    self.requests.append(request)\n    self.updated_at = current_timestamp()\n    self.version += 1\n    return True\n\ndef get_size(self) -&gt; int:\n    \"\"\"Get current batch size\"\"\"\n    return len(self.requests)\n</code></pre>"},{"location":"data_models/inference/batch/#5-serialization","title":"5. Serialization","text":"<pre><code>{\n  \"id\": \"ff0e8400-e29b-41d4-a716-446655440011\",\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"requests\": [\n    {\"id\": \"dd0e8400-e29b-41d4-a716-446655440009\", ...},\n    {\"id\": \"dd0e8400-e29b-41d4-a716-446655440012\", ...}\n  ],\n  \"max_batch_size\": 32,\n  \"max_wait_ms\": 100,\n  \"created_at\": \"2025-12-04T10:30:00Z\",\n  \"updated_at\": \"2025-12-04T10:30:00.050Z\",\n  \"version\": 2\n}\n</code></pre>"},{"location":"data_models/inference/batch/#6-database-schema","title":"6. Database Schema","text":"<pre><code>CREATE TABLE batch (\n    id UUID PRIMARY KEY,\n    model_id VARCHAR(255) NOT NULL,\n    max_batch_size INTEGER NOT NULL DEFAULT 32,\n    max_wait_ms INTEGER NOT NULL DEFAULT 100,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1\n);\n\nCREATE TABLE batch_request (\n    batch_id UUID NOT NULL REFERENCES batch(id) ON DELETE CASCADE,\n    request_id UUID NOT NULL REFERENCES inference_request(id) ON DELETE CASCADE,\n    PRIMARY KEY (batch_id, request_id)\n);\n\nCREATE INDEX idx_batch_model_id ON batch(model_id);\nCREATE INDEX idx_batch_created ON batch(created_at);\n</code></pre>"},{"location":"data_models/inference/batch/#7-usage-example","title":"7. Usage Example","text":"<pre><code># Create batch\nbatch = Batch(\n    id=generate_uuid(),\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    requests=[],\n    max_batch_size=32,\n    max_wait_ms=100,\n    created_at=now(),\n    updated_at=now(),\n    version=1\n)\n\n# Add requests\nfor request in pending_requests:\n    if not batch.add_request(request):\n        # Batch full, create new one\n        break\n\n# Process when ready\nif batch.is_ready():\n    process_batch(batch)\n</code></pre>"},{"location":"data_models/inference/batch/#8-related-models","title":"8. Related Models","text":"<ul> <li>Inference Request - Batched requests</li> <li>Inference Queue - Manages batches</li> </ul>"},{"location":"data_models/inference/inference_job/","title":"Inference Job Entity","text":"<p>Context: Inference Type: Entity Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/inference/inference_job/#1-overview","title":"1. Overview","text":"<p>The <code>InferenceJob</code> entity tracks the lifecycle and results of an inference request, including status, timing, and output.</p>"},{"location":"data_models/inference/inference_job/#2-structure","title":"2. Structure","text":"<pre>d843d533144411980a15f5cee9eac0803d9c24980850259b1f8125ac22c9dcde177491116860126264b5bd6b400392642f62d3fa6002c10cb46dfc9d9b679a20</pre><pre>ebc070a6bf2954a23a452be75e524db9a36d07844ce36b98ee8af35be568c6a7edf073819337f734528e5293480c73c09613a4982499e57a04d7ca2831eddcc9</pre>"},{"location":"data_models/inference/inference_job/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/inference/inference_job/#31-id","title":"3.1 id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique job identifier</li> </ul>"},{"location":"data_models/inference/inference_job/#32-request","title":"3.2 request","text":"<ul> <li>Type: InferenceRequest</li> <li>Constraint: MUST</li> <li>Description: Original inference request</li> </ul>"},{"location":"data_models/inference/inference_job/#33-status","title":"3.3 status","text":"<ul> <li>Type: JobStatus</li> <li>Constraint: MUST</li> <li>Description: Current job status</li> </ul>"},{"location":"data_models/inference/inference_job/#34-result","title":"3.4 result","text":"<ul> <li>Type: OutputData</li> <li>Constraint: MAY</li> <li>Description: Inference output (when completed)</li> </ul>"},{"location":"data_models/inference/inference_job/#35-error_message","title":"3.5 error_message","text":"<ul> <li>Type: String</li> <li>Constraint: MAY</li> <li>Description: Error details (when failed)</li> </ul>"},{"location":"data_models/inference/inference_job/#36-started_at","title":"3.6 started_at","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MAY</li> <li>Description: When processing began</li> </ul>"},{"location":"data_models/inference/inference_job/#37-completed_at","title":"3.7 completed_at","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MAY</li> <li>Description: When processing finished</li> </ul>"},{"location":"data_models/inference/inference_job/#38-processing_time_ms-derived","title":"3.8 processing_time_ms (Derived)","text":"<ul> <li>Type: Float</li> <li>Calculation: <code>(completed_at - started_at) * 1000</code></li> </ul>"},{"location":"data_models/inference/inference_job/#4-behavior","title":"4. Behavior","text":"<pre><code>def is_complete(self) -&gt; bool:\n    \"\"\"Check if job is in terminal state\"\"\"\n    return self.status.is_terminal()\n\ndef update_status(self, status: JobStatus) -&gt; None:\n    \"\"\"Update job status\"\"\"\n    self.status = status\n    if status.status == JobStatusEnum.PROCESSING and not self.started_at:\n        self.started_at = current_timestamp()\n    elif status.is_terminal() and not self.completed_at:\n        self.completed_at = current_timestamp()\n    self.updated_at = current_timestamp()\n    self.version += 1\n\ndef set_result(self, output: OutputData) -&gt; void:\n    \"\"\"Set successful result\"\"\"\n    self.result = output\n    self.update_status(JobStatus(status=JobStatusEnum.COMPLETED))\n\ndef set_error(self, error: String) -&gt; void:\n    \"\"\"Set error result\"\"\"\n    self.error_message = error\n    self.update_status(JobStatus(status=JobStatusEnum.FAILED))\n</code></pre>"},{"location":"data_models/inference/inference_job/#5-serialization","title":"5. Serialization","text":"<pre><code>{\n  \"id\": \"ee0e8400-e29b-41d4-a716-446655440010\",\n  \"request\": {\n    \"id\": \"dd0e8400-e29b-41d4-a716-446655440009\",\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"input_data\": {...}\n  },\n  \"status\": \"completed\",\n  \"result\": {\n    \"task_type\": \"txt2embed\",\n    \"embedding\": [0.123, -0.456, ...]\n  },\n  \"started_at\": \"2025-12-04T10:30:05Z\",\n  \"completed_at\": \"2025-12-04T10:30:05.123Z\",\n  \"created_at\": \"2025-12-04T10:30:00Z\",\n  \"updated_at\": \"2025-12-04T10:30:05.123Z\",\n  \"version\": 3\n}\n</code></pre>"},{"location":"data_models/inference/inference_job/#6-database-schema","title":"6. Database Schema","text":"<pre><code>CREATE TABLE inference_job (\n    id UUID PRIMARY KEY,\n    request_id UUID NOT NULL REFERENCES inference_request(id) ON DELETE CASCADE,\n    status job_status_enum NOT NULL DEFAULT 'pending',\n    result JSONB,\n    error_message TEXT,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1\n);\n\nCREATE INDEX idx_inference_job_status ON inference_job(status);\nCREATE INDEX idx_inference_job_request ON inference_job(request_id);\nCREATE INDEX idx_inference_job_created ON inference_job(created_at);\n</code></pre>"},{"location":"data_models/inference/inference_job/#7-related-models","title":"7. Related Models","text":"<ul> <li>Inference Request - Job input</li> <li>Job Status - Current state</li> <li>Output Data - Job result</li> <li>Inference Queue - Manages jobs</li> </ul>"},{"location":"data_models/inference/inference_queue/","title":"Inference Queue Aggregate","text":"<p>Context: Inference Type: Aggregate Root Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/inference/inference_queue/#1-overview","title":"1. Overview","text":"<p>The <code>InferenceQueue</code> aggregate manages the lifecycle of inference jobs, including queuing, prioritization, batching, and execution coordination.</p>"},{"location":"data_models/inference/inference_queue/#2-structure","title":"2. Structure","text":"<pre>841fe41664d9deb48d71727554c5b31ffab0674a2080fdb01c089944cd56ea978a09723885dd1c49bc75004f508676e4913d10916274d565a054f680d0d3afeb</pre><pre>e50675a2c33926eb21cd4140b5407270ee48e8eaa21299c6a3a7fc82402320a44df322edaef3a55834c6285ab90ce90d923cad821b9f67c16eb80a1a5cf57c57</pre>"},{"location":"data_models/inference/inference_queue/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/inference/inference_queue/#31-id","title":"3.1 id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique queue identifier</li> </ul>"},{"location":"data_models/inference/inference_queue/#32-jobs","title":"3.2 jobs","text":"<ul> <li>Type: PriorityQueue\\ <li>Constraint: MUST</li> <li>Description: Jobs prioritized by request priority</li>"},{"location":"data_models/inference/inference_queue/#33-batches","title":"3.3 batches","text":"<ul> <li>Type: Map <li>Constraint: MUST</li> <li>Description: Active batches per model</li>"},{"location":"data_models/inference/inference_queue/#34-max_queue_size","title":"3.4 max_queue_size","text":"<ul> <li>Type: Integer</li> <li>Constraint: MUST</li> <li>Default: 1000</li> <li>Description: Maximum jobs in queue</li> </ul>"},{"location":"data_models/inference/inference_queue/#35-events","title":"3.5 events","text":"<ul> <li>Type: Collection\\ <li>Description: Domain events</li>"},{"location":"data_models/inference/inference_queue/#4-behavior","title":"4. Behavior","text":""},{"location":"data_models/inference/inference_queue/#41-request-submission","title":"4.1 Request Submission","text":"<pre><code>def submit_request(self, request: InferenceRequest) -&gt; InferenceJob:\n    \"\"\"Submit inference request and create job\"\"\"\n    # Check queue capacity\n    if len(self.jobs) &gt;= self.max_queue_size:\n        raise QueueFullError(\"Queue at capacity\")\n\n    # Create job\n    job = InferenceJob(\n        id=generate_uuid(),\n        request=request,\n        status=JobStatus(status=JobStatusEnum.PENDING),\n        created_at=now(),\n        updated_at=now(),\n        version=1\n    )\n\n    # Add to priority queue\n    self.jobs.put((job.request.priority.score, job))\n\n    # Update job status\n    job.update_status(JobStatus(status=JobStatusEnum.QUEUED))\n\n    self.updated_at = now()\n    self.version += 1\n\n    # Generate event\n    self.addEvent(JobSubmittedEvent(\n        queue_id=self.id,\n        job_id=job.id,\n        timestamp=now()\n    ))\n\n    return job\n</code></pre>"},{"location":"data_models/inference/inference_queue/#42-job-retrieval","title":"4.2 Job Retrieval","text":"<pre><code>def get_next_job(self) -&gt; InferenceJob:\n    \"\"\"Get highest priority job from queue\"\"\"\n    if self.jobs.empty():\n        return None\n\n    _, job = self.jobs.get()\n    job.update_status(JobStatus(status=JobStatusEnum.PROCESSING))\n\n    return job\n</code></pre>"},{"location":"data_models/inference/inference_queue/#43-batch-management","title":"4.3 Batch Management","text":"<pre><code>def get_ready_batch(self, model_id: ModelId) -&gt; Batch:\n    \"\"\"Get or create batch for model\"\"\"\n    # Get existing batch\n    if model_id in self.batches:\n        batch = self.batches[model_id]\n        if batch.is_ready():\n            del self.batches[model_id]\n            return batch\n        return None\n\n    # Create new batch\n    batch = Batch(\n        id=generate_uuid(),\n        model_id=model_id,\n        requests=[],\n        max_batch_size=32,\n        max_wait_ms=100,\n        created_at=now(),\n        updated_at=now(),\n        version=1\n    )\n\n    self.batches[model_id] = batch\n    return None\n</code></pre>"},{"location":"data_models/inference/inference_queue/#44-job-cancellation","title":"4.4 Job Cancellation","text":"<pre><code>def cancel_job(self, job_id: UUID) -&gt; void:\n    \"\"\"Cancel a pending or queued job\"\"\"\n    job = self.get_job(job_id)\n\n    if not job.status.can_cancel():\n        raise InvalidOperationError(\"Job cannot be cancelled\")\n\n    job.update_status(JobStatus(status=JobStatusEnum.CANCELLED))\n\n    self.updated_at = now()\n    self.version += 1\n\n    # Generate event\n    self.addEvent(JobCancelledEvent(\n        queue_id=self.id,\n        job_id=job_id,\n        timestamp=now()\n    ))\n</code></pre>"},{"location":"data_models/inference/inference_queue/#5-domain-events","title":"5. Domain Events","text":""},{"location":"data_models/inference/inference_queue/#51-jobsubmittedevent","title":"5.1 JobSubmittedEvent","text":"<pre><code>class JobSubmittedEvent:\n    queue_id: UUID\n    job_id: UUID\n    model_id: ModelId\n    priority: Priority\n    timestamp: Timestamp\n</code></pre>"},{"location":"data_models/inference/inference_queue/#52-jobcompletedevent","title":"5.2 JobCompletedEvent","text":"<pre><code>class JobCompletedEvent:\n    queue_id: UUID\n    job_id: UUID\n    processing_time_ms: Float\n    timestamp: Timestamp\n</code></pre>"},{"location":"data_models/inference/inference_queue/#53-jobcancelledevent","title":"5.3 JobCancelledEvent","text":"<pre><code>class JobCancelledEvent:\n    queue_id: UUID\n    job_id: UUID\n    timestamp: Timestamp\n</code></pre>"},{"location":"data_models/inference/inference_queue/#6-serialization","title":"6. Serialization","text":"<pre><code>{\n  \"id\": \"11000000-e29b-41d4-a716-446655440013\",\n  \"max_queue_size\": 1000,\n  \"jobs\": [\n    {\n      \"id\": \"ee0e8400-e29b-41d4-a716-446655440010\",\n      \"status\": \"queued\"\n    }\n  ],\n  \"batches\": {\n    \"sentence-transformers/all-MiniLM-L6-v2\": {\n      \"id\": \"ff0e8400-e29b-41d4-a716-446655440011\",\n      \"size\": 5\n    }\n  },\n  \"created_at\": \"2025-12-04T10:00:00Z\",\n  \"updated_at\": \"2025-12-04T10:30:15Z\",\n  \"version\": 42\n}\n</code></pre>"},{"location":"data_models/inference/inference_queue/#7-database-schema","title":"7. Database Schema","text":"<pre><code>CREATE TABLE inference_queue (\n    id UUID PRIMARY KEY,\n    max_queue_size INTEGER NOT NULL DEFAULT 1000,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1\n);\n\n-- inference_job and batch tables reference queue\nALTER TABLE inference_job ADD COLUMN queue_id UUID REFERENCES inference_queue(id);\nALTER TABLE batch ADD COLUMN queue_id UUID REFERENCES inference_queue(id);\n\nCREATE TRIGGER update_inference_queue_timestamp\nBEFORE UPDATE ON inference_queue\nFOR EACH ROW\nEXECUTE FUNCTION update_timestamp_and_version();\n</code></pre>"},{"location":"data_models/inference/inference_queue/#8-usage-examples","title":"8. Usage Examples","text":""},{"location":"data_models/inference/inference_queue/#81-submitting-request","title":"8.1 Submitting Request","text":"<pre><code># Submit request\nrequest = InferenceRequest(...)\njob = queue.submit_request(request)\n\n# Poll for result\nwhile not job.is_complete():\n    time.sleep(0.1)\n    job = queue.get_job(job.id)\n\nresult = job.result\n</code></pre>"},{"location":"data_models/inference/inference_queue/#82-processing-loop","title":"8.2 Processing Loop","text":"<pre><code>while True:\n    # Get next job\n    job = queue.get_next_job()\n    if not job:\n        time.sleep(0.1)\n        continue\n\n    try:\n        # Process\n        result = model.infer(job.request.input_data)\n        job.set_result(result)\n    except Exception as e:\n        job.set_error(str(e))\n\n    # Publish events\n    for event in queue.events:\n        event_bus.publish(event)\n    queue.clearEvents()\n</code></pre>"},{"location":"data_models/inference/inference_queue/#9-related-models","title":"9. Related Models","text":"<ul> <li>Inference Job - Managed by queue</li> <li>Inference Request - Job input</li> <li>Batch - Batching optimization</li> <li>Job Status - Job state</li> <li>Priority - Queue ordering</li> </ul>"},{"location":"data_models/inference/inference_request/","title":"Inference Request Entity","text":"<p>Context: Inference Type: Entity Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/inference/inference_request/#1-overview","title":"1. Overview","text":"<p>The <code>InferenceRequest</code> entity represents a single inference request with input data, target model, and priority.</p>"},{"location":"data_models/inference/inference_request/#2-structure","title":"2. Structure","text":"<pre>453d5e2328472267e6cdc932f8c2499cc35395f0f1a109c47d732dc745bb0794e6426d9d24abfe0bc2e67d73cb23b2c8ac7f18ac11245f31a3922526a261fd3a</pre><pre>6c25e00875d7b698d11a872aef5d09c7f865d074005e04e1f252a7ad83d1293728a47bc487781a80c3bacd489f8ae0b8c5357c780f89f5acb18e21216adea7d5</pre>"},{"location":"data_models/inference/inference_request/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/inference/inference_request/#31-id","title":"3.1 id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique request identifier</li> </ul>"},{"location":"data_models/inference/inference_request/#32-model_id","title":"3.2 model_id","text":"<ul> <li>Type: ModelId</li> <li>Constraint: MUST</li> <li>Description: Target model for inference</li> </ul>"},{"location":"data_models/inference/inference_request/#33-input_data","title":"3.3 input_data","text":"<ul> <li>Type: InputData</li> <li>Constraint: MUST</li> <li>Description: Task-specific input</li> </ul>"},{"location":"data_models/inference/inference_request/#34-priority","title":"3.4 priority","text":"<ul> <li>Type: Priority</li> <li>Constraint: MUST</li> <li>Default: MEDIUM</li> <li>Description: Request priority level</li> </ul>"},{"location":"data_models/inference/inference_request/#35-timeout_seconds","title":"3.5 timeout_seconds","text":"<ul> <li>Type: Integer</li> <li>Constraint: MAY</li> <li>Default: 60</li> <li>Description: Maximum processing time</li> </ul>"},{"location":"data_models/inference/inference_request/#36-metadata","title":"3.6 metadata","text":"<ul> <li>Type: Map <li>Constraint: MAY</li> <li>Description: User-defined metadata (tracing, correlation IDs)</li>"},{"location":"data_models/inference/inference_request/#4-serialization","title":"4. Serialization","text":"<pre><code>{\n  \"id\": \"dd0e8400-e29b-41d4-a716-446655440009\",\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"input_data\": {\n    \"task_type\": \"txt2embed\",\n    \"text\": \"Hello world\"\n  },\n  \"priority\": \"high\",\n  \"timeout_seconds\": 60,\n  \"metadata\": {\n    \"user_id\": \"user123\",\n    \"trace_id\": \"abc-def-ghi\"\n  },\n  \"created_at\": \"2025-12-04T10:30:00Z\",\n  \"updated_at\": \"2025-12-04T10:30:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/inference/inference_request/#5-database-schema","title":"5. Database Schema","text":"<pre><code>CREATE TABLE inference_request (\n    id UUID PRIMARY KEY,\n    model_id VARCHAR(255) NOT NULL,\n    input_data JSONB NOT NULL,\n    priority priority_enum NOT NULL DEFAULT 'medium',\n    timeout_seconds INTEGER DEFAULT 60,\n    metadata JSONB,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1\n);\n\nCREATE INDEX idx_inference_request_model_id ON inference_request(model_id);\nCREATE INDEX idx_inference_request_priority ON inference_request(priority);\n</code></pre>"},{"location":"data_models/inference/inference_request/#6-related-models","title":"6. Related Models","text":"<ul> <li>Inference Job - Wraps InferenceRequest with status</li> <li>Batch - Groups multiple requests</li> <li>Input Data - Request input</li> </ul>"},{"location":"data_models/inference/input_data/","title":"Input Data Model","text":"<p>Context: Inference Type: Polymorphic Value Object Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/inference/input_data/#1-overview","title":"1. Overview","text":"<p><code>InputData</code> represents task-specific input for inference requests. Structure varies by TaskType.</p>"},{"location":"data_models/inference/input_data/#2-task-specific-structures","title":"2. Task-Specific Structures","text":""},{"location":"data_models/inference/input_data/#21-txt2embed-input","title":"2.1 txt2embed Input","text":"<pre><code>{\n  \"task_type\": \"txt2embed\",\n  \"text\": \"Hello world\",\n  \"normalize\": true\n}\n</code></pre>"},{"location":"data_models/inference/input_data/#22-txt2txt-input","title":"2.2 txt2txt Input","text":"<pre><code>{\n  \"task_type\": \"txt2txt\",\n  \"prompt\": \"Translate to French: Hello\",\n  \"max_tokens\": 100,\n  \"temperature\": 0.7\n}\n</code></pre>"},{"location":"data_models/inference/input_data/#23-txt2img-input","title":"2.3 txt2img Input","text":"<pre><code>{\n  \"task_type\": \"txt2img\",\n  \"prompt\": \"A serene landscape\",\n  \"width\": 512,\n  \"height\": 512,\n  \"steps\": 50\n}\n</code></pre>"},{"location":"data_models/inference/input_data/#24-img2txt-input","title":"2.4 img2txt Input","text":"<pre><code>{\n  \"task_type\": \"img2txt\",\n  \"image\": \"&lt;base64_encoded_image&gt;\",\n  \"max_length\": 100\n}\n</code></pre>"},{"location":"data_models/inference/input_data/#3-common-fields","title":"3. Common Fields","text":"<ul> <li>task_type: TaskType identifier</li> <li>Additional fields: Task-specific parameters</li> </ul>"},{"location":"data_models/inference/input_data/#4-validation","title":"4. Validation","text":"<p>Input data MUST conform to TaskType schema (see TaskType model for schemas).</p>"},{"location":"data_models/inference/input_data/#5-protocol-buffers","title":"5. Protocol Buffers","text":"<pre><code>message InputData {\n  string task_type = 1;\n  google.protobuf.Struct data = 2;  // Flexible task-specific data\n}\n</code></pre>"},{"location":"data_models/inference/input_data/#6-related-models","title":"6. Related Models","text":"<ul> <li>Task Type - Defines input schemas</li> <li>Inference Request - Contains InputData</li> <li>Output Data - Corresponding output</li> </ul>"},{"location":"data_models/inference/job_status/","title":"Job Status Value Object","text":"<p>Context: Inference Type: Value Object Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/inference/job_status/#1-overview","title":"1. Overview","text":"<p>The <code>JobStatus</code> value object represents the current state of an inference job in the processing pipeline.</p>"},{"location":"data_models/inference/job_status/#2-structure","title":"2. Structure","text":"<pre>7f08736f14d5034624c0c8b46a168fa6033c86ac6dd7f82fe72205097ee37fb73e51b452a92f42b74fd83be61fdecad4973dc21e3cf2c5c390c6bb823e9c21f9</pre><pre>147bcdee2d736f3226fa17fb37e7769359436bf7ca1bd4fe4e1f94a673ac6b36b226a94491c74a7e4441b0ec6eaed8003fb1b6e81410703fd962c8945d5071ea</pre>"},{"location":"data_models/inference/job_status/#3-status-enumeration","title":"3. Status Enumeration","text":"Status Description Terminal Cancellable <code>PENDING</code> Job created, awaiting queue No Yes <code>QUEUED</code> In queue, waiting for processing No Yes <code>PROCESSING</code> Currently being processed No No <code>COMPLETED</code> Successfully completed Yes No <code>FAILED</code> Processing failed Yes No <code>CANCELLED</code> Cancelled by user Yes No"},{"location":"data_models/inference/job_status/#4-behavior","title":"4. Behavior","text":"<pre><code>def is_terminal(self) -&gt; bool:\n    \"\"\"Check if status is final\"\"\"\n    return self.status in [\n        JobStatusEnum.COMPLETED,\n        JobStatusEnum.FAILED,\n        JobStatusEnum.CANCELLED\n    ]\n\ndef can_cancel(self) -&gt; bool:\n    \"\"\"Check if job can be cancelled\"\"\"\n    return self.status in [\n        JobStatusEnum.PENDING,\n        JobStatusEnum.QUEUED\n    ]\n</code></pre>"},{"location":"data_models/inference/job_status/#5-serialization","title":"5. Serialization","text":"<pre><code>{\n  \"status\": \"processing\"\n}\n</code></pre> <pre><code>enum JobStatus {\n  JOB_STATUS_UNSPECIFIED = 0;\n  JOB_STATUS_PENDING = 1;\n  JOB_STATUS_QUEUED = 2;\n  JOB_STATUS_PROCESSING = 3;\n  JOB_STATUS_COMPLETED = 4;\n  JOB_STATUS_FAILED = 5;\n  JOB_STATUS_CANCELLED = 6;\n}\n</code></pre>"},{"location":"data_models/inference/job_status/#6-related-models","title":"6. Related Models","text":"<ul> <li>Inference Job - Uses JobStatus</li> <li>Inference Queue - Manages jobs by status</li> </ul>"},{"location":"data_models/inference/output_data/","title":"Output Data Model","text":"<p>Context: Inference Type: Polymorphic Value Object Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/inference/output_data/#1-overview","title":"1. Overview","text":"<p><code>OutputData</code> represents task-specific inference results. Structure varies by TaskType.</p>"},{"location":"data_models/inference/output_data/#2-task-specific-structures","title":"2. Task-Specific Structures","text":""},{"location":"data_models/inference/output_data/#21-txt2embed-output","title":"2.1 txt2embed Output","text":"<pre><code>{\n  \"task_type\": \"txt2embed\",\n  \"embedding\": [0.123, -0.456, 0.789, ...]\n}\n</code></pre>"},{"location":"data_models/inference/output_data/#22-txt2txt-output","title":"2.2 txt2txt Output","text":"<pre><code>{\n  \"task_type\": \"txt2txt\",\n  \"text\": \"Bonjour\",\n  \"tokens_used\": 15\n}\n</code></pre>"},{"location":"data_models/inference/output_data/#23-txt2img-output","title":"2.3 txt2img Output","text":"<pre><code>{\n  \"task_type\": \"txt2img\",\n  \"image\": \"&lt;base64_encoded_image&gt;\",\n  \"format\": \"png\"\n}\n</code></pre>"},{"location":"data_models/inference/output_data/#24-classification-output","title":"2.4 classification Output","text":"<pre><code>{\n  \"task_type\": \"classification\",\n  \"classes\": [\n    {\"label\": \"positive\", \"score\": 0.95},\n    {\"label\": \"negative\", \"score\": 0.05}\n  ]\n}\n</code></pre>"},{"location":"data_models/inference/output_data/#3-common-fields","title":"3. Common Fields","text":"<ul> <li>task_type: TaskType identifier</li> <li>Additional fields: Task-specific results</li> </ul>"},{"location":"data_models/inference/output_data/#4-protocol-buffers","title":"4. Protocol Buffers","text":"<pre><code>message OutputData {\n  string task_type = 1;\n  google.protobuf.Struct data = 2;  // Flexible task-specific data\n}\n</code></pre>"},{"location":"data_models/inference/output_data/#5-related-models","title":"5. Related Models","text":"<ul> <li>Task Type - Defines output schemas</li> <li>Inference Job - Contains OutputData as result</li> <li>Input Data - Corresponding input</li> </ul>"},{"location":"data_models/inference/priority/","title":"Priority Value Object","text":"<p>Context: Inference Type: Value Object Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/inference/priority/#1-overview","title":"1. Overview","text":"<p>The <code>Priority</code> value object defines the urgency level of inference requests for queue prioritization.</p>"},{"location":"data_models/inference/priority/#2-structure","title":"2. Structure","text":"<pre>0c0b2d662d7bd57420fed40cbce8c82a04e6bc60382dcebc26054404914d9c9041e9739ba4423ae76648a3c608605d4a09e377544c870cbafd509c36660a8f1f</pre><pre>31714bff30fc48d2a895bacd680240472fde98010690a3cb177d0da7613d02593ef8d82c807d32169f0996e1e0c2df57e741488b5d5374dcdb610c9e26b967fa</pre>"},{"location":"data_models/inference/priority/#3-priority-levels","title":"3. Priority Levels","text":"Level Score Use Case <code>LOW</code> 1 Batch processing, background tasks <code>MEDIUM</code> 5 Standard API requests <code>HIGH</code> 10 Interactive user requests <code>CRITICAL</code> 20 Real-time, time-sensitive operations"},{"location":"data_models/inference/priority/#4-behavior","title":"4. Behavior","text":"<pre><code>@property\ndef score(self) -&gt; int:\n    \"\"\"Get numeric priority score for sorting\"\"\"\n    return self.level.value\n\ndef compare_to(self, other: Priority) -&gt; int:\n    \"\"\"Compare priorities (for sorting)\"\"\"\n    return self.score - other.score\n</code></pre>"},{"location":"data_models/inference/priority/#5-serialization","title":"5. Serialization","text":"<pre><code>{\n  \"priority\": \"high\"\n}\n</code></pre> <pre><code>enum Priority {\n  PRIORITY_UNSPECIFIED = 0;\n  PRIORITY_LOW = 1;\n  PRIORITY_MEDIUM = 5;\n  PRIORITY_HIGH = 10;\n  PRIORITY_CRITICAL = 20;\n}\n</code></pre>"},{"location":"data_models/inference/priority/#6-usage-example","title":"6. Usage Example","text":"<pre><code># Priority queue sorting\njobs.sort(key=lambda j: -j.request.priority.score)  # High to low\n</code></pre>"},{"location":"data_models/inference/priority/#7-related-models","title":"7. Related Models","text":"<ul> <li>Inference Request - Includes Priority</li> <li>Inference Queue - Uses Priority for ordering</li> </ul>"},{"location":"data_models/lifecycle/health_status/","title":"Health Status Value Object","text":"<p>Context: Lifecycle Type: Value Object Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/lifecycle/health_status/#1-overview","title":"1. Overview","text":"<p>The <code>HealthStatus</code> value object represents the health state of a loaded model, including readiness for inference and diagnostic information.</p>"},{"location":"data_models/lifecycle/health_status/#2-structure","title":"2. Structure","text":""},{"location":"data_models/lifecycle/health_status/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>012465637ce73d39167bfd96611df86bb6d1f1d787a57ad56e58a80a9cdc8d5c3af9a59b5b18eb0ef90a5e68e6246b3d480724ede11e9e85f505396f1eb144db</pre><pre>e64018f94cd66d1b4a7810ccd164c4e5d2f75d4c7e4e62635725072880a1308a20109b4c5131dde7498f00d08f49401e5502a3e52246a7a835f6bb1bcc2c405f</pre>"},{"location":"data_models/lifecycle/health_status/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/lifecycle/health_status/#31-healthy","title":"3.1 healthy","text":"<ul> <li>Type: Boolean</li> <li>Constraint: MUST, Immutable</li> <li>Description: Overall health status</li> <li>Validation:</li> <li><code>true</code>: Model operational and passing health checks</li> <li><code>false</code>: Model failing health checks</li> </ul>"},{"location":"data_models/lifecycle/health_status/#32-last_check","title":"3.2 last_check","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MUST, Immutable</li> <li>Description: When health check was performed</li> <li>Validation:</li> <li>MUST be valid ISO 8601 timestamp</li> </ul>"},{"location":"data_models/lifecycle/health_status/#33-error_message","title":"3.3 error_message","text":"<ul> <li>Type: String</li> <li>Constraint: MAY, Immutable</li> <li>Description: Error details if unhealthy</li> <li>Validation:</li> <li>Length: 0-1000 characters</li> <li>Should be present when <code>healthy = false</code></li> </ul>"},{"location":"data_models/lifecycle/health_status/#34-consecutive_failures","title":"3.4 consecutive_failures","text":"<ul> <li>Type: Integer</li> <li>Constraint: MAY, Immutable</li> <li>Description: Number of consecutive failed health checks</li> <li>Validation:</li> <li>MUST be &gt;= 0</li> <li>Used to trigger automatic restarts</li> </ul>"},{"location":"data_models/lifecycle/health_status/#35-response_time_ms","title":"3.5 response_time_ms","text":"<ul> <li>Type: Float</li> <li>Constraint: MAY, Immutable</li> <li>Description: Health check response time in milliseconds</li> <li>Validation:</li> <li>MUST be &gt;= 0</li> <li>Used to detect performance degradation</li> </ul>"},{"location":"data_models/lifecycle/health_status/#4-constraints","title":"4. Constraints","text":""},{"location":"data_models/lifecycle/health_status/#41-value-object-constraints","title":"4.1 Value Object Constraints","text":"<ul> <li>All fields MUST be immutable after creation</li> <li>Equality based on structural comparison</li> <li>No identity field required</li> </ul>"},{"location":"data_models/lifecycle/health_status/#42-business-rules","title":"4.2 Business Rules","text":"<ul> <li>If <code>healthy = false</code>, <code>error_message</code> SHOULD be present</li> <li><code>consecutive_failures</code> resets to 0 when <code>healthy = true</code></li> <li>High <code>response_time_ms</code> may indicate degraded performance</li> <li>Exceeding failure threshold triggers state transition to UNHEALTHY</li> </ul>"},{"location":"data_models/lifecycle/health_status/#5-validation","title":"5. Validation","text":""},{"location":"data_models/lifecycle/health_status/#51-syntax-validation","title":"5.1 Syntax Validation","text":"<pre><code>def validate_syntax(status: HealthStatus) -&gt; ValidationResult:\n    errors = []\n\n    if status.healthy is None:\n        errors.append(\"healthy is required\")\n\n    if not status.last_check:\n        errors.append(\"last_check is required\")\n\n    if status.consecutive_failures is not None and status.consecutive_failures &lt; 0:\n        errors.append(\"consecutive_failures must be &gt;= 0\")\n\n    if status.response_time_ms is not None and status.response_time_ms &lt; 0:\n        errors.append(\"response_time_ms must be &gt;= 0\")\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#52-semantic-validation","title":"5.2 Semantic Validation","text":"<pre><code>def validate_semantic(status: HealthStatus) -&gt; ValidationResult:\n    warnings = []\n\n    # Unhealthy without error message\n    if not status.healthy and not status.error_message:\n        warnings.append(\"Unhealthy status should include error_message\")\n\n    # High consecutive failures\n    if status.consecutive_failures and status.consecutive_failures &gt; 3:\n        warnings.append(\n            f\"High consecutive failures: {status.consecutive_failures}\"\n        )\n\n    # Slow response time\n    if status.response_time_ms and status.response_time_ms &gt; 5000:\n        warnings.append(\n            f\"Slow health check response: {status.response_time_ms}ms\"\n        )\n\n    return ValidationResult(valid=True, warnings=warnings)\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#6-behavior","title":"6. Behavior","text":""},{"location":"data_models/lifecycle/health_status/#61-creation","title":"6.1 Creation","text":"<pre><code>def create_healthy_status(response_time_ms: float = None) -&gt; HealthStatus:\n    \"\"\"Create a healthy status\"\"\"\n    return HealthStatus(\n        healthy=True,\n        last_check=current_timestamp(),\n        error_message=None,\n        consecutive_failures=0,\n        response_time_ms=response_time_ms\n    )\n\ndef create_unhealthy_status(\n    error_message: str,\n    consecutive_failures: int = 1,\n    response_time_ms: float = None\n) -&gt; HealthStatus:\n    \"\"\"Create an unhealthy status\"\"\"\n    return HealthStatus(\n        healthy=False,\n        last_check=current_timestamp(),\n        error_message=error_message,\n        consecutive_failures=consecutive_failures,\n        response_time_ms=response_time_ms\n    )\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#62-status-queries","title":"6.2 Status Queries","text":"<pre><code>def is_degraded(self) -&gt; bool:\n    \"\"\"Check if performance is degraded but still operational\"\"\"\n    if not self.healthy:\n        return False\n\n    # Slow response time indicates degradation\n    if self.response_time_ms and self.response_time_ms &gt; 3000:\n        return True\n\n    return False\n\ndef requires_restart(self) -&gt; bool:\n    \"\"\"Check if model should be restarted\"\"\"\n    # Too many consecutive failures\n    if self.consecutive_failures and self.consecutive_failures &gt;= 5:\n        return True\n\n    return False\n\ndef is_stale(self, max_age_seconds: int = 60) -&gt; bool:\n    \"\"\"Check if health check is stale\"\"\"\n    age_seconds = (current_timestamp() - self.last_check).total_seconds()\n    return age_seconds &gt; max_age_seconds\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#63-value-object-methods","title":"6.3 Value Object Methods","text":"<pre><code>def equals(self, other: HealthStatus) -&gt; bool:\n    \"\"\"Structural equality\"\"\"\n    return (\n        self.healthy == other.healthy and\n        self.last_check == other.last_check\n    )\n\ndef hash(self) -&gt; int:\n    \"\"\"Hash based on values\"\"\"\n    return hash((self.healthy, self.last_check))\n\ndef toString(self) -&gt; str:\n    \"\"\"Human-readable representation\"\"\"\n    status_str = \"healthy\" if self.healthy else \"unhealthy\"\n    details = \"\"\n\n    if not self.healthy and self.error_message:\n        details = f\" ({self.error_message})\"\n    elif self.consecutive_failures and self.consecutive_failures &gt; 0:\n        details = f\" ({self.consecutive_failures} failures)\"\n\n    return f\"{status_str}{details}\"\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#7-serialization","title":"7. Serialization","text":""},{"location":"data_models/lifecycle/health_status/#71-json-example","title":"7.1 JSON Example","text":"<pre><code>{\n  \"healthy\": true,\n  \"last_check\": \"2025-12-04T10:30:15Z\",\n  \"consecutive_failures\": 0,\n  \"response_time_ms\": 45.3\n}\n</code></pre> <pre><code>{\n  \"healthy\": false,\n  \"last_check\": \"2025-12-04T10:30:15Z\",\n  \"error_message\": \"Model inference timeout after 30s\",\n  \"consecutive_failures\": 3,\n  \"response_time_ms\": 30000.0\n}\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#72-protocol-buffers","title":"7.2 Protocol Buffers","text":"<pre><code>message HealthStatus {\n  bool healthy = 1;\n  google.protobuf.Timestamp last_check = 2;\n  string error_message = 3;\n  int32 consecutive_failures = 4;\n  float response_time_ms = 5;\n}\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#8-database-schema","title":"8. Database Schema","text":"<pre><code>-- Embedded in loaded_model table as JSONB\nCREATE TABLE loaded_model (\n    ...\n    health_status JSONB NOT NULL,\n    ...\n);\n\n-- Or separate table for history\nCREATE TABLE health_check_history (\n    loaded_model_id UUID NOT NULL REFERENCES loaded_model(id) ON DELETE CASCADE,\n    healthy BOOLEAN NOT NULL,\n    last_check TIMESTAMP NOT NULL DEFAULT NOW(),\n    error_message TEXT,\n    consecutive_failures INTEGER DEFAULT 0,\n    response_time_ms REAL,\n    PRIMARY KEY (loaded_model_id, last_check)\n);\n\nCREATE INDEX idx_health_check_timestamp ON health_check_history(last_check);\nCREATE INDEX idx_health_check_model ON health_check_history(loaded_model_id);\nCREATE INDEX idx_health_check_status ON health_check_history(healthy);\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#9-usage-examples","title":"9. Usage Examples","text":""},{"location":"data_models/lifecycle/health_status/#91-performing-health-check","title":"9.1 Performing Health Check","text":"<pre><code>def perform_health_check(model) -&gt; HealthStatus:\n    \"\"\"Execute health check on loaded model\"\"\"\n    start_time = time.time()\n\n    try:\n        # Simple inference test\n        result = model.inference(test_input)\n        response_time_ms = (time.time() - start_time) * 1000\n\n        if result is None:\n            return create_unhealthy_status(\n                error_message=\"Model returned None\",\n                response_time_ms=response_time_ms\n            )\n\n        return create_healthy_status(response_time_ms=response_time_ms)\n\n    except TimeoutError as e:\n        response_time_ms = (time.time() - start_time) * 1000\n        return create_unhealthy_status(\n            error_message=f\"Inference timeout: {str(e)}\",\n            response_time_ms=response_time_ms\n        )\n    except Exception as e:\n        response_time_ms = (time.time() - start_time) * 1000\n        return create_unhealthy_status(\n            error_message=f\"Inference failed: {str(e)}\",\n            response_time_ms=response_time_ms\n        )\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#92-health-monitoring-loop","title":"9.2 Health Monitoring Loop","text":"<pre><code>def monitor_model_health(loaded_model, check_interval_seconds: int = 30):\n    \"\"\"Continuously monitor model health\"\"\"\n    previous_status = loaded_model.health_status\n\n    while loaded_model.state.is_operational():\n        # Perform health check\n        current_status = perform_health_check(loaded_model)\n\n        # Update consecutive failures\n        if not current_status.healthy:\n            failures = (previous_status.consecutive_failures or 0) + 1\n            current_status = HealthStatus(\n                healthy=current_status.healthy,\n                last_check=current_status.last_check,\n                error_message=current_status.error_message,\n                consecutive_failures=failures,\n                response_time_ms=current_status.response_time_ms\n            )\n\n        # Update model health status\n        loaded_model.health_status = current_status\n\n        # Check if restart needed\n        if current_status.requires_restart():\n            logger.error(f\"Model {loaded_model.model_id} requires restart\")\n            restart_model(loaded_model)\n\n        previous_status = current_status\n        time.sleep(check_interval_seconds)\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#93-state-transition-integration","title":"9.3 State Transition Integration","text":"<pre><code>def update_model_state_from_health(loaded_model):\n    \"\"\"Update model state based on health status\"\"\"\n    status = loaded_model.health_status\n\n    if loaded_model.state == ModelState.LOADED:\n        if not status.healthy and status.consecutive_failures &gt;= 3:\n            # Transition to UNHEALTHY\n            loaded_model.state = ModelState(state=ModelStateEnum.UNHEALTHY)\n            logger.warning(\n                f\"Model {loaded_model.model_id} transitioned to UNHEALTHY\"\n            )\n\n    elif loaded_model.state == ModelState.UNHEALTHY:\n        if status.healthy and status.consecutive_failures == 0:\n            # Recovery: transition back to LOADED\n            loaded_model.state = ModelState(state=ModelStateEnum.LOADED)\n            logger.info(\n                f\"Model {loaded_model.model_id} recovered to LOADED\"\n            )\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#94-health-dashboard","title":"9.4 Health Dashboard","text":"<pre><code>def get_health_summary(lifecycle_manager):\n    \"\"\"Get overall health summary of all loaded models\"\"\"\n    healthy_count = 0\n    unhealthy_count = 0\n    degraded_count = 0\n\n    for model in lifecycle_manager.loaded_models.values():\n        status = model.health_status\n\n        if status.healthy:\n            if status.is_degraded():\n                degraded_count += 1\n            else:\n                healthy_count += 1\n        else:\n            unhealthy_count += 1\n\n    return {\n        \"healthy\": healthy_count,\n        \"degraded\": degraded_count,\n        \"unhealthy\": unhealthy_count,\n        \"total\": len(lifecycle_manager.loaded_models)\n    }\n</code></pre>"},{"location":"data_models/lifecycle/health_status/#10-related-models","title":"10. Related Models","text":"<ul> <li>Loaded Model - Contains HealthStatus</li> <li>Model State - Influenced by HealthStatus (healthy \u2194 unhealthy)</li> <li>Model Lifecycle Manager - Monitors health and manages transitions</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/","title":"Loaded Model Entity","text":"<p>Context: Lifecycle Type: Entity Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/lifecycle/loaded_model/#1-overview","title":"1. Overview","text":"<p>The <code>LoadedModel</code> entity represents a model loaded into memory, tracking its runtime state, process, resource usage, and health.</p>"},{"location":"data_models/lifecycle/loaded_model/#2-structure","title":"2. Structure","text":""},{"location":"data_models/lifecycle/loaded_model/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>dbb0bcc37479f6baf99c4ef34040185276a942b997c32ab234efaab1d6d56329c6f5cfac3a82e75213cca6180dbaf7e7ec60b38f3caaa85efa013b7f5cbc352a</pre><pre>b68b9079dc6eecbd41c7d0c23c0a8849b0f9425fbb5574907037e373894833591dbb19c75d104145eb0cf0d55792ce0fd02db70d04583e328adcb23c979dbeaa</pre>"},{"location":"data_models/lifecycle/loaded_model/#22-relationships","title":"2.2 Relationships","text":"<pre>7367effd982003f3cd556cbf053294b0056f3d1c43903ace78229730ef8838fb7d0d9e51d226b7b406de2cb12474c71a942b6793d07c0bad087ac7bcdc075e11</pre><pre>f4fb087467b2e9c64d716a3bcf41780dc5aeddea5906001361144ec94d70a0edd1e4c131a8715b28fe97f77d514d667c0be5fd04ce404388775af43f18c9ff3e</pre>"},{"location":"data_models/lifecycle/loaded_model/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/lifecycle/loaded_model/#31-id","title":"3.1 id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique identifier</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/#32-model_id","title":"3.2 model_id","text":"<ul> <li>Type: ModelId</li> <li>Constraint: MUST</li> <li>Description: Reference to registered model</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/#33-version","title":"3.3 version","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Description: Loaded model version</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/#34-process","title":"3.4 process","text":"<ul> <li>Type: ModelProcess</li> <li>Constraint: MUST</li> <li>Description: OS process running the model</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/#35-state","title":"3.5 state","text":"<ul> <li>Type: ModelState</li> <li>Constraint: MUST</li> <li>Description: Current lifecycle state</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/#36-memory_usage","title":"3.6 memory_usage","text":"<ul> <li>Type: MemoryUsage</li> <li>Constraint: MUST</li> <li>Description: Current memory consumption</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/#37-health_status","title":"3.7 health_status","text":"<ul> <li>Type: HealthStatus</li> <li>Constraint: MUST</li> <li>Description: Health check results</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/#38-loaded_at","title":"3.8 loaded_at","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MUST</li> <li>Description: When model was loaded</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/#39-last_used_at","title":"3.9 last_used_at","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MUST</li> <li>Description: Last inference request time (LRU tracking)</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/#310-request_count","title":"3.10 request_count","text":"<ul> <li>Type: Integer</li> <li>Constraint: MUST</li> <li>Description: Total inference requests served</li> </ul>"},{"location":"data_models/lifecycle/loaded_model/#4-behavior","title":"4. Behavior","text":""},{"location":"data_models/lifecycle/loaded_model/#41-availability-check","title":"4.1 Availability Check","text":"<pre><code>def is_available(self) -&gt; bool:\n    \"\"\"Check if model can accept requests\"\"\"\n    return (\n        self.state.is_operational() and\n        self.health_status.healthy and\n        self.process.is_running()\n    )\n</code></pre>"},{"location":"data_models/lifecycle/loaded_model/#42-usage-tracking","title":"4.2 Usage Tracking","text":"<pre><code>def update_last_used(self) -&gt; None:\n    \"\"\"Update last used timestamp for LRU\"\"\"\n    self.last_used_at = current_timestamp()\n    self.updated_at = current_timestamp()\n    self.version_number += 1\n\ndef increment_request_count(self) -&gt; None:\n    \"\"\"Increment request counter\"\"\"\n    self.request_count += 1\n    self.updated_at = current_timestamp()\n    self.version_number += 1\n</code></pre>"},{"location":"data_models/lifecycle/loaded_model/#5-serialization","title":"5. Serialization","text":""},{"location":"data_models/lifecycle/loaded_model/#51-json-example","title":"5.1 JSON Example","text":"<pre><code>{\n  \"id\": \"bb0e8400-e29b-41d4-a716-446655440007\",\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"version\": \"v2.2.2\",\n  \"process\": {\n    \"id\": \"aa0e8400-e29b-41d4-a716-446655440006\",\n    \"process_id\": 12345\n  },\n  \"state\": {\n    \"state\": \"loaded\"\n  },\n  \"memory_usage\": {\n    \"used_mb\": 512.5,\n    \"peak_mb\": 550.0,\n    \"timestamp\": \"2025-12-04T10:30:15Z\"\n  },\n  \"health_status\": {\n    \"healthy\": true,\n    \"last_check\": \"2025-12-04T10:30:15Z\"\n  },\n  \"loaded_at\": \"2025-12-04T10:25:00Z\",\n  \"last_used_at\": \"2025-12-04T10:30:00Z\",\n  \"request_count\": 42,\n  \"created_at\": \"2025-12-04T10:25:00Z\",\n  \"updated_at\": \"2025-12-04T10:30:15Z\",\n  \"version\": 5\n}\n</code></pre>"},{"location":"data_models/lifecycle/loaded_model/#6-database-schema","title":"6. Database Schema","text":"<pre><code>CREATE TABLE loaded_model (\n    id UUID PRIMARY KEY,\n    model_id VARCHAR(255) NOT NULL,\n    version VARCHAR(100) NOT NULL,\n    process_id UUID NOT NULL REFERENCES model_process(id) ON DELETE CASCADE,\n    state model_state_enum NOT NULL DEFAULT 'unloaded',\n    memory_usage JSONB NOT NULL,\n    health_status JSONB NOT NULL,\n    loaded_at TIMESTAMP NOT NULL,\n    last_used_at TIMESTAMP NOT NULL,\n    request_count INTEGER NOT NULL DEFAULT 0,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1,\n    UNIQUE (model_id, version)\n);\n\nCREATE INDEX idx_loaded_model_model_id ON loaded_model(model_id);\nCREATE INDEX idx_loaded_model_state ON loaded_model(state);\nCREATE INDEX idx_loaded_model_last_used ON loaded_model(last_used_at);\n\nCREATE TRIGGER update_loaded_model_timestamp\nBEFORE UPDATE ON loaded_model\nFOR EACH ROW\nEXECUTE FUNCTION update_timestamp_and_version();\n</code></pre>"},{"location":"data_models/lifecycle/loaded_model/#7-usage-examples","title":"7. Usage Examples","text":""},{"location":"data_models/lifecycle/loaded_model/#71-loading-model","title":"7.1 Loading Model","text":"<pre><code>def load_model(model_id: ModelId, version: str) -&gt; LoadedModel:\n    \"\"\"Load model into memory\"\"\"\n    # Start process\n    process = start_model_process(model_id, {})\n\n    # Create loaded model\n    loaded_model = LoadedModel(\n        id=generate_uuid(),\n        model_id=model_id,\n        version=version,\n        process=process,\n        state=ModelState(state=ModelStateEnum.LOADING),\n        memory_usage=MemoryUsage(used_mb=0, peak_mb=0, timestamp=now()),\n        health_status=create_healthy_status(),\n        loaded_at=now(),\n        last_used_at=now(),\n        request_count=0,\n        created_at=now(),\n        updated_at=now(),\n        version_number=1\n    )\n\n    # Complete loading\n    loaded_model.state = ModelState(state=ModelStateEnum.LOADED)\n    return loaded_model\n</code></pre>"},{"location":"data_models/lifecycle/loaded_model/#72-lru-eviction","title":"7.2 LRU Eviction","text":"<pre><code>def get_lru_model(loaded_models: List[LoadedModel]) -&gt; LoadedModel:\n    \"\"\"Find least recently used model for eviction\"\"\"\n    return min(loaded_models, key=lambda m: m.last_used_at)\n</code></pre>"},{"location":"data_models/lifecycle/loaded_model/#8-related-models","title":"8. Related Models","text":"<ul> <li>Model Process - Process running the model</li> <li>Model State - Lifecycle state</li> <li>Memory Usage - Resource consumption</li> <li>Health Status - Health check results</li> <li>Model Lifecycle Manager - Manages loaded models</li> </ul>"},{"location":"data_models/lifecycle/memory_usage/","title":"Memory Usage Value Object","text":"<p>Context: Lifecycle Type: Value Object Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/lifecycle/memory_usage/#1-overview","title":"1. Overview","text":"<p>The <code>MemoryUsage</code> value object tracks memory consumption of a loaded model, including current usage, peak usage, and allocation metrics.</p>"},{"location":"data_models/lifecycle/memory_usage/#2-structure","title":"2. Structure","text":""},{"location":"data_models/lifecycle/memory_usage/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>864f4ee37bcd819f2a48897d1d2345a5e9a5251b42ccc9f174a3fb58e74618178170744535a51122f4812f7637ae5342c720280bad01830ba5b300aaf6f3010e</pre><pre>b0e4ceed0cac8a099f468df4c89b0609d64ca4f8e32dcc72cd1a8a72705383b69f8fe6802f7a7535c9486607e29cdd9b7fc29e098b2e4ef5da044b7ad19ee99b</pre>"},{"location":"data_models/lifecycle/memory_usage/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/lifecycle/memory_usage/#31-used_mb","title":"3.1 used_mb","text":"<ul> <li>Type: Float</li> <li>Constraint: MUST, Immutable</li> <li>Description: Current memory usage in megabytes</li> <li>Validation:</li> <li>MUST be &gt;= 0</li> <li>Includes model weights and runtime buffers</li> </ul>"},{"location":"data_models/lifecycle/memory_usage/#32-peak_mb","title":"3.2 peak_mb","text":"<ul> <li>Type: Float</li> <li>Constraint: MUST, Immutable</li> <li>Description: Peak memory usage since model loaded</li> <li>Validation:</li> <li>MUST be &gt;= used_mb</li> <li>Tracks maximum memory footprint</li> </ul>"},{"location":"data_models/lifecycle/memory_usage/#33-allocated_mb","title":"3.3 allocated_mb","text":"<ul> <li>Type: Float</li> <li>Constraint: MAY, Immutable</li> <li>Description: Memory allocated (vs. actually used)</li> <li>Validation:</li> <li>MUST be &gt;= used_mb if specified</li> <li>May be higher due to pre-allocation</li> </ul>"},{"location":"data_models/lifecycle/memory_usage/#34-gpu_vram_mb","title":"3.4 gpu_vram_mb","text":"<ul> <li>Type: Float</li> <li>Constraint: MAY, Immutable</li> <li>Description: GPU VRAM usage in megabytes</li> <li>Validation:</li> <li>MUST be &gt;= 0 if specified</li> <li>0 or null for CPU-only models</li> </ul>"},{"location":"data_models/lifecycle/memory_usage/#35-timestamp","title":"3.5 timestamp","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MUST, Immutable</li> <li>Description: When measurement was taken</li> <li>Validation:</li> <li>MUST be valid ISO 8601 timestamp</li> <li>Used for time-series analysis</li> </ul>"},{"location":"data_models/lifecycle/memory_usage/#4-constraints","title":"4. Constraints","text":""},{"location":"data_models/lifecycle/memory_usage/#41-value-object-constraints","title":"4.1 Value Object Constraints","text":"<ul> <li>All fields MUST be immutable after creation</li> <li>Equality based on structural comparison</li> <li>No identity field required</li> </ul>"},{"location":"data_models/lifecycle/memory_usage/#42-business-rules","title":"4.2 Business Rules","text":"<ul> <li><code>peak_mb</code> &gt;= <code>used_mb</code> (peak is always maximum)</li> <li><code>allocated_mb</code> &gt;= <code>used_mb</code> if specified</li> <li><code>gpu_vram_mb</code> should align with GPU model requirements</li> <li>New measurements should have increasing timestamps</li> </ul>"},{"location":"data_models/lifecycle/memory_usage/#5-validation","title":"5. Validation","text":""},{"location":"data_models/lifecycle/memory_usage/#51-syntax-validation","title":"5.1 Syntax Validation","text":"<pre><code>def validate_syntax(usage: MemoryUsage) -&gt; ValidationResult:\n    errors = []\n\n    if usage.used_mb is None:\n        errors.append(\"used_mb is required\")\n    elif usage.used_mb &lt; 0:\n        errors.append(\"used_mb must be &gt;= 0\")\n\n    if usage.peak_mb is None:\n        errors.append(\"peak_mb is required\")\n    elif usage.peak_mb &lt; 0:\n        errors.append(\"peak_mb must be &gt;= 0\")\n    elif usage.peak_mb &lt; usage.used_mb:\n        errors.append(\"peak_mb must be &gt;= used_mb\")\n\n    if usage.allocated_mb is not None:\n        if usage.allocated_mb &lt; 0:\n            errors.append(\"allocated_mb must be &gt;= 0\")\n        elif usage.allocated_mb &lt; usage.used_mb:\n            errors.append(\"allocated_mb must be &gt;= used_mb\")\n\n    if usage.gpu_vram_mb is not None and usage.gpu_vram_mb &lt; 0:\n        errors.append(\"gpu_vram_mb must be &gt;= 0\")\n\n    if not usage.timestamp:\n        errors.append(\"timestamp is required\")\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#6-behavior","title":"6. Behavior","text":""},{"location":"data_models/lifecycle/memory_usage/#61-creation","title":"6.1 Creation","text":"<pre><code>def create_memory_usage(\n    used_mb: float,\n    peak_mb: float,\n    allocated_mb: float = None,\n    gpu_vram_mb: float = None\n) -&gt; MemoryUsage:\n    \"\"\"Create memory usage snapshot\"\"\"\n    return MemoryUsage(\n        used_mb=used_mb,\n        peak_mb=max(peak_mb, used_mb),  # Ensure peak &gt;= used\n        allocated_mb=allocated_mb,\n        gpu_vram_mb=gpu_vram_mb,\n        timestamp=current_timestamp()\n    )\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#62-metric-calculations","title":"6.2 Metric Calculations","text":"<pre><code>def exceeds_threshold(self, threshold_mb: float) -&gt; bool:\n    \"\"\"Check if usage exceeds threshold\"\"\"\n    return self.used_mb &gt; threshold_mb\n\ndef utilization_percent(self, total_mb: float) -&gt; float:\n    \"\"\"Calculate memory utilization percentage\"\"\"\n    if total_mb &lt;= 0:\n        return 0.0\n    return (self.used_mb / total_mb) * 100.0\n\ndef fragmentation_percent(self) -&gt; float:\n    \"\"\"Calculate memory fragmentation (allocated vs. used)\"\"\"\n    if self.allocated_mb is None or self.allocated_mb == 0:\n        return 0.0\n    return ((self.allocated_mb - self.used_mb) / self.allocated_mb) * 100.0\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#63-value-object-methods","title":"6.3 Value Object Methods","text":"<pre><code>def equals(self, other: MemoryUsage) -&gt; bool:\n    \"\"\"Structural equality (comparing snapshots)\"\"\"\n    return (\n        abs(self.used_mb - other.used_mb) &lt; 0.01 and\n        abs(self.peak_mb - other.peak_mb) &lt; 0.01 and\n        self.timestamp == other.timestamp\n    )\n\ndef hash(self) -&gt; int:\n    \"\"\"Hash based on values\"\"\"\n    return hash((\n        round(self.used_mb, 2),\n        round(self.peak_mb, 2),\n        self.timestamp\n    ))\n\ndef toString(self) -&gt; str:\n    \"\"\"Human-readable representation\"\"\"\n    gpu_info = f\", GPU: {self.gpu_vram_mb:.1f}MB\" if self.gpu_vram_mb else \"\"\n    return f\"Memory: {self.used_mb:.1f}MB / {self.peak_mb:.1f}MB peak{gpu_info}\"\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#7-serialization","title":"7. Serialization","text":""},{"location":"data_models/lifecycle/memory_usage/#71-json-example","title":"7.1 JSON Example","text":"<pre><code>{\n  \"used_mb\": 2048.5,\n  \"peak_mb\": 2150.3,\n  \"allocated_mb\": 2200.0,\n  \"gpu_vram_mb\": 0.0,\n  \"timestamp\": \"2025-12-04T10:30:15Z\"\n}\n</code></pre> <pre><code>{\n  \"used_mb\": 15360.2,\n  \"peak_mb\": 15890.7,\n  \"allocated_mb\": 16384.0,\n  \"gpu_vram_mb\": 15360.2,\n  \"timestamp\": \"2025-12-04T10:30:15Z\"\n}\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#72-protocol-buffers","title":"7.2 Protocol Buffers","text":"<pre><code>message MemoryUsage {\n  float used_mb = 1;\n  float peak_mb = 2;\n  float allocated_mb = 3;\n  float gpu_vram_mb = 4;\n  google.protobuf.Timestamp timestamp = 5;\n}\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#8-database-schema","title":"8. Database Schema","text":"<pre><code>-- Stored as JSONB in loaded_model table or separate metrics table\nCREATE TABLE memory_usage_history (\n    loaded_model_id UUID NOT NULL REFERENCES loaded_model(id) ON DELETE CASCADE,\n    used_mb REAL NOT NULL CHECK (used_mb &gt;= 0),\n    peak_mb REAL NOT NULL CHECK (peak_mb &gt;= used_mb),\n    allocated_mb REAL CHECK (allocated_mb &gt;= used_mb),\n    gpu_vram_mb REAL CHECK (gpu_vram_mb &gt;= 0),\n    timestamp TIMESTAMP NOT NULL DEFAULT NOW(),\n    PRIMARY KEY (loaded_model_id, timestamp)\n);\n\nCREATE INDEX idx_memory_usage_timestamp ON memory_usage_history(timestamp);\nCREATE INDEX idx_memory_usage_model ON memory_usage_history(loaded_model_id);\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#9-usage-examples","title":"9. Usage Examples","text":""},{"location":"data_models/lifecycle/memory_usage/#91-measuring-memory","title":"9.1 Measuring Memory","text":"<pre><code>import psutil\n\ndef measure_model_memory(model_process_id: int) -&gt; MemoryUsage:\n    \"\"\"Measure current memory usage of model process\"\"\"\n    process = psutil.Process(model_process_id)\n    mem_info = process.memory_info()\n\n    used_mb = mem_info.rss / (1024 * 1024)  # Convert to MB\n\n    # Track peak from previous measurements\n    peak_mb = max(used_mb, get_previous_peak(model_process_id))\n\n    return MemoryUsage(\n        used_mb=used_mb,\n        peak_mb=peak_mb,\n        timestamp=current_timestamp()\n    )\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#92-eviction-decision","title":"9.2 Eviction Decision","text":"<pre><code>def should_evict_model(\n    usage: MemoryUsage,\n    system_capacity_mb: float,\n    eviction_threshold: float = 0.85\n) -&gt; bool:\n    \"\"\"Determine if model should be evicted based on memory usage\"\"\"\n    utilization = usage.utilization_percent(system_capacity_mb)\n    return utilization &gt; (eviction_threshold * 100)\n\n# Example\nusage = MemoryUsage(used_mb=7200, peak_mb=7500, timestamp=now())\nif should_evict_model(usage, system_capacity_mb=8192):\n    evict_lru_model()\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#93-time-series-monitoring","title":"9.3 Time-Series Monitoring","text":"<pre><code>def track_memory_over_time(model_id: str, duration_seconds: int):\n    \"\"\"Track memory usage over time for analysis\"\"\"\n    measurements = []\n\n    for _ in range(duration_seconds):\n        usage = measure_model_memory(model_id)\n        measurements.append(usage)\n        time.sleep(1)\n\n    # Analyze trend\n    avg_usage = sum(m.used_mb for m in measurements) / len(measurements)\n    max_peak = max(m.peak_mb for m in measurements)\n\n    return {\n        \"average_mb\": avg_usage,\n        \"max_peak_mb\": max_peak,\n        \"measurements\": measurements\n    }\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#94-alerting","title":"9.4 Alerting","text":"<pre><code>def check_memory_alerts(usage: MemoryUsage, requirements: ResourceRequirements):\n    \"\"\"Check for memory-related alerts\"\"\"\n    warnings = []\n\n    # Usage exceeds requirements\n    if usage.used_mb &gt; requirements.memory_mb * 1.2:\n        warnings.append(\n            f\"Memory usage ({usage.used_mb:.1f}MB) exceeds \"\n            f\"requirements ({requirements.memory_mb}MB) by 20%\"\n        )\n\n    # High fragmentation\n    fragmentation = usage.fragmentation_percent()\n    if fragmentation &gt; 25:\n        warnings.append(f\"High memory fragmentation: {fragmentation:.1f}%\")\n\n    return warnings\n</code></pre>"},{"location":"data_models/lifecycle/memory_usage/#10-related-models","title":"10. Related Models","text":"<ul> <li>Loaded Model - Contains MemoryUsage measurements</li> <li>Resource Requirements - Expected memory usage</li> <li>Model Lifecycle Manager - Monitors memory for eviction</li> </ul>"},{"location":"data_models/lifecycle/model_lifecycle_manager/","title":"Model Lifecycle Manager Aggregate","text":"<p>Context: Lifecycle Type: Aggregate Root Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#1-overview","title":"1. Overview","text":"<p>The <code>ModelLifecycleManager</code> aggregate coordinates model loading, unloading, health monitoring, and resource management. It enforces lifecycle invariants and manages the collection of loaded models.</p>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#2-structure","title":"2. Structure","text":""},{"location":"data_models/lifecycle/model_lifecycle_manager/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>f81c8b2d5eaec1d99e9b4bde1c87dc5d42d203aa522c83b65ecde75d1103eb1f2eaa154c1ba713e5dba11611bb5d3dca7c53015fb360e993718bd21e19907be0</pre><pre>91494a4b0f771b7f3a153e55fbe5794375b62b2a2dde5dc0329c05aeb694428f047e389d36d281342d8c5fb5c05be955c66dd158349e58bafae48a24bf51b42b</pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/lifecycle/model_lifecycle_manager/#31-id","title":"3.1 id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique identifier</li> </ul>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#32-loaded_models","title":"3.2 loaded_models","text":"<ul> <li>Type: Map <li>Constraint: MUST</li> <li>Description: Currently loaded models indexed by ModelId</li>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#33-max_memory_mb","title":"3.3 max_memory_mb","text":"<ul> <li>Type: Float</li> <li>Constraint: MUST</li> <li>Description: Maximum memory threshold for loaded models</li> </ul>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#34-max_models","title":"3.4 max_models","text":"<ul> <li>Type: Integer</li> <li>Constraint: MUST</li> <li>Description: Maximum number of models that can be loaded</li> </ul>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#35-events","title":"3.5 events","text":"<ul> <li>Type: Collection\\ <li>Description: Domain events generated</li>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#4-behavior","title":"4. Behavior","text":""},{"location":"data_models/lifecycle/model_lifecycle_manager/#41-model-loading","title":"4.1 Model Loading","text":"<pre><code>def load_model(self, model_id: ModelId, version: str) -&gt; LoadedModel:\n    \"\"\"Load a model into memory\"\"\"\n    # Check if already loaded\n    if model_id in self.loaded_models:\n        return self.loaded_models[model_id]\n\n    # Get model requirements\n    model_version = registry.get_version(model_id, version)\n    requirements = model_version.resource_requirements\n\n    # Check if we can load\n    if not self.can_load_model(requirements):\n        # Evict LRU model\n        self.evict_lru_model()\n\n    # Load model\n    loaded_model = create_loaded_model(model_id, version)\n    self.loaded_models[model_id] = loaded_model\n\n    self.updated_at = current_timestamp()\n    self.version += 1\n\n    # Generate event\n    self.addEvent(ModelLoadedEvent(\n        manager_id=self.id,\n        model_id=model_id,\n        timestamp=current_timestamp()\n    ))\n\n    return loaded_model\n</code></pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#42-model-unloading","title":"4.2 Model Unloading","text":"<pre><code>def unload_model(self, model_id: ModelId) -&gt; None:\n    \"\"\"Unload a model from memory\"\"\"\n    if model_id not in self.loaded_models:\n        raise ValueError(f\"Model {model_id} not loaded\")\n\n    loaded_model = self.loaded_models[model_id]\n\n    # Transition to unloading\n    loaded_model.state = ModelState(state=ModelStateEnum.UNLOADING)\n\n    # Terminate process\n    loaded_model.process.terminate()\n\n    # Remove from loaded models\n    del self.loaded_models[model_id]\n\n    self.updated_at = current_timestamp()\n    self.version += 1\n\n    # Generate event\n    self.addEvent(ModelUnloadedEvent(\n        manager_id=self.id,\n        model_id=model_id,\n        timestamp=current_timestamp()\n    ))\n</code></pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#43-resource-management","title":"4.3 Resource Management","text":"<pre><code>def get_total_memory_usage(self) -&gt; float:\n    \"\"\"Calculate total memory used by all loaded models\"\"\"\n    return sum(m.memory_usage.used_mb for m in self.loaded_models.values())\n\ndef can_load_model(self, requirements: ResourceRequirements) -&gt; bool:\n    \"\"\"Check if model can be loaded within resource limits\"\"\"\n    # Check model count\n    if len(self.loaded_models) &gt;= self.max_models:\n        return False\n\n    # Check memory\n    current_memory = self.get_total_memory_usage()\n    if current_memory + requirements.memory_mb &gt; self.max_memory_mb:\n        return False\n\n    return True\n\ndef evict_lru_model(self) -&gt; None:\n    \"\"\"Evict least recently used model\"\"\"\n    if not self.loaded_models:\n        return\n\n    lru_model = min(\n        self.loaded_models.values(),\n        key=lambda m: m.last_used_at\n    )\n\n    self.unload_model(lru_model.model_id)\n</code></pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#44-health-monitoring","title":"4.4 Health Monitoring","text":"<pre><code>def check_all_health(self) -&gt; None:\n    \"\"\"Check health of all loaded models\"\"\"\n    for loaded_model in self.loaded_models.values():\n        # Perform health check\n        status = perform_health_check(loaded_model)\n        loaded_model.health_status = status\n\n        # Update state if needed\n        if not status.healthy and status.consecutive_failures &gt;= 3:\n            loaded_model.state = ModelState(state=ModelStateEnum.UNHEALTHY)\n</code></pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#5-serialization","title":"5. Serialization","text":""},{"location":"data_models/lifecycle/model_lifecycle_manager/#51-json-example","title":"5.1 JSON Example","text":"<pre><code>{\n  \"id\": \"cc0e8400-e29b-41d4-a716-446655440008\",\n  \"max_memory_mb\": 16384.0,\n  \"max_models\": 10,\n  \"loaded_models\": {\n    \"sentence-transformers/all-MiniLM-L6-v2\": {\n      \"id\": \"bb0e8400-e29b-41d4-a716-446655440007\",\n      \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n      \"state\": {\n        \"state\": \"loaded\"\n      }\n    }\n  },\n  \"created_at\": \"2025-12-04T10:00:00Z\",\n  \"updated_at\": \"2025-12-04T10:30:15Z\",\n  \"version\": 5\n}\n</code></pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#6-database-schema","title":"6. Database Schema","text":"<pre><code>CREATE TABLE model_lifecycle_manager (\n    id UUID PRIMARY KEY,\n    max_memory_mb REAL NOT NULL CHECK (max_memory_mb &gt; 0),\n    max_models INTEGER NOT NULL CHECK (max_models &gt; 0),\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1\n);\n\n-- loaded_model table references manager\nALTER TABLE loaded_model ADD COLUMN manager_id UUID REFERENCES model_lifecycle_manager(id);\n\nCREATE TRIGGER update_lifecycle_manager_timestamp\nBEFORE UPDATE ON model_lifecycle_manager\nFOR EACH ROW\nEXECUTE FUNCTION update_timestamp_and_version();\n</code></pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#7-domain-events","title":"7. Domain Events","text":""},{"location":"data_models/lifecycle/model_lifecycle_manager/#71-modelloadedevent","title":"7.1 ModelLoadedEvent","text":"<pre><code>class ModelLoadedEvent:\n    manager_id: UUID\n    model_id: ModelId\n    version: String\n    timestamp: Timestamp\n</code></pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#72-modelunloadedevent","title":"7.2 ModelUnloadedEvent","text":"<pre><code>class ModelUnloadedEvent:\n    manager_id: UUID\n    model_id: ModelId\n    timestamp: Timestamp\n</code></pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#8-usage-examples","title":"8. Usage Examples","text":""},{"location":"data_models/lifecycle/model_lifecycle_manager/#81-initialization","title":"8.1 Initialization","text":"<pre><code>manager = ModelLifecycleManager(\n    id=generate_uuid(),\n    loaded_models={},\n    max_memory_mb=16384.0,\n    max_models=10,\n    events=[],\n    created_at=now(),\n    updated_at=now(),\n    version=1\n)\n</code></pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#82-loading-with-auto-eviction","title":"8.2 Loading with Auto-Eviction","text":"<pre><code># Load model (auto-evicts if needed)\nloaded_model = manager.load_model(\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    version=\"v2.2.2\"\n)\n\n# Check availability\nif loaded_model.is_available():\n    result = inference_service.infer(loaded_model, request)\n</code></pre>"},{"location":"data_models/lifecycle/model_lifecycle_manager/#9-related-models","title":"9. Related Models","text":"<ul> <li>Loaded Model - Managed by aggregate</li> <li>Model Process - Created during load</li> <li>Model State - Tracked per model</li> <li>Memory Usage - Monitored for eviction</li> <li>Health Status - Checked periodically</li> </ul>"},{"location":"data_models/lifecycle/model_process/","title":"Model Process Entity","text":"<p>Context: Lifecycle Type: Entity Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/lifecycle/model_process/#1-overview","title":"1. Overview","text":"<p>The <code>ModelProcess</code> entity represents the operating system process running a loaded model, tracking process-level details for management and monitoring.</p>"},{"location":"data_models/lifecycle/model_process/#2-structure","title":"2. Structure","text":""},{"location":"data_models/lifecycle/model_process/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>89acd189ccbf5236cb1f843cfdf8bbfe121486f155f187446bcd3f61cf2295d390ca03cd380a398094db8b10e1d5651b0df46dec21df791597862f6ce0c5bade</pre><pre>7545015f4ff7f6a66b9bd7ce5de9b4f496a39e7b06217935ddd029de1062fc0251090aed143bc005891af84c6af681ce462128cbecfa05ec53aa5f870435c5e5</pre>"},{"location":"data_models/lifecycle/model_process/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/lifecycle/model_process/#31-id","title":"3.1 id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique identifier for process entity</li> </ul>"},{"location":"data_models/lifecycle/model_process/#32-process_id","title":"3.2 process_id","text":"<ul> <li>Type: Integer</li> <li>Constraint: MUST</li> <li>Description: Operating system process ID (PID)</li> <li>Validation: MUST be &gt; 0</li> </ul>"},{"location":"data_models/lifecycle/model_process/#33-model_id","title":"3.3 model_id","text":"<ul> <li>Type: ModelId</li> <li>Constraint: MUST</li> <li>Description: Model running in this process</li> </ul>"},{"location":"data_models/lifecycle/model_process/#34-started_at","title":"3.4 started_at","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MUST</li> <li>Description: When process was started</li> </ul>"},{"location":"data_models/lifecycle/model_process/#35-command_line","title":"3.5 command_line","text":"<ul> <li>Type: String</li> <li>Constraint: MAY</li> <li>Description: Command used to start process</li> </ul>"},{"location":"data_models/lifecycle/model_process/#36-working_directory","title":"3.6 working_directory","text":"<ul> <li>Type: String</li> <li>Constraint: MAY</li> <li>Description: Process working directory</li> </ul>"},{"location":"data_models/lifecycle/model_process/#37-environment_vars","title":"3.7 environment_vars","text":"<ul> <li>Type: Map <li>Constraint: MAY</li> <li>Description: Environment variables for process</li>"},{"location":"data_models/lifecycle/model_process/#38-entity-pattern-fields","title":"3.8 Entity Pattern Fields","text":"<ul> <li>created_at: When entity was created</li> <li>updated_at: Last modification time</li> <li>version: Optimistic locking version</li> </ul>"},{"location":"data_models/lifecycle/model_process/#4-constraints","title":"4. Constraints","text":""},{"location":"data_models/lifecycle/model_process/#41-entity-pattern-constraints","title":"4.1 Entity Pattern Constraints","text":"<ul> <li><code>id</code> MUST be unique</li> <li><code>created_at</code> immutable</li> <li><code>updated_at</code> auto-update</li> <li><code>version</code> auto-increment</li> </ul>"},{"location":"data_models/lifecycle/model_process/#42-business-rules","title":"4.2 Business Rules","text":"<ul> <li><code>process_id</code> MUST correspond to actual OS process</li> <li>Cannot have multiple ModelProcess entities with same <code>process_id</code></li> <li>Process MUST be running when entity created</li> <li><code>started_at</code> &lt;= <code>created_at</code></li> </ul>"},{"location":"data_models/lifecycle/model_process/#5-behavior","title":"5. Behavior","text":""},{"location":"data_models/lifecycle/model_process/#51-process-management","title":"5.1 Process Management","text":"<pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Check if process is still running\"\"\"\n    try:\n        process = psutil.Process(self.process_id)\n        return process.is_running()\n    except psutil.NoSuchProcess:\n        return False\n\ndef get_uptime_seconds(self) -&gt; float:\n    \"\"\"Get process uptime in seconds\"\"\"\n    return (current_timestamp() - self.started_at).total_seconds()\n\ndef terminate(self) -&gt; None:\n    \"\"\"Gracefully terminate process\"\"\"\n    try:\n        process = psutil.Process(self.process_id)\n        process.terminate()\n    except psutil.NoSuchProcess:\n        pass\n\ndef kill(self) -&gt; None:\n    \"\"\"Force kill process\"\"\"\n    try:\n        process = psutil.Process(self.process_id)\n        process.kill()\n    except psutil.NoSuchProcess:\n        pass\n</code></pre>"},{"location":"data_models/lifecycle/model_process/#6-serialization","title":"6. Serialization","text":""},{"location":"data_models/lifecycle/model_process/#61-json-example","title":"6.1 JSON Example","text":"<pre><code>{\n  \"id\": \"aa0e8400-e29b-41d4-a716-446655440006\",\n  \"process_id\": 12345,\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"started_at\": \"2025-12-04T10:25:00Z\",\n  \"command_line\": \"python model_worker.py --model sentence-transformers/all-MiniLM-L6-v2\",\n  \"working_directory\": \"/opt/modelmora\",\n  \"created_at\": \"2025-12-04T10:25:00Z\",\n  \"updated_at\": \"2025-12-04T10:25:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/lifecycle/model_process/#7-database-schema","title":"7. Database Schema","text":"<pre><code>CREATE TABLE model_process (\n    id UUID PRIMARY KEY,\n    process_id INTEGER NOT NULL UNIQUE,\n    model_id VARCHAR(255) NOT NULL,\n    started_at TIMESTAMP NOT NULL,\n    command_line TEXT,\n    working_directory VARCHAR(500),\n    environment_vars JSONB,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1\n);\n\nCREATE INDEX idx_model_process_pid ON model_process(process_id);\nCREATE INDEX idx_model_process_model_id ON model_process(model_id);\n\nCREATE TRIGGER update_model_process_timestamp\nBEFORE UPDATE ON model_process\nFOR EACH ROW\nEXECUTE FUNCTION update_timestamp_and_version();\n</code></pre>"},{"location":"data_models/lifecycle/model_process/#8-usage-examples","title":"8. Usage Examples","text":""},{"location":"data_models/lifecycle/model_process/#81-starting-model-process","title":"8.1 Starting Model Process","text":"<pre><code>def start_model_process(model_id: ModelId, config: dict) -&gt; ModelProcess:\n    \"\"\"Start a new model worker process\"\"\"\n    cmd = [\n        \"python\", \"model_worker.py\",\n        \"--model\", str(model_id),\n        \"--port\", str(config[\"port\"])\n    ]\n\n    proc = subprocess.Popen(\n        cmd,\n        cwd=\"/opt/modelmora\",\n        env=config.get(\"environment\", {})\n    )\n\n    return ModelProcess(\n        id=generate_uuid(),\n        process_id=proc.pid,\n        model_id=model_id,\n        started_at=current_timestamp(),\n        command_line=\" \".join(cmd),\n        working_directory=\"/opt/modelmora\",\n        environment_vars=config.get(\"environment\", {}),\n        created_at=current_timestamp(),\n        updated_at=current_timestamp(),\n        version=1\n    )\n</code></pre>"},{"location":"data_models/lifecycle/model_process/#9-related-models","title":"9. Related Models","text":"<ul> <li>Loaded Model - Contains ModelProcess</li> <li>Model Lifecycle Manager - Manages processes</li> </ul>"},{"location":"data_models/lifecycle/model_state/","title":"Model State Value Object","text":"<p>Context: Lifecycle Type: Value Object Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/lifecycle/model_state/#1-overview","title":"1. Overview","text":"<p>The <code>ModelState</code> value object represents the current state of a loaded model in its lifecycle. It defines valid states and transitions for model management.</p>"},{"location":"data_models/lifecycle/model_state/#2-structure","title":"2. Structure","text":""},{"location":"data_models/lifecycle/model_state/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>2002d3bf407d5872b6665b0efc631feac02f9d3112790f0d1c6ad43f967dcd27a39b0dc0ad1fe4780c630f339db56ab01eab02d8641414f0c52e3768ef81690b</pre><pre>743ebe0076aae33266d4041ebe981b33005da0d513e0c4364d3c1aa153d9c8f226db37251803a1d1e6c018e9e2029c6fcaa53a719675cdb488f31dcbbd0b6bc4</pre>"},{"location":"data_models/lifecycle/model_state/#22-state-transition-diagram","title":"2.2 State Transition Diagram","text":"<pre>b6067cd0b5d825a0aec171edcfe311861ecb3f56abc68abde850d9dd1dede38e3d306d5c0c0e6752535532967e6839475d4a1babee16a5323d7aa8afb97942de</pre><pre>30b6c3eaf41982f8635ad823876cc38891cff6805a2a93b992c0853593f382baf87ac5f5e6dbcf1ca70d7aa87c94a8ebd3ce74a623638d77708398f3b1c14e74</pre>"},{"location":"data_models/lifecycle/model_state/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/lifecycle/model_state/#31-state","title":"3.1 state","text":"<ul> <li>Type: ModelStateEnum</li> <li>Constraint: MUST, Immutable</li> <li>Description: The current lifecycle state</li> <li>Validation:</li> <li>MUST be one of the defined enum values</li> </ul>"},{"location":"data_models/lifecycle/model_state/#4-state-enumeration","title":"4. State Enumeration","text":""},{"location":"data_models/lifecycle/model_state/#41-state-definitions","title":"4.1 State Definitions","text":"State Description Entry Condition Exit Actions <code>UNLOADED</code> Model not in memory Initial state, after unload None <code>LOADING</code> Model being loaded load() called Allocate resources <code>LOADED</code> Model ready for inference Load successful Accept requests <code>UNHEALTHY</code> Model loaded but failing Health check failed Reject new requests <code>UNLOADING</code> Model being removed unload() called Release resources <code>FAILED</code> Load operation failed Load error Log error, cleanup"},{"location":"data_models/lifecycle/model_state/#42-state-behaviors","title":"4.2 State Behaviors","text":""},{"location":"data_models/lifecycle/model_state/#unloaded","title":"UNLOADED","text":"<ul> <li>Model artifacts not in memory</li> <li>No resources allocated</li> <li>Can transition to: LOADING</li> <li>Cannot accept inference requests</li> </ul>"},{"location":"data_models/lifecycle/model_state/#loading","title":"LOADING","text":"<ul> <li>Model artifacts being loaded into memory</li> <li>Resources being allocated</li> <li>Can transition to: LOADED, FAILED</li> <li>Cannot accept inference requests</li> </ul>"},{"location":"data_models/lifecycle/model_state/#loaded","title":"LOADED","text":"<ul> <li>Model fully loaded and operational</li> <li>All resources allocated</li> <li>Can transition to: UNHEALTHY, UNLOADING</li> <li>Can accept inference requests</li> </ul>"},{"location":"data_models/lifecycle/model_state/#unhealthy","title":"UNHEALTHY","text":"<ul> <li>Model loaded but failing health checks</li> <li>Resources still allocated</li> <li>Can transition to: UNLOADING, LOADED (recovery)</li> <li>Should reject new inference requests</li> <li>Existing requests may complete</li> </ul>"},{"location":"data_models/lifecycle/model_state/#unloading","title":"UNLOADING","text":"<ul> <li>Model being removed from memory</li> <li>Resources being released</li> <li>Can transition to: UNLOADED</li> <li>Cannot accept inference requests</li> </ul>"},{"location":"data_models/lifecycle/model_state/#failed","title":"FAILED","text":"<ul> <li>Load operation encountered error</li> <li>Partial resources may be allocated</li> <li>Can transition to: UNLOADED (cleanup)</li> <li>Cannot accept inference requests</li> </ul>"},{"location":"data_models/lifecycle/model_state/#5-constraints","title":"5. Constraints","text":""},{"location":"data_models/lifecycle/model_state/#51-value-object-constraints","title":"5.1 Value Object Constraints","text":"<ul> <li><code>state</code> MUST be immutable after creation</li> <li>Equality based on <code>state</code> value (structural)</li> <li>No identity field required</li> </ul>"},{"location":"data_models/lifecycle/model_state/#52-state-transition-rules","title":"5.2 State Transition Rules","text":"<p>Valid transitions:</p> <ul> <li>UNLOADED \u2192 LOADING</li> <li>LOADING \u2192 LOADED</li> <li>LOADING \u2192 FAILED</li> <li>LOADED \u2192 UNHEALTHY</li> <li>LOADED \u2192 UNLOADING</li> <li>UNHEALTHY \u2192 UNLOADING</li> <li>UNHEALTHY \u2192 LOADED (recovery)</li> <li>UNLOADING \u2192 UNLOADED</li> <li>FAILED \u2192 UNLOADED</li> </ul> <p>Invalid transitions (must follow valid path):</p> <ul> <li>LOADING \u2192 UNLOADED (must complete or fail)</li> <li>UNLOADED \u2192 LOADED (must go through LOADING)</li> <li>FAILED \u2192 LOADED (must unload first)</li> </ul>"},{"location":"data_models/lifecycle/model_state/#6-validation","title":"6. Validation","text":""},{"location":"data_models/lifecycle/model_state/#61-syntax-validation","title":"6.1 Syntax Validation","text":"<pre><code>def validate_syntax(state: ModelState) -&gt; ValidationResult:\n    errors = []\n\n    if not state.state:\n        errors.append(\"state is required\")\n\n    valid_states = [\n        \"unloaded\", \"loading\", \"loaded\",\n        \"unhealthy\", \"unloading\", \"failed\"\n    ]\n\n    if state.state not in valid_states:\n        errors.append(f\"state must be one of: {', '.join(valid_states)}\")\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#62-transition-validation","title":"6.2 Transition Validation","text":"<pre><code>def can_transition_to(self, target: ModelState) -&gt; bool:\n    \"\"\"Check if transition to target state is valid\"\"\"\n    valid_transitions = {\n        ModelStateEnum.UNLOADED: [ModelStateEnum.LOADING],\n        ModelStateEnum.LOADING: [ModelStateEnum.LOADED, ModelStateEnum.FAILED],\n        ModelStateEnum.LOADED: [ModelStateEnum.UNHEALTHY, ModelStateEnum.UNLOADING],\n        ModelStateEnum.UNHEALTHY: [ModelStateEnum.UNLOADING, ModelStateEnum.LOADED],\n        ModelStateEnum.UNLOADING: [ModelStateEnum.UNLOADED],\n        ModelStateEnum.FAILED: [ModelStateEnum.UNLOADED],\n    }\n\n    return target.state in valid_transitions.get(self.state, [])\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#7-behavior","title":"7. Behavior","text":""},{"location":"data_models/lifecycle/model_state/#71-creation","title":"7.1 Creation","text":"<pre><code>def create_model_state(state_str: str) -&gt; ModelState:\n    \"\"\"Create ModelState from string\"\"\"\n    return ModelState(state=ModelStateEnum[state_str.upper()])\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#72-state-queries","title":"7.2 State Queries","text":"<pre><code>def is_operational(self) -&gt; bool:\n    \"\"\"Check if model can handle requests\"\"\"\n    return self.state == ModelStateEnum.LOADED\n\ndef is_transitioning(self) -&gt; bool:\n    \"\"\"Check if model is in transition state\"\"\"\n    return self.state in [ModelStateEnum.LOADING, ModelStateEnum.UNLOADING]\n\ndef is_terminal(self) -&gt; bool:\n    \"\"\"Check if state is terminal (requires intervention)\"\"\"\n    return self.state in [ModelStateEnum.FAILED, ModelStateEnum.UNHEALTHY]\n\ndef requires_resources(self) -&gt; bool:\n    \"\"\"Check if state requires resources allocated\"\"\"\n    return self.state in [\n        ModelStateEnum.LOADING,\n        ModelStateEnum.LOADED,\n        ModelStateEnum.UNHEALTHY,\n        ModelStateEnum.UNLOADING\n    ]\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#73-value-object-methods","title":"7.3 Value Object Methods","text":"<pre><code>def equals(self, other: ModelState) -&gt; bool:\n    \"\"\"Structural equality\"\"\"\n    return self.state == other.state\n\ndef hash(self) -&gt; int:\n    \"\"\"Hash based on value\"\"\"\n    return hash(self.state)\n\ndef toString(self) -&gt; str:\n    \"\"\"String representation\"\"\"\n    return self.state.lower()\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#8-serialization","title":"8. Serialization","text":""},{"location":"data_models/lifecycle/model_state/#81-json-example","title":"8.1 JSON Example","text":"<pre><code>{\n  \"state\": \"loaded\"\n}\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#82-protocol-buffers","title":"8.2 Protocol Buffers","text":"<pre><code>enum ModelState {\n  MODEL_STATE_UNSPECIFIED = 0;\n  MODEL_STATE_UNLOADED = 1;\n  MODEL_STATE_LOADING = 2;\n  MODEL_STATE_LOADED = 3;\n  MODEL_STATE_UNHEALTHY = 4;\n  MODEL_STATE_UNLOADING = 5;\n  MODEL_STATE_FAILED = 6;\n}\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#9-database-schema","title":"9. Database Schema","text":"<pre><code>CREATE TYPE model_state_enum AS ENUM (\n    'unloaded',\n    'loading',\n    'loaded',\n    'unhealthy',\n    'unloading',\n    'failed'\n);\n\nCREATE TABLE loaded_model (\n    ...\n    state model_state_enum NOT NULL DEFAULT 'unloaded',\n    ...\n);\n\nCREATE INDEX idx_loaded_model_state ON loaded_model(state);\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#10-usage-examples","title":"10. Usage Examples","text":""},{"location":"data_models/lifecycle/model_state/#101-state-management","title":"10.1 State Management","text":"<pre><code># Create initial state\nstate = ModelState(state=ModelStateEnum.UNLOADED)\n\n# Check if operational\nif state.is_operational():\n    # Process request\n    pass\n\n# Validate transition\nnew_state = ModelState(state=ModelStateEnum.LOADING)\nif state.can_transition_to(new_state):\n    # Perform state transition\n    model.state = new_state\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#102-state-machine","title":"10.2 State Machine","text":"<pre><code>class LoadedModelStateMachine:\n    def __init__(self):\n        self.state = ModelState(state=ModelStateEnum.UNLOADED)\n\n    def load(self):\n        if not self.state.can_transition_to(\n            ModelState(state=ModelStateEnum.LOADING)\n        ):\n            raise InvalidStateTransition(\n                f\"Cannot load from state {self.state}\"\n            )\n\n        self.state = ModelState(state=ModelStateEnum.LOADING)\n        # Perform loading...\n        self.state = ModelState(state=ModelStateEnum.LOADED)\n\n    def unload(self):\n        if not self.state.can_transition_to(\n            ModelState(state=ModelStateEnum.UNLOADING)\n        ):\n            raise InvalidStateTransition(\n                f\"Cannot unload from state {self.state}\"\n            )\n\n        self.state = ModelState(state=ModelStateEnum.UNLOADING)\n        # Perform unloading...\n        self.state = ModelState(state=ModelStateEnum.UNLOADED)\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#103-state-monitoring","title":"10.3 State Monitoring","text":"<pre><code>def monitor_model_states(lifecycle_manager):\n    \"\"\"Monitor and log model states\"\"\"\n    state_counts = {}\n\n    for model in lifecycle_manager.loaded_models.values():\n        state_str = model.state.toString()\n        state_counts[state_str] = state_counts.get(state_str, 0) + 1\n\n    logger.info(f\"Model states: {state_counts}\")\n\n    # Alert on unhealthy models\n    if state_counts.get(\"unhealthy\", 0) &gt; 0:\n        alert(\"Models in unhealthy state detected\")\n</code></pre>"},{"location":"data_models/lifecycle/model_state/#11-related-models","title":"11. Related Models","text":"<ul> <li>Loaded Model - Uses ModelState to track lifecycle</li> <li>Model Lifecycle Manager - Manages state transitions</li> <li>Health Status - Influences state (healthy \u2192 unhealthy)</li> </ul>"},{"location":"data_models/observability/log_entry/","title":"Log Entry Entity","text":"<p>Context: Observability Type: Entity Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/observability/log_entry/#1-overview","title":"1. Overview","text":"<p>The <code>LogEntry</code> entity represents a structured log message with context and severity level.</p>"},{"location":"data_models/observability/log_entry/#2-structure","title":"2. Structure","text":"<pre>f34108c90fcb5993fed84c62c5218cdf58112bad5802f9de4fc1079076e4124e816d633233e3f47cb19f570d15dac20482413d1d561adf169e59a854ce1761c0</pre><pre>c97b6ecca55ed474bac213429e673197a4b390a4121794ca71226cc22b7c9be8ec1e72aa74d029737254003c42302f21102b122fbe54b1136aef8b25ad37dd5d</pre>"},{"location":"data_models/observability/log_entry/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/observability/log_entry/#31-level","title":"3.1 level","text":"<ul> <li>Type: LogLevel (Enum)</li> <li>Constraint: MUST</li> <li>Values: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code></li> </ul>"},{"location":"data_models/observability/log_entry/#32-message","title":"3.2 message","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Description: Log message text</li> </ul>"},{"location":"data_models/observability/log_entry/#33-context","title":"3.3 context","text":"<ul> <li>Type: Map <li>Constraint: MAY</li> <li>Description: Additional context (model_id, job_id, etc.)</li>"},{"location":"data_models/observability/log_entry/#34-source","title":"3.4 source","text":"<ul> <li>Type: String</li> <li>Constraint: MAY</li> <li>Description: Log source component</li> <li>Examples: <code>lifecycle_manager</code>, <code>inference_queue</code>, <code>model_worker</code></li> </ul>"},{"location":"data_models/observability/log_entry/#35-timestamp","title":"3.5 timestamp","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MUST</li> <li>Description: When log was generated</li> </ul>"},{"location":"data_models/observability/log_entry/#4-serialization","title":"4. Serialization","text":"<pre><code>{\n  \"id\": \"33000000-e29b-41d4-a716-446655440015\",\n  \"level\": \"error\",\n  \"message\": \"Model inference timeout\",\n  \"context\": {\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"job_id\": \"ee0e8400-e29b-41d4-a716-446655440010\",\n    \"timeout_seconds\": 60\n  },\n  \"source\": \"model_worker\",\n  \"timestamp\": \"2025-12-04T10:30:15.123Z\",\n  \"created_at\": \"2025-12-04T10:30:15.123Z\",\n  \"updated_at\": \"2025-12-04T10:30:15.123Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/observability/log_entry/#5-database-schema","title":"5. Database Schema","text":"<pre><code>CREATE TYPE log_level_enum AS ENUM (\n    'debug',\n    'info',\n    'warning',\n    'error',\n    'critical'\n);\n\nCREATE TABLE log_entry (\n    id UUID PRIMARY KEY,\n    level log_level_enum NOT NULL,\n    message TEXT NOT NULL,\n    context JSONB,\n    source VARCHAR(255),\n    timestamp TIMESTAMP NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1\n);\n\nCREATE INDEX idx_log_entry_level ON log_entry(level);\nCREATE INDEX idx_log_entry_timestamp ON log_entry(timestamp);\nCREATE INDEX idx_log_entry_source ON log_entry(source);\nCREATE INDEX idx_log_entry_context ON log_entry USING GIN (context);\n</code></pre>"},{"location":"data_models/observability/log_entry/#6-usage-example","title":"6. Usage Example","text":"<pre><code># Log error\nlog_entry = LogEntry(\n    id=generate_uuid(),\n    level=LogLevel.ERROR,\n    message=\"Model inference timeout\",\n    context={\n        \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n        \"job_id\": str(job.id),\n        \"timeout_seconds\": 60\n    },\n    source=\"model_worker\",\n    timestamp=now(),\n    created_at=now(),\n    updated_at=now(),\n    version=1\n)\n</code></pre>"},{"location":"data_models/observability/log_entry/#7-related-models","title":"7. Related Models","text":"<ul> <li>Trace - Distributed tracing context</li> <li>Metric - Quantitative measurements</li> </ul>"},{"location":"data_models/observability/metric/","title":"Metric Entity","text":"<p>Context: Observability Type: Entity Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/observability/metric/#1-overview","title":"1. Overview","text":"<p>The <code>Metric</code> entity represents a time-series data point for system and model monitoring.</p>"},{"location":"data_models/observability/metric/#2-structure","title":"2. Structure","text":"<pre>13fdad51ca62629400a2eafaed47dd1b37518a25cdb5655dd07f4185a5d03b053c8fe0a930cc6d15dfaf10139f39e0610d70562c479dc6254b9de4c6dfda5b1c</pre><pre>b1282a772b82e51dd0329ac3f50c5e49cce5e1256e722632a8a3dc116860ef09155a6c9fe2105966ddf36e7ee11e4f730aeead948739bf9f9118971afaeefa88</pre>"},{"location":"data_models/observability/metric/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/observability/metric/#31-name","title":"3.1 name","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Examples: <code>model.inference.latency</code>, <code>system.memory.used</code>, <code>queue.depth</code></li> </ul>"},{"location":"data_models/observability/metric/#32-value","title":"3.2 value","text":"<ul> <li>Type: Float</li> <li>Constraint: MUST</li> <li>Description: Numeric metric value</li> </ul>"},{"location":"data_models/observability/metric/#33-unit","title":"3.3 unit","text":"<ul> <li>Type: String</li> <li>Constraint: MAY</li> <li>Examples: <code>ms</code>, <code>mb</code>, <code>count</code>, <code>percent</code></li> </ul>"},{"location":"data_models/observability/metric/#34-tags","title":"3.4 tags","text":"<ul> <li>Type: Map <li>Constraint: MAY</li> <li>Description: Dimensional tags (model_id, region, etc.)</li> <li>Example: <code>{\"model_id\": \"...\", \"priority\": \"high\"}</code></li>"},{"location":"data_models/observability/metric/#35-timestamp","title":"3.5 timestamp","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MUST</li> <li>Description: When metric was recorded</li> </ul>"},{"location":"data_models/observability/metric/#4-common-metrics","title":"4. Common Metrics","text":"<ul> <li><code>model.inference.latency</code> - Inference response time (ms)</li> <li><code>model.inference.count</code> - Request count</li> <li><code>model.memory.used</code> - Memory usage (MB)</li> <li><code>queue.depth</code> - Pending jobs count</li> <li><code>model.health.status</code> - Health check result (0/1)</li> </ul>"},{"location":"data_models/observability/metric/#5-serialization","title":"5. Serialization","text":"<pre><code>{\n  \"id\": \"22000000-e29b-41d4-a716-446655440014\",\n  \"name\": \"model.inference.latency\",\n  \"value\": 45.3,\n  \"unit\": \"ms\",\n  \"tags\": {\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"priority\": \"high\"\n  },\n  \"timestamp\": \"2025-12-04T10:30:15.123Z\",\n  \"created_at\": \"2025-12-04T10:30:15.123Z\",\n  \"updated_at\": \"2025-12-04T10:30:15.123Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/observability/metric/#6-database-schema","title":"6. Database Schema","text":"<pre><code>CREATE TABLE metric (\n    id UUID PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    value REAL NOT NULL,\n    unit VARCHAR(50),\n    tags JSONB,\n    timestamp TIMESTAMP NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1\n);\n\nCREATE INDEX idx_metric_name ON metric(name);\nCREATE INDEX idx_metric_timestamp ON metric(timestamp);\nCREATE INDEX idx_metric_tags ON metric USING GIN (tags);\n</code></pre>"},{"location":"data_models/observability/metric/#7-related-models","title":"7. Related Models","text":"<ul> <li>Loaded Model - Generates metrics</li> <li>Inference Job - Generates latency metrics</li> </ul>"},{"location":"data_models/observability/trace/","title":"Trace Entity","text":"<p>Context: Observability Type: Entity Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/observability/trace/#1-overview","title":"1. Overview","text":"<p>The <code>Trace</code> entity represents a distributed tracing span for tracking request flow across system components.</p>"},{"location":"data_models/observability/trace/#2-structure","title":"2. Structure","text":"<pre>9b4aae9258968cffd70f32aac32b3750d46e987f8e8665e6ecf0d0d1c38dcc72b484bd65ff9607d5293bbdf78464e121f7f7d7364179250499013eac1d01e191</pre><pre>504318be8b3a59acf680bcd43958902de7ffcd70853e7cf3609fea3fa47ef08c3c240a509bf2a61150bae25bc8e21f56672cf18b9670875e94193025a67df4bc</pre>"},{"location":"data_models/observability/trace/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/observability/trace/#31-trace_id","title":"3.1 trace_id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique identifier for entire trace</li> </ul>"},{"location":"data_models/observability/trace/#32-parent_span_id","title":"3.2 parent_span_id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MAY</li> <li>Description: Parent span ID (null for root span)</li> </ul>"},{"location":"data_models/observability/trace/#33-span_name","title":"3.3 span_name","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Description: Operation name</li> <li>Examples: <code>model.load</code>, <code>inference.request</code>, <code>batch.process</code></li> </ul>"},{"location":"data_models/observability/trace/#34-start_time","title":"3.4 start_time","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MUST</li> <li>Description: Span start time</li> </ul>"},{"location":"data_models/observability/trace/#35-end_time","title":"3.5 end_time","text":"<ul> <li>Type: Timestamp</li> <li>Constraint: MAY</li> <li>Description: Span end time (null if ongoing)</li> </ul>"},{"location":"data_models/observability/trace/#36-duration_ms-derived","title":"3.6 duration_ms (Derived)","text":"<ul> <li>Type: Float</li> <li>Calculation: <code>(end_time - start_time) * 1000</code></li> </ul>"},{"location":"data_models/observability/trace/#37-attributes","title":"3.7 attributes","text":"<ul> <li>Type: Map <li>Constraint: MAY</li> <li>Description: Span attributes (model_id, job_id, etc.)</li>"},{"location":"data_models/observability/trace/#38-status","title":"3.8 status","text":"<ul> <li>Type: SpanStatus</li> <li>Constraint: MUST</li> <li>Values: <code>OK</code>, <code>ERROR</code></li> </ul>"},{"location":"data_models/observability/trace/#4-serialization","title":"4. Serialization","text":"<pre><code>{\n  \"id\": \"44000000-e29b-41d4-a716-446655440016\",\n  \"trace_id\": \"55000000-e29b-41d4-a716-446655440017\",\n  \"parent_span_id\": null,\n  \"span_name\": \"inference.request\",\n  \"start_time\": \"2025-12-04T10:30:00.000Z\",\n  \"end_time\": \"2025-12-04T10:30:00.123Z\",\n  \"attributes\": {\n    \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"job_id\": \"ee0e8400-e29b-41d4-a716-446655440010\",\n    \"priority\": \"high\"\n  },\n  \"status\": \"ok\",\n  \"created_at\": \"2025-12-04T10:30:00.000Z\",\n  \"updated_at\": \"2025-12-04T10:30:00.123Z\",\n  \"version\": 2\n}\n</code></pre>"},{"location":"data_models/observability/trace/#5-database-schema","title":"5. Database Schema","text":"<pre><code>CREATE TYPE span_status_enum AS ENUM ('ok', 'error');\n\nCREATE TABLE trace (\n    id UUID PRIMARY KEY,\n    trace_id UUID NOT NULL,\n    parent_span_id UUID,\n    span_name VARCHAR(255) NOT NULL,\n    start_time TIMESTAMP NOT NULL,\n    end_time TIMESTAMP,\n    attributes JSONB,\n    status span_status_enum NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1\n);\n\nCREATE INDEX idx_trace_trace_id ON trace(trace_id);\nCREATE INDEX idx_trace_parent ON trace(parent_span_id);\nCREATE INDEX idx_trace_span_name ON trace(span_name);\nCREATE INDEX idx_trace_start_time ON trace(start_time);\n</code></pre>"},{"location":"data_models/observability/trace/#6-usage-example","title":"6. Usage Example","text":"<pre><code># Start trace\ntrace = Trace(\n    id=generate_uuid(),\n    trace_id=generate_uuid(),\n    parent_span_id=None,\n    span_name=\"inference.request\",\n    start_time=now(),\n    attributes={\n        \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n        \"job_id\": str(job.id)\n    },\n    status=SpanStatus.OK,\n    created_at=now(),\n    updated_at=now(),\n    version=1\n)\n\n# Complete trace\ntrace.end_time = now()\ntrace.updated_at = now()\ntrace.version += 1\n\n# Create child span\nchild_trace = Trace(\n    id=generate_uuid(),\n    trace_id=trace.trace_id,\n    parent_span_id=trace.id,\n    span_name=\"model.inference\",\n    start_time=now(),\n    attributes={},\n    status=SpanStatus.OK,\n    created_at=now(),\n    updated_at=now(),\n    version=1\n)\n</code></pre>"},{"location":"data_models/observability/trace/#7-related-models","title":"7. Related Models","text":"<ul> <li>Log Entry - Can include trace_id for correlation</li> <li>Metric - Can include trace_id in tags</li> <li>Inference Job - Traced operations</li> </ul>"},{"location":"data_models/registry/model_catalog/","title":"Model Catalog Aggregate","text":"<p>Context: Registry Type: Aggregate Root Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/registry/model_catalog/#1-overview","title":"1. Overview","text":"<p>The <code>ModelCatalog</code> aggregate manages the collection of registered models and enforces consistency boundaries for model registration, versioning, and catalog operations.</p>"},{"location":"data_models/registry/model_catalog/#2-structure","title":"2. Structure","text":""},{"location":"data_models/registry/model_catalog/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>50a3a148d3e3d39ef7484b8b3e2f7d5abe1e168e374b35cd23ee7e58cf39decce1573595d67a9b49f854329d7805849dceb8ad9084cc62631debb91687322e9d</pre><pre>9b1c8eda1cb3d1e0c7a4d25a71b0e08372934532074d85eb521d32642a2acc21cd6c7514fe1d2382f615434b9d84fce78f2afbddd98a16918e762f5c324db8c9</pre>"},{"location":"data_models/registry/model_catalog/#22-relationships","title":"2.2 Relationships","text":"<pre>234bdde0af69aa6cc999869cff583f619eb29c984c0d5f56ce024bc29eaaa6d899f2cab5b78b41eacc98ffb7dd81e7d49c47ce0eb5142165e2768301397e37ba</pre><pre>bac954be875695d44f4a474fa32486dbc4fbd309715d826f32086bd7b3300ff079f69325bb6fbbc105269381f8277e1f99dd8f16ee27e6e321224dcdf38d2803</pre>"},{"location":"data_models/registry/model_catalog/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/registry/model_catalog/#31-id","title":"3.1 id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique identifier for the catalog</li> <li>Validation:</li> <li>MUST be valid UUID format</li> <li>MUST be unique across all catalogs</li> </ul>"},{"location":"data_models/registry/model_catalog/#32-name","title":"3.2 name","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Description: Catalog name for identification</li> <li>Validation:</li> <li>Length: 1-255 characters</li> <li>Examples: <code>default</code>, <code>production-catalog</code>, <code>experimental</code></li> </ul>"},{"location":"data_models/registry/model_catalog/#33-models","title":"3.3 models","text":"<ul> <li>Type: Map <li>Constraint: MUST</li> <li>Description: Collection of registered models indexed by ModelId</li> <li>Validation:</li> <li>Keys MUST be unique ModelId values</li> <li>Values MUST be valid Model entities</li>"},{"location":"data_models/registry/model_catalog/#34-events","title":"3.4 events","text":"<ul> <li>Type: Collection\\ <li>Constraint: Internal</li> <li>Description: Domain events generated by aggregate operations</li> <li>Validation:</li> <li>Used for event sourcing and notifications</li> <li>Cleared after event dispatch</li>"},{"location":"data_models/registry/model_catalog/#35-entity-pattern-fields","title":"3.5 Entity Pattern Fields","text":"<ul> <li>created_at: Timestamp when catalog was created (immutable)</li> <li>updated_at: Timestamp of last modification (auto-update)</li> <li>version: Integer for optimistic locking (auto-increment)</li> </ul>"},{"location":"data_models/registry/model_catalog/#4-constraints","title":"4. Constraints","text":""},{"location":"data_models/registry/model_catalog/#41-aggregate-pattern-constraints","title":"4.1 Aggregate Pattern Constraints","text":"<ul> <li>External code MUST NOT directly modify Model entities</li> <li>All model changes MUST go through ModelCatalog methods</li> <li>Aggregate MUST maintain invariants across all operations</li> <li>Events MUST be generated for all state changes</li> </ul>"},{"location":"data_models/registry/model_catalog/#42-entity-pattern-constraints","title":"4.2 Entity Pattern Constraints","text":"<ul> <li><code>id</code> MUST be unique across all ModelCatalog entities</li> <li><code>created_at</code> MUST NOT be modified after creation</li> <li><code>updated_at</code> MUST be updated on every modification</li> <li><code>version</code> MUST increment on every modification</li> </ul>"},{"location":"data_models/registry/model_catalog/#43-uniqueness-constraints","title":"4.3 Uniqueness Constraints","text":"<ul> <li>Each ModelId MUST appear at most once in <code>models</code> map</li> <li>Catalog <code>name</code> SHOULD be unique for easier identification</li> </ul>"},{"location":"data_models/registry/model_catalog/#44-referential-integrity","title":"4.4 Referential Integrity","text":"<ul> <li>Cannot delete catalog if models are referenced by active locks</li> <li>Deleting catalog cascades to all contained models and versions</li> </ul>"},{"location":"data_models/registry/model_catalog/#45-business-rules","title":"4.5 Business Rules","text":"<ul> <li>Cannot register model with duplicate ModelId</li> <li>Cannot add version to non-existent model</li> <li>Model MUST have at least one version after registration</li> <li>ModelId format MUST be validated before registration</li> </ul>"},{"location":"data_models/registry/model_catalog/#5-validation","title":"5. Validation","text":""},{"location":"data_models/registry/model_catalog/#51-syntax-validation","title":"5.1 Syntax Validation","text":"<pre><code>def validate_syntax(catalog: ModelCatalog) -&gt; ValidationResult:\n    errors = []\n\n    # Required fields\n    if not catalog.id:\n        errors.append(\"id is required\")\n    if not catalog.name:\n        errors.append(\"name is required\")\n    if catalog.models is None:\n        errors.append(\"models collection is required\")\n\n    # Validate each model\n    for model_id, model in catalog.models.items():\n        model_errors = validate_model_syntax(model)\n        errors.extend([f\"Model {model_id}: {e}\" for e in model_errors])\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/registry/model_catalog/#52-semantic-validation","title":"5.2 Semantic Validation","text":"<pre><code>def validate_semantic(catalog: ModelCatalog) -&gt; ValidationResult:\n    errors = []\n\n    # Check model consistency\n    for model_id, model in catalog.models.items():\n        if model.model_id != model_id:\n            errors.append(f\"Model key {model_id} doesn't match model.model_id {model.model_id}\")\n\n        # Each model must have versions\n        if not model.versions or len(model.versions) == 0:\n            errors.append(f\"Model {model_id} has no versions\")\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/registry/model_catalog/#53-invariant-validation","title":"5.3 Invariant Validation","text":"<pre><code>def validate(self) -&gt; ValidationResult:\n    \"\"\"Validate aggregate invariants\"\"\"\n    errors = []\n\n    # All models must be valid\n    for model in self.models.values():\n        result = model.validate()\n        if not result.valid:\n            errors.extend(result.errors)\n\n    # Referential integrity\n    for model in self.models.values():\n        for version in model.versions:\n            if version.model_id != model.model_id:\n                errors.append(\n                    f\"Version {version.id} references wrong model {version.model_id}\"\n                )\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/registry/model_catalog/#6-behavior","title":"6. Behavior","text":""},{"location":"data_models/registry/model_catalog/#61-model-registration","title":"6.1 Model Registration","text":"<pre><code>def register_model(self, model: Model) -&gt; None:\n    \"\"\"Register a new model in the catalog\"\"\"\n    # Validate model doesn't exist\n    if model.model_id in self.models:\n        raise ValueError(f\"Model {model.model_id} already registered\")\n\n    # Validate model has at least one version\n    if not model.versions or len(model.versions) == 0:\n        raise ValueError(\"Model must have at least one version\")\n\n    # Add to catalog\n    self.models[model.model_id] = model\n    self.updated_at = current_timestamp()\n    self.version += 1\n\n    # Generate event\n    self.addEvent(ModelRegisteredEvent(\n        catalog_id=self.id,\n        model_id=model.model_id,\n        timestamp=current_timestamp()\n    ))\n</code></pre>"},{"location":"data_models/registry/model_catalog/#62-model-unregistration","title":"6.2 Model Unregistration","text":"<pre><code>def unregister_model(self, model_id: ModelId) -&gt; None:\n    \"\"\"Remove a model from the catalog\"\"\"\n    if model_id not in self.models:\n        raise ValueError(f\"Model {model_id} not found\")\n\n    model = self.models[model_id]\n    del self.models[model_id]\n    self.updated_at = current_timestamp()\n    self.version += 1\n\n    # Generate event\n    self.addEvent(ModelUnregisteredEvent(\n        catalog_id=self.id,\n        model_id=model_id,\n        timestamp=current_timestamp()\n    ))\n</code></pre>"},{"location":"data_models/registry/model_catalog/#63-version-management","title":"6.3 Version Management","text":"<pre><code>def add_version(self, model_id: ModelId, version: ModelVersion) -&gt; None:\n    \"\"\"Add a new version to an existing model\"\"\"\n    if model_id not in self.models:\n        raise ValueError(f\"Model {model_id} not found\")\n\n    model = self.models[model_id]\n\n    # Delegate to model entity\n    model.add_version(version)\n\n    self.updated_at = current_timestamp()\n    self.version += 1\n\n    # Generate event\n    self.addEvent(ModelVersionAddedEvent(\n        catalog_id=self.id,\n        model_id=model_id,\n        version=version.version,\n        timestamp=current_timestamp()\n    ))\n</code></pre>"},{"location":"data_models/registry/model_catalog/#64-query-operations","title":"6.4 Query Operations","text":"<pre><code>def get_model(self, model_id: ModelId) -&gt; Model:\n    \"\"\"Retrieve a model by ID\"\"\"\n    if model_id not in self.models:\n        raise ValueError(f\"Model {model_id} not found\")\n    return self.models[model_id]\n\ndef list_models(self, filter: ModelFilter = None) -&gt; Collection[Model]:\n    \"\"\"List models with optional filtering\"\"\"\n    models = list(self.models.values())\n\n    if filter:\n        if filter.task_type:\n            models = [m for m in models if m.task_type == filter.task_type]\n        if filter.search_text:\n            models = [m for m in models if filter.search_text in str(m.model_id)]\n\n    return models\n\ndef find_by_task_type(self, task_type: TaskType) -&gt; Collection[Model]:\n    \"\"\"Find all models supporting a task type\"\"\"\n    return [m for m in self.models.values() if m.task_type == task_type]\n</code></pre>"},{"location":"data_models/registry/model_catalog/#65-event-management","title":"6.5 Event Management","text":"<pre><code>def addEvent(self, event: DomainEvent) -&gt; None:\n    \"\"\"Add a domain event\"\"\"\n    self.events.append(event)\n\ndef clearEvents(self) -&gt; None:\n    \"\"\"Clear all pending events\"\"\"\n    self.events.clear()\n</code></pre>"},{"location":"data_models/registry/model_catalog/#66-identity-methods","title":"6.6 Identity Methods","text":"<pre><code>def equals(self, other: ModelCatalog) -&gt; bool:\n    \"\"\"Identity-based equality\"\"\"\n    return self.id == other.id\n\ndef hash(self) -&gt; int:\n    \"\"\"Hash based on identity\"\"\"\n    return hash(self.id)\n</code></pre>"},{"location":"data_models/registry/model_catalog/#7-domain-events","title":"7. Domain Events","text":""},{"location":"data_models/registry/model_catalog/#71-modelregisteredevent","title":"7.1 ModelRegisteredEvent","text":"<pre><code>class ModelRegisteredEvent:\n    catalog_id: UUID\n    model_id: ModelId\n    task_type: TaskType\n    timestamp: Timestamp\n</code></pre>"},{"location":"data_models/registry/model_catalog/#72-modelunregisteredevent","title":"7.2 ModelUnregisteredEvent","text":"<pre><code>class ModelUnregisteredEvent:\n    catalog_id: UUID\n    model_id: ModelId\n    timestamp: Timestamp\n</code></pre>"},{"location":"data_models/registry/model_catalog/#73-modelversionaddedevent","title":"7.3 ModelVersionAddedEvent","text":"<pre><code>class ModelVersionAddedEvent:\n    catalog_id: UUID\n    model_id: ModelId\n    version: String\n    timestamp: Timestamp\n</code></pre>"},{"location":"data_models/registry/model_catalog/#8-serialization","title":"8. Serialization","text":""},{"location":"data_models/registry/model_catalog/#81-json-example","title":"8.1 JSON Example","text":"<pre><code>{\n  \"id\": \"770e8400-e29b-41d4-a716-446655440003\",\n  \"name\": \"default\",\n  \"models\": {\n    \"sentence-transformers/all-MiniLM-L6-v2\": {\n      \"id\": \"880e8400-e29b-41d4-a716-446655440004\",\n      \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n      \"task_type\": \"txt2embed\",\n      \"versions\": [\n        {\n          \"id\": \"990e8400-e29b-41d4-a716-446655440005\",\n          \"version\": \"v2.2.2\",\n          \"checksum\": \"sha256:abc123...\"\n        }\n      ]\n    }\n  },\n  \"created_at\": \"2025-12-04T10:30:00Z\",\n  \"updated_at\": \"2025-12-04T10:30:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/registry/model_catalog/#82-protocol-buffers","title":"8.2 Protocol Buffers","text":"<pre><code>message ModelCatalog {\n  string id = 1;\n  string name = 2;\n  map&lt;string, Model&gt; models = 3;\n  google.protobuf.Timestamp created_at = 4;\n  google.protobuf.Timestamp updated_at = 5;\n  int32 version_number = 6;\n}\n</code></pre>"},{"location":"data_models/registry/model_catalog/#9-database-schema","title":"9. Database Schema","text":"<pre><code>CREATE TABLE model_catalog (\n    id UUID PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1\n);\n\n-- Models belong to catalog\nCREATE TABLE model (\n    id UUID PRIMARY KEY,\n    catalog_id UUID NOT NULL REFERENCES model_catalog(id) ON DELETE CASCADE,\n    model_id VARCHAR(255) NOT NULL,\n    task_type task_type_enum NOT NULL,\n    ...\n    UNIQUE (catalog_id, model_id)\n);\n\nCREATE INDEX idx_model_catalog_id ON model(catalog_id);\n\n-- Trigger to auto-update updated_at and version\nCREATE TRIGGER update_model_catalog_timestamp\nBEFORE UPDATE ON model_catalog\nFOR EACH ROW\nEXECUTE FUNCTION update_timestamp_and_version();\n</code></pre>"},{"location":"data_models/registry/model_catalog/#10-usage-examples","title":"10. Usage Examples","text":""},{"location":"data_models/registry/model_catalog/#101-creating-catalog","title":"10.1 Creating Catalog","text":"<pre><code># Create new catalog\ncatalog = ModelCatalog(\n    id=generate_uuid(),\n    name=\"production-catalog\",\n    models={},\n    events=[],\n    created_at=current_timestamp(),\n    updated_at=current_timestamp(),\n    version=1\n)\n</code></pre>"},{"location":"data_models/registry/model_catalog/#102-registering-models","title":"10.2 Registering Models","text":"<pre><code># Register a model\nmodel = Model(\n    id=generate_uuid(),\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    task_type=TaskType.from_string(\"txt2embed\"),\n    versions=[version1]\n)\n\ncatalog.register_model(model)\n\n# Events are generated\nfor event in catalog.events:\n    event_bus.publish(event)\ncatalog.clearEvents()\n</code></pre>"},{"location":"data_models/registry/model_catalog/#103-querying-catalog","title":"10.3 Querying Catalog","text":"<pre><code># Get specific model\nmodel = catalog.get_model(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Find models by task type\nembedding_models = catalog.find_by_task_type(TaskType.from_string(\"txt2embed\"))\n\n# List with filter\nfilter = ModelFilter(task_type=TaskType.from_string(\"txt2txt\"))\ntext_models = catalog.list_models(filter)\n</code></pre>"},{"location":"data_models/registry/model_catalog/#104-version-management","title":"10.4 Version Management","text":"<pre><code># Add new version to existing model\nnew_version = ModelVersion(\n    id=generate_uuid(),\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    version=\"v2.3.0\",\n    checksum=\"sha256:new123...\",\n    artifact_uri=\"https://...\",\n    resource_requirements=ResourceRequirements(...)\n)\n\ncatalog.add_version(\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    version=new_version\n)\n</code></pre>"},{"location":"data_models/registry/model_catalog/#105-transaction-pattern","title":"10.5 Transaction Pattern","text":"<pre><code>def register_model_transaction(catalog: ModelCatalog, model: Model):\n    \"\"\"Register model with transactional event publishing\"\"\"\n    try:\n        # Validate\n        result = catalog.validate()\n        if not result.valid:\n            raise ValidationError(result.errors)\n\n        # Register\n        catalog.register_model(model)\n\n        # Persist\n        repository.save(catalog)\n\n        # Publish events\n        for event in catalog.events:\n            event_bus.publish(event)\n        catalog.clearEvents()\n\n        # Commit transaction\n        transaction.commit()\n    except Exception as e:\n        transaction.rollback()\n        raise\n</code></pre>"},{"location":"data_models/registry/model_catalog/#11-related-models","title":"11. Related Models","text":"<ul> <li>Model Entity - Child entity managed by aggregate</li> <li>Model Version - Grandchild entity</li> <li>Model Lock - References models in catalog</li> <li>Domain Events - Events generated by catalog operations</li> </ul>"},{"location":"data_models/registry/model_lock/","title":"Model Lock Entity","text":"<p>Context: Registry Type: Entity Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/registry/model_lock/#1-overview","title":"1. Overview","text":"<p>The <code>ModelLock</code> entity provides reproducible deployment by locking specific model versions with their exact configurations and checksums. It's analogous to a <code>package-lock.json</code> file, ensuring consistent deployments across environments.</p>"},{"location":"data_models/registry/model_lock/#2-structure","title":"2. Structure","text":""},{"location":"data_models/registry/model_lock/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>a8aa8550aaea45ad7b71bf8814667008db9b9a93d78c8c303898a3fdf92a2d13555a9709a13078d7dead78bb7c7fdaf6c3055f95f1a4f66a96c3ea4bc6db4ea1</pre><pre>c565d933c7d89e29524b60eaeb465706071e84c8b04f3d458e00a6a1e70163af5979f9d57a0e0e220c13abc17eae139e04a143165ac0af4fe6b6acaebdbd1d0e</pre>"},{"location":"data_models/registry/model_lock/#22-relationships","title":"2.2 Relationships","text":"<pre>c1bcb03ad0e93fcc6cd05366c402261304be25734159d0da5800340608a72ec8381d8f338d76044365d9ad1b6c11c4a5a863c02afb0bc8234b75a2f43739028a</pre><pre>bf9319588c7bb633bb15a911a99238793f747ca8b29fbd8049a27f52bc0f000ea3439d61125d1bfe6882bff6551e198b58b944e880c4b196d4058555a14831be</pre>"},{"location":"data_models/registry/model_lock/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/registry/model_lock/#31-id","title":"3.1 id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique identifier for the lock file</li> <li>Validation:</li> <li>MUST be valid UUID format</li> <li>MUST be unique across all lock files</li> </ul>"},{"location":"data_models/registry/model_lock/#32-name","title":"3.2 name","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Description: Human-readable name for the lock file</li> <li>Validation:</li> <li>Length: 1-255 characters</li> <li>Examples: <code>production-v1</code>, <code>experiment-baseline</code></li> </ul>"},{"location":"data_models/registry/model_lock/#33-description","title":"3.3 description","text":"<ul> <li>Type: String</li> <li>Constraint: MAY</li> <li>Description: Detailed description of the lock file purpose</li> <li>Validation:</li> <li>Length: 0-1000 characters</li> </ul>"},{"location":"data_models/registry/model_lock/#34-locked_models","title":"3.4 locked_models","text":"<ul> <li>Type: Collection\\ <li>Constraint: MUST</li> <li>Description: Set of locked model versions</li> <li>Validation:</li> <li>MUST contain at least one entry</li> <li>Each entry MUST have unique model_id</li>"},{"location":"data_models/registry/model_lock/#35-environment","title":"3.5 environment","text":"<ul> <li>Type: String</li> <li>Constraint: MAY</li> <li>Description: Target environment for this lock</li> <li>Validation:</li> <li>Common values: <code>production</code>, <code>staging</code>, <code>development</code></li> <li>Length: 1-50 characters</li> </ul>"},{"location":"data_models/registry/model_lock/#36-entity-pattern-fields","title":"3.6 Entity Pattern Fields","text":"<ul> <li>created_at: Timestamp when lock was created (immutable)</li> <li>updated_at: Timestamp of last modification (auto-update)</li> <li>version: Integer for optimistic locking (auto-increment)</li> <li>created_by: User who created the lock file (optional)</li> </ul>"},{"location":"data_models/registry/model_lock/#4-nested-type-lockedmodelentry","title":"4. Nested Type: LockedModelEntry","text":""},{"location":"data_models/registry/model_lock/#41-model_id","title":"4.1 model_id","text":"<ul> <li>Type: ModelId</li> <li>Constraint: MUST</li> <li>Description: Model identifier</li> <li>Validation:</li> <li>MUST match pattern: <code>{org}/{repo}</code></li> </ul>"},{"location":"data_models/registry/model_lock/#42-version","title":"4.2 version","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Description: Locked version string</li> <li>Validation:</li> <li>MUST be semantic version or branch name</li> <li>Length: 1-100 characters</li> </ul>"},{"location":"data_models/registry/model_lock/#43-checksum","title":"4.3 checksum","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Description: SHA256 hash of locked artifacts</li> <li>Validation:</li> <li>MUST be valid SHA256 format: <code>sha256:{64-char-hex}</code></li> </ul>"},{"location":"data_models/registry/model_lock/#44-artifact_uri","title":"4.4 artifact_uri","text":"<ul> <li>Type: String (URL)</li> <li>Constraint: MUST</li> <li>Description: Exact artifact location</li> <li>Validation:</li> <li>MUST be valid URL format</li> </ul>"},{"location":"data_models/registry/model_lock/#45-resource_requirements","title":"4.5 resource_requirements","text":"<ul> <li>Type: ResourceRequirements</li> <li>Constraint: MUST</li> <li>Description: Resource needs for this version</li> <li>Validation:</li> <li>MUST contain valid resource specifications</li> </ul>"},{"location":"data_models/registry/model_lock/#5-constraints","title":"5. Constraints","text":""},{"location":"data_models/registry/model_lock/#51-entity-pattern-constraints","title":"5.1 Entity Pattern Constraints","text":"<ul> <li><code>id</code> MUST be unique across all ModelLock entities</li> <li><code>created_at</code> MUST NOT be modified after creation</li> <li><code>updated_at</code> MUST be updated on every modification</li> <li><code>version</code> MUST increment on every modification</li> </ul>"},{"location":"data_models/registry/model_lock/#52-uniqueness-constraints","title":"5.2 Uniqueness Constraints","text":"<ul> <li><code>name</code> SHOULD be unique for easier identification</li> <li>Within <code>locked_models</code>, each <code>model_id</code> MUST be unique</li> </ul>"},{"location":"data_models/registry/model_lock/#53-referential-integrity","title":"5.3 Referential Integrity","text":"<ul> <li>Each <code>LockedModelEntry.model_id</code> SHOULD reference existing Model</li> <li>Each <code>LockedModelEntry.version</code> SHOULD reference existing ModelVersion</li> <li>However, lock files MAY contain references to deleted models (historical record)</li> </ul>"},{"location":"data_models/registry/model_lock/#54-business-rules","title":"5.4 Business Rules","text":"<ul> <li>Locked entries are immutable snapshots (checksum verification)</li> <li>Adding/removing entries creates new lock version</li> <li>Lock files enable rollback and audit trails</li> <li>SHOULD verify checksums before deployment</li> </ul>"},{"location":"data_models/registry/model_lock/#6-validation","title":"6. Validation","text":""},{"location":"data_models/registry/model_lock/#61-syntax-validation","title":"6.1 Syntax Validation","text":"<pre><code>def validate_syntax(lock: ModelLock) -&gt; ValidationResult:\n    errors = []\n\n    # Required fields\n    if not lock.id:\n        errors.append(\"id is required\")\n    if not lock.name:\n        errors.append(\"name is required\")\n    if not lock.locked_models or len(lock.locked_models) == 0:\n        errors.append(\"locked_models must contain at least one entry\")\n\n    # Validate each locked entry\n    for entry in lock.locked_models:\n        if not entry.model_id:\n            errors.append(f\"model_id is required for entry\")\n        if not entry.version:\n            errors.append(f\"version is required for entry {entry.model_id}\")\n        if not entry.checksum or not entry.checksum.startswith(\"sha256:\"):\n            errors.append(f\"valid checksum required for entry {entry.model_id}\")\n        if not entry.artifact_uri:\n            errors.append(f\"artifact_uri is required for entry {entry.model_id}\")\n\n    # Check for duplicate model_ids\n    model_ids = [e.model_id for e in lock.locked_models]\n    if len(model_ids) != len(set(model_ids)):\n        errors.append(\"locked_models contains duplicate model_ids\")\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/registry/model_lock/#62-semantic-validation","title":"6.2 Semantic Validation","text":"<pre><code>def validate_semantic(lock: ModelLock, registry: Registry) -&gt; ValidationResult:\n    errors = []\n    warnings = []\n\n    for entry in lock.locked_models:\n        # Warn if model doesn't exist (may have been deleted)\n        if not registry.model_exists(entry.model_id):\n            warnings.append(f\"Model {entry.model_id} not found in registry\")\n\n        # Verify checksum matches if version exists\n        version = registry.get_version(entry.model_id, entry.version)\n        if version and version.checksum != entry.checksum:\n            errors.append(f\"Checksum mismatch for {entry.model_id}@{entry.version}\")\n\n    return ValidationResult(\n        valid=len(errors) == 0,\n        errors=errors,\n        warnings=warnings\n    )\n</code></pre>"},{"location":"data_models/registry/model_lock/#7-behavior","title":"7. Behavior","text":""},{"location":"data_models/registry/model_lock/#71-creation","title":"7.1 Creation","text":"<pre><code>def create_model_lock(\n    name: str,\n    locked_models: List[LockedModelEntry],\n    description: str = None,\n    environment: str = None\n) -&gt; ModelLock:\n    \"\"\"Create a new model lock file\"\"\"\n    return ModelLock(\n        id=generate_uuid(),\n        name=name,\n        description=description,\n        locked_models=locked_models,\n        environment=environment,\n        created_at=current_timestamp(),\n        updated_at=current_timestamp(),\n        version=1\n    )\n</code></pre>"},{"location":"data_models/registry/model_lock/#72-lock-management","title":"7.2 Lock Management","text":"<pre><code>def add_locked_model(self, entry: LockedModelEntry) -&gt; None:\n    \"\"\"Add a model to the lock file\"\"\"\n    if any(e.model_id == entry.model_id for e in self.locked_models):\n        raise ValueError(f\"Model {entry.model_id} already locked\")\n\n    self.locked_models.append(entry)\n    self.updated_at = current_timestamp()\n    self.version += 1\n\ndef remove_locked_model(self, model_id: ModelId) -&gt; None:\n    \"\"\"Remove a model from the lock file\"\"\"\n    self.locked_models = [e for e in self.locked_models if e.model_id != model_id]\n    self.updated_at = current_timestamp()\n    self.version += 1\n\ndef get_locked_version(self, model_id: ModelId) -&gt; LockedModelEntry:\n    \"\"\"Get locked version for a model\"\"\"\n    for entry in self.locked_models:\n        if entry.model_id == model_id:\n            return entry\n    return None\n</code></pre>"},{"location":"data_models/registry/model_lock/#73-identity-methods","title":"7.3 Identity Methods","text":"<pre><code>def equals(self, other: ModelLock) -&gt; bool:\n    \"\"\"Identity-based equality\"\"\"\n    return self.id == other.id\n\ndef hash(self) -&gt; int:\n    \"\"\"Hash based on identity\"\"\"\n    return hash(self.id)\n</code></pre>"},{"location":"data_models/registry/model_lock/#8-serialization","title":"8. Serialization","text":""},{"location":"data_models/registry/model_lock/#81-json-example","title":"8.1 JSON Example","text":"<pre><code>{\n  \"id\": \"660e8400-e29b-41d4-a716-446655440002\",\n  \"name\": \"production-v1\",\n  \"description\": \"Production deployment lock for Q4 2025\",\n  \"environment\": \"production\",\n  \"locked_models\": [\n    {\n      \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n      \"version\": \"v2.2.2\",\n      \"checksum\": \"sha256:abc123def456789abc123def456789abc123def456789abc123def456789abcd\",\n      \"artifact_uri\": \"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/v2.2.2/model.safetensors\",\n      \"resource_requirements\": {\n        \"memory_mb\": 512,\n        \"gpu_vram_mb\": 0,\n        \"cpu_threads\": 2\n      }\n    },\n    {\n      \"model_id\": \"meta-llama/Llama-3.1-8B\",\n      \"version\": \"v1.0.0\",\n      \"checksum\": \"sha256:def789abc123456def789abc123456def789abc123456def789abc123456def\",\n      \"artifact_uri\": \"s3://models/llama-3.1-8b/model.safetensors\",\n      \"resource_requirements\": {\n        \"memory_mb\": 16384,\n        \"gpu_vram_mb\": 16384,\n        \"cpu_threads\": 8\n      }\n    }\n  ],\n  \"created_at\": \"2025-12-04T10:30:00Z\",\n  \"updated_at\": \"2025-12-04T10:30:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/registry/model_lock/#82-yaml-format-lock-file","title":"8.2 YAML Format (Lock File)","text":"<pre><code># ModelMora Lock File - production-v1\nname: production-v1\ndescription: Production deployment lock for Q4 2025\nenvironment: production\ncreated_at: 2025-12-04T10:30:00Z\n\nlocked_models:\n  - model_id: sentence-transformers/all-MiniLM-L6-v2\n    version: v2.2.2\n    checksum: sha256:abc123def456789abc123def456789abc123def456789abc123def456789abcd\n    artifact_uri: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/v2.2.2/model.safetensors\n    resource_requirements:\n      memory_mb: 512\n      gpu_vram_mb: 0\n      cpu_threads: 2\n\n  - model_id: meta-llama/Llama-3.1-8B\n    version: v1.0.0\n    checksum: sha256:def789abc123456def789abc123456def789abc123456def789abc123456def\n    artifact_uri: s3://models/llama-3.1-8b/model.safetensors\n    resource_requirements:\n      memory_mb: 16384\n      gpu_vram_mb: 16384\n      cpu_threads: 8\n</code></pre>"},{"location":"data_models/registry/model_lock/#9-database-schema","title":"9. Database Schema","text":"<pre><code>CREATE TABLE model_lock (\n    id UUID PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    environment VARCHAR(50),\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1,\n    created_by VARCHAR(255)\n);\n\nCREATE TABLE locked_model_entry (\n    lock_id UUID NOT NULL REFERENCES model_lock(id) ON DELETE CASCADE,\n    model_id VARCHAR(255) NOT NULL,\n    version VARCHAR(100) NOT NULL,\n    checksum VARCHAR(71) NOT NULL,\n    artifact_uri TEXT NOT NULL,\n    resource_requirements JSONB NOT NULL,\n    PRIMARY KEY (lock_id, model_id)\n);\n\nCREATE INDEX idx_model_lock_name ON model_lock(name);\nCREATE INDEX idx_model_lock_environment ON model_lock(environment);\nCREATE INDEX idx_locked_entry_lock_id ON locked_model_entry(lock_id);\n\n-- Trigger to auto-update updated_at and version\nCREATE TRIGGER update_model_lock_timestamp\nBEFORE UPDATE ON model_lock\nFOR EACH ROW\nEXECUTE FUNCTION update_timestamp_and_version();\n</code></pre>"},{"location":"data_models/registry/model_lock/#10-usage-examples","title":"10. Usage Examples","text":""},{"location":"data_models/registry/model_lock/#101-creating-lock-file","title":"10.1 Creating Lock File","text":"<pre><code># Create lock from current registry state\nentries = [\n    LockedModelEntry(\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        version=\"v2.2.2\",\n        checksum=\"sha256:abc123...\",\n        artifact_uri=\"https://...\",\n        resource_requirements=ResourceRequirements(...)\n    )\n]\n\nlock = create_model_lock(\n    name=\"production-v1\",\n    locked_models=entries,\n    description=\"Production deployment\",\n    environment=\"production\"\n)\n</code></pre>"},{"location":"data_models/registry/model_lock/#102-deploying-from-lock","title":"10.2 Deploying from Lock","text":"<pre><code># Load models from lock file\nlock = registry.get_lock(\"production-v1\")\n\nfor entry in lock.locked_models:\n    # Verify checksum\n    if not verify_checksum(entry.artifact_uri, entry.checksum):\n        raise IntegrityError(f\"Checksum mismatch for {entry.model_id}\")\n\n    # Load exact version\n    lifecycle_manager.load_model(\n        model_id=entry.model_id,\n        version=entry.version,\n        artifact_uri=entry.artifact_uri\n    )\n</code></pre>"},{"location":"data_models/registry/model_lock/#103-lock-file-exportimport","title":"10.3 Lock File Export/Import","text":"<pre><code># Export to YAML file\nlock_yaml = lock.to_yaml()\nwith open(\"modelmora.lock\", \"w\") as f:\n    f.write(lock_yaml)\n\n# Import from YAML file\nwith open(\"modelmora.lock\", \"r\") as f:\n    lock = ModelLock.from_yaml(f.read())\n    registry.register_lock(lock)\n</code></pre>"},{"location":"data_models/registry/model_lock/#11-related-models","title":"11. Related Models","text":"<ul> <li>Model Entity - Referenced model</li> <li>Model Version - Locked version details</li> <li>Resource Requirements - Resource specifications</li> </ul>"},{"location":"data_models/registry/model_version/","title":"Model Version Entity","text":"<p>Context: Registry Type: Entity Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/registry/model_version/#1-overview","title":"1. Overview","text":"<p>The <code>ModelVersion</code> entity represents a specific version of a model, including its artifacts, metadata, and resource requirements. It enables version tracking, reproducible deployments, and rollback capabilities.</p>"},{"location":"data_models/registry/model_version/#2-structure","title":"2. Structure","text":""},{"location":"data_models/registry/model_version/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>a79a91c4e61a31c20e2ab04eb9fad1ce7dbd42e1df67a8a6b3f017ec46956191d912cda427e42ab631e7cb2ae5fb314dc945f804180bf184877e12161eea45cf</pre><pre>c41ba8cc6c5ba1699320a63306d1f66f6e7c6a5b08d044cefafce3182ba877c0c6e89e0aee9d5ce16d21b79acb99f6df4eb6119289194e0b48f6269d5308a174</pre>"},{"location":"data_models/registry/model_version/#22-relationships","title":"2.2 Relationships","text":"<pre>2fbc1985e5e2446e13bbccdfd9eafb990002343530fbcd0418e77b6a782edbd322a83fc1d57bc5b645cc35f9d03b7de5e72aa7e58b9784cfdeabed83442c2094</pre><pre>943cf8b1c5e5a7803c3b71ad48793204e01118e347d28304edd48a11cb1898549a1e8b88eeaf1411af56535efe97298a399a2e0d366c484d2f507648dec13a7a</pre>"},{"location":"data_models/registry/model_version/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/registry/model_version/#31-id","title":"3.1 id","text":"<ul> <li>Type: UUID</li> <li>Constraint: MUST</li> <li>Description: Unique identifier for the model version</li> <li>Validation:</li> <li>MUST be valid UUID format</li> <li>MUST be unique across all model versions</li> </ul>"},{"location":"data_models/registry/model_version/#32-model_id","title":"3.2 model_id","text":"<ul> <li>Type: ModelId (Value Object)</li> <li>Constraint: MUST</li> <li>Description: Reference to parent model entity</li> <li>Validation:</li> <li>MUST match pattern: <code>{org}/{repo}</code></li> <li>MUST reference existing model in registry</li> </ul>"},{"location":"data_models/registry/model_version/#33-version","title":"3.3 version","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Description: Version identifier (semantic version or branch name)</li> <li>Validation:</li> <li>MUST match pattern: <code>v{major}.{minor}.{patch}</code> OR branch name</li> <li>Examples: <code>v2.2.2</code>, <code>main</code>, <code>dev</code></li> <li>Length: 1-100 characters</li> </ul>"},{"location":"data_models/registry/model_version/#34-checksum","title":"3.4 checksum","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Description: SHA256 hash of model artifacts for integrity verification</li> <li>Validation:</li> <li>MUST be valid SHA256 format: <code>sha256:{64-char-hex}</code></li> <li>Example: <code>sha256:abc123...def789</code></li> </ul>"},{"location":"data_models/registry/model_version/#35-artifact_uri","title":"3.5 artifact_uri","text":"<ul> <li>Type: String (URL)</li> <li>Constraint: MUST</li> <li>Description: Location of model artifacts</li> <li>Validation:</li> <li>MUST be valid URL format</li> <li>Supported schemes: <code>https://</code>, <code>s3://</code>, <code>file://</code></li> <li>Examples: <code>https://huggingface.co/...</code>, <code>s3://bucket/model.bin</code></li> </ul>"},{"location":"data_models/registry/model_version/#36-resource_requirements","title":"3.6 resource_requirements","text":"<ul> <li>Type: ResourceRequirements (Value Object)</li> <li>Constraint: MUST</li> <li>Description: Compute resources needed to run this version</li> <li>Validation:</li> <li>MUST contain valid resource specifications</li> <li>See ResourceRequirements model for details</li> </ul>"},{"location":"data_models/registry/model_version/#37-framework","title":"3.7 framework","text":"<ul> <li>Type: String</li> <li>Constraint: MUST</li> <li>Description: ML framework used by this version</li> <li>Validation:</li> <li>MUST be one of: <code>pytorch</code>, <code>tensorflow</code>, <code>jax</code>, <code>onnx</code></li> <li>Length: 1-50 characters</li> </ul>"},{"location":"data_models/registry/model_version/#38-framework_version","title":"3.8 framework_version","text":"<ul> <li>Type: String</li> <li>Constraint: MAY</li> <li>Description: Specific framework version</li> <li>Validation:</li> <li>SHOULD follow semantic versioning</li> <li>Example: <code>2.0.1</code>, <code>2.11.0</code></li> <li>Length: 1-50 characters</li> </ul>"},{"location":"data_models/registry/model_version/#39-metadata","title":"3.9 metadata","text":"<ul> <li>Type: Map <li>Constraint: MAY</li> <li>Description: Additional version-specific metadata</li> <li>Examples:</li> <li><code>author</code>, <code>license</code>, <code>tags</code>, <code>description</code></li> <li><code>training_dataset</code>, <code>accuracy_metrics</code></li>"},{"location":"data_models/registry/model_version/#310-entity-pattern-fields","title":"3.10 Entity Pattern Fields","text":"<ul> <li>created_at: Timestamp when version was registered (immutable)</li> <li>updated_at: Timestamp of last modification (auto-update)</li> <li>version: Integer for optimistic locking (auto-increment)</li> <li>created_by: User who registered this version (optional)</li> </ul>"},{"location":"data_models/registry/model_version/#4-constraints","title":"4. Constraints","text":""},{"location":"data_models/registry/model_version/#41-entity-pattern-constraints","title":"4.1 Entity Pattern Constraints","text":"<ul> <li><code>id</code> MUST be unique across all ModelVersion entities</li> <li><code>created_at</code> MUST NOT be modified after creation</li> <li><code>updated_at</code> MUST be updated on every modification</li> <li><code>version</code> MUST increment on every modification</li> </ul>"},{"location":"data_models/registry/model_version/#42-uniqueness-constraints","title":"4.2 Uniqueness Constraints","text":"<ul> <li><code>(model_id, version)</code> combination MUST be unique</li> <li><code>checksum</code> SHOULD be unique (same artifacts shouldn't be registered twice)</li> </ul>"},{"location":"data_models/registry/model_version/#43-referential-integrity","title":"4.3 Referential Integrity","text":"<ul> <li><code>model_id</code> MUST reference existing Model entity</li> <li>Deleting a Model MUST cascade delete all its ModelVersion entities</li> </ul>"},{"location":"data_models/registry/model_version/#44-business-rules","title":"4.4 Business Rules","text":"<ul> <li>Cannot modify <code>checksum</code> after creation</li> <li>Cannot modify <code>artifact_uri</code> after creation (immutable artifact location)</li> <li><code>version</code> string cannot be reused for same <code>model_id</code> even after deletion</li> <li>Framework and framework_version SHOULD match artifact's actual implementation</li> </ul>"},{"location":"data_models/registry/model_version/#5-validation","title":"5. Validation","text":""},{"location":"data_models/registry/model_version/#51-syntax-validation","title":"5.1 Syntax Validation","text":"<pre><code>def validate_syntax(model_version: ModelVersion) -&gt; ValidationResult:\n    errors = []\n\n    # Required fields\n    if not model_version.id:\n        errors.append(\"id is required\")\n    if not model_version.model_id:\n        errors.append(\"model_id is required\")\n    if not model_version.version:\n        errors.append(\"version is required\")\n    if not model_version.checksum:\n        errors.append(\"checksum is required\")\n    if not model_version.artifact_uri:\n        errors.append(\"artifact_uri is required\")\n    if not model_version.resource_requirements:\n        errors.append(\"resource_requirements is required\")\n    if not model_version.framework:\n        errors.append(\"framework is required\")\n\n    # Format validation\n    if model_version.checksum and not model_version.checksum.startswith(\"sha256:\"):\n        errors.append(\"checksum must be SHA256 format\")\n\n    if model_version.framework not in [\"pytorch\", \"tensorflow\", \"jax\", \"onnx\"]:\n        errors.append(\"framework must be one of: pytorch, tensorflow, jax, onnx\")\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/registry/model_version/#52-semantic-validation","title":"5.2 Semantic Validation","text":"<pre><code>def validate_semantic(model_version: ModelVersion, registry: Registry) -&gt; ValidationResult:\n    errors = []\n\n    # Model must exist\n    if not registry.model_exists(model_version.model_id):\n        errors.append(f\"Model {model_version.model_id} not found\")\n\n    # Version uniqueness\n    if registry.version_exists(model_version.model_id, model_version.version):\n        errors.append(f\"Version {model_version.version} already exists for {model_version.model_id}\")\n\n    # Artifact accessibility\n    if not is_accessible(model_version.artifact_uri):\n        errors.append(f\"Artifact URI not accessible: {model_version.artifact_uri}\")\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/registry/model_version/#6-behavior","title":"6. Behavior","text":""},{"location":"data_models/registry/model_version/#61-creation","title":"6.1 Creation","text":"<pre><code>def create_model_version(\n    model_id: ModelId,\n    version: str,\n    artifact_uri: str,\n    checksum: str,\n    framework: str,\n    resource_requirements: ResourceRequirements,\n    metadata: dict = None\n) -&gt; ModelVersion:\n    \"\"\"Register a new model version\"\"\"\n    return ModelVersion(\n        id=generate_uuid(),\n        model_id=model_id,\n        version=version,\n        artifact_uri=artifact_uri,\n        checksum=checksum,\n        framework=framework,\n        resource_requirements=resource_requirements,\n        metadata=metadata or {},\n        created_at=current_timestamp(),\n        updated_at=current_timestamp(),\n        version=1\n    )\n</code></pre>"},{"location":"data_models/registry/model_version/#62-metadata-update","title":"6.2 Metadata Update","text":"<pre><code>def update_metadata(self, new_metadata: dict) -&gt; None:\n    \"\"\"Update version metadata (non-artifact changes only)\"\"\"\n    if new_metadata:\n        self.metadata.update(new_metadata)\n        self.updated_at = current_timestamp()\n        self.version += 1\n</code></pre>"},{"location":"data_models/registry/model_version/#63-identity-methods","title":"6.3 Identity Methods","text":"<pre><code>def equals(self, other: ModelVersion) -&gt; bool:\n    \"\"\"Identity-based equality\"\"\"\n    return self.id == other.id\n\ndef hash(self) -&gt; int:\n    \"\"\"Hash based on identity\"\"\"\n    return hash(self.id)\n</code></pre>"},{"location":"data_models/registry/model_version/#7-serialization","title":"7. Serialization","text":""},{"location":"data_models/registry/model_version/#71-json-example","title":"7.1 JSON Example","text":"<pre><code>{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440001\",\n  \"model_id\": \"sentence-transformers/all-MiniLM-L6-v2\",\n  \"version\": \"v2.2.2\",\n  \"checksum\": \"sha256:abc123def456789abc123def456789abc123def456789abc123def456789abcd\",\n  \"artifact_uri\": \"https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/v2.2.2/model.safetensors\",\n  \"resource_requirements\": {\n    \"memory_mb\": 512,\n    \"gpu_vram_mb\": 0,\n    \"cpu_threads\": 2\n  },\n  \"framework\": \"pytorch\",\n  \"framework_version\": \"2.0.1\",\n  \"metadata\": {\n    \"author\": \"sentence-transformers\",\n    \"license\": \"Apache-2.0\",\n    \"max_seq_length\": 256\n  },\n  \"created_at\": \"2025-12-04T10:30:00Z\",\n  \"updated_at\": \"2025-12-04T10:30:00Z\",\n  \"version\": 1\n}\n</code></pre>"},{"location":"data_models/registry/model_version/#72-protocol-buffers","title":"7.2 Protocol Buffers","text":"<pre><code>message ModelVersion {\n  string id = 1;\n  string model_id = 2;\n  string version = 3;\n  string checksum = 4;\n  string artifact_uri = 5;\n  ResourceRequirements resource_requirements = 6;\n  string framework = 7;\n  string framework_version = 8;\n  map&lt;string, google.protobuf.Any&gt; metadata = 9;\n  google.protobuf.Timestamp created_at = 10;\n  google.protobuf.Timestamp updated_at = 11;\n  int32 version_number = 12;\n}\n</code></pre>"},{"location":"data_models/registry/model_version/#8-database-schema","title":"8. Database Schema","text":"<pre><code>CREATE TABLE model_version (\n    id UUID PRIMARY KEY,\n    model_id UUID NOT NULL REFERENCES model(id) ON DELETE CASCADE,\n    version VARCHAR(100) NOT NULL,\n    checksum VARCHAR(71) NOT NULL,  -- sha256: + 64 hex chars\n    artifact_uri TEXT NOT NULL,\n    framework VARCHAR(50) NOT NULL CHECK (framework IN ('pytorch', 'tensorflow', 'jax', 'onnx')),\n    framework_version VARCHAR(50),\n    metadata JSONB,\n    created_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),\n    version_number INTEGER NOT NULL DEFAULT 1,\n    created_by VARCHAR(255),\n    UNIQUE (model_id, version)\n);\n\nCREATE INDEX idx_model_version_model_id ON model_version(model_id);\nCREATE INDEX idx_model_version_checksum ON model_version(checksum);\n\n-- Trigger to auto-update updated_at and version\nCREATE TRIGGER update_model_version_timestamp\nBEFORE UPDATE ON model_version\nFOR EACH ROW\nEXECUTE FUNCTION update_timestamp_and_version();\n</code></pre>"},{"location":"data_models/registry/model_version/#9-usage-examples","title":"9. Usage Examples","text":""},{"location":"data_models/registry/model_version/#91-registering-new-version","title":"9.1 Registering New Version","text":"<pre><code># Register a new version\nversion = create_model_version(\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    version=\"v2.2.2\",\n    artifact_uri=\"https://huggingface.co/.../model.safetensors\",\n    checksum=\"sha256:abc123...\",\n    framework=\"pytorch\",\n    resource_requirements=ResourceRequirements(\n        memory_mb=512,\n        gpu_vram_mb=0,\n        cpu_threads=2\n    ),\n    metadata={\n        \"max_seq_length\": 256,\n        \"author\": \"sentence-transformers\"\n    }\n)\n</code></pre>"},{"location":"data_models/registry/model_version/#92-version-comparison","title":"9.2 Version Comparison","text":"<pre><code># Find versions for a model\nversions = registry.get_versions(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Get latest version\nlatest = max(versions, key=lambda v: parse_version(v.version))\n\n# Get version by checksum\nversion = registry.get_version_by_checksum(\"sha256:abc123...\")\n</code></pre>"},{"location":"data_models/registry/model_version/#10-related-models","title":"10. Related Models","text":"<ul> <li>Model Entity - Parent model entity</li> <li>Resource Requirements - Compute specifications</li> <li>Model Lock - Uses ModelVersion for reproducible deployments</li> </ul>"},{"location":"data_models/registry/resource_requirements/","title":"Resource Requirements Value Object","text":"<p>Context: Registry Type: Value Object Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/registry/resource_requirements/#1-overview","title":"1. Overview","text":"<p>The <code>ResourceRequirements</code> value object specifies the compute resources needed to load and run a model. It enables resource-aware scheduling and capacity planning.</p>"},{"location":"data_models/registry/resource_requirements/#2-structure","title":"2. Structure","text":""},{"location":"data_models/registry/resource_requirements/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>86d0ff7b15cab3ff1b4c292ef1eff0963c911d848cf7f66b4f4afc454096c5b379b336d69ddd6df1149ea3c1bdb7127790ae34ffdd947ab1292813f51192de69</pre><pre>20236a37caa751ed40bcc42f59431d5c048e86c52fe6c049240556bcc41027b183f9329d7c3baf946d2f48cefa3e54593ff37bb16118997b98ca3f8b74fff20f</pre>"},{"location":"data_models/registry/resource_requirements/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/registry/resource_requirements/#31-memory_mb","title":"3.1 memory_mb","text":"<ul> <li>Type: Integer</li> <li>Constraint: MUST, Immutable</li> <li>Description: Required system memory in megabytes</li> <li>Validation:</li> <li>MUST be &gt; 0</li> <li>Typical range: 100 MB to 100,000 MB</li> <li>Includes model weights + runtime overhead</li> </ul>"},{"location":"data_models/registry/resource_requirements/#32-gpu_vram_mb","title":"3.2 gpu_vram_mb","text":"<ul> <li>Type: Integer</li> <li>Constraint: MUST, Immutable</li> <li>Description: Required GPU VRAM in megabytes (0 for CPU-only)</li> <li>Validation:</li> <li>MUST be &gt;= 0</li> <li>0 indicates CPU-only model</li> <li>Typical range: 0 MB to 80,000 MB</li> </ul>"},{"location":"data_models/registry/resource_requirements/#33-cpu_threads","title":"3.3 cpu_threads","text":"<ul> <li>Type: Integer</li> <li>Constraint: MUST, Immutable</li> <li>Description: Number of CPU threads for inference</li> <li>Validation:</li> <li>MUST be &gt; 0</li> <li>Typical range: 1-64 threads</li> <li>Used for CPU inference and preprocessing</li> </ul>"},{"location":"data_models/registry/resource_requirements/#34-gpu_count","title":"3.4 gpu_count","text":"<ul> <li>Type: Integer</li> <li>Constraint: MAY, Immutable</li> <li>Description: Number of GPUs required</li> <li>Validation:</li> <li>MUST be &gt;= 0</li> <li>Default: 0 (CPU-only) or 1 (single GPU)</li> <li>Multi-GPU models: &gt; 1</li> </ul>"},{"location":"data_models/registry/resource_requirements/#35-min_memory_mb","title":"3.5 min_memory_mb","text":"<ul> <li>Type: Integer</li> <li>Constraint: MAY, Immutable</li> <li>Description: Minimum memory for reduced precision mode</li> <li>Validation:</li> <li>MUST be &lt;= memory_mb if specified</li> <li>Used for optimization hints</li> </ul>"},{"location":"data_models/registry/resource_requirements/#36-disk_space_mb","title":"3.6 disk_space_mb","text":"<ul> <li>Type: Integer</li> <li>Constraint: MAY, Immutable</li> <li>Description: Required disk space for model artifacts</li> <li>Validation:</li> <li>MUST be &gt; 0 if specified</li> <li>Used for cache management</li> </ul>"},{"location":"data_models/registry/resource_requirements/#4-constraints","title":"4. Constraints","text":""},{"location":"data_models/registry/resource_requirements/#41-value-object-constraints","title":"4.1 Value Object Constraints","text":"<ul> <li>All fields MUST be immutable after creation</li> <li>Equality based on structural comparison</li> <li>No identity field required</li> </ul>"},{"location":"data_models/registry/resource_requirements/#42-business-rules","title":"4.2 Business Rules","text":"<ul> <li><code>memory_mb</code> MUST be &gt; 0 (models require memory)</li> <li><code>gpu_vram_mb</code> = 0 indicates CPU-only model</li> <li><code>gpu_vram_mb</code> &gt; 0 AND <code>gpu_count</code> &gt; 0 indicates GPU model</li> <li><code>cpu_threads</code> SHOULD match available CPU cores for best performance</li> <li><code>min_memory_mb</code> &lt;= <code>memory_mb</code> if both specified</li> </ul>"},{"location":"data_models/registry/resource_requirements/#5-validation","title":"5. Validation","text":""},{"location":"data_models/registry/resource_requirements/#51-syntax-validation","title":"5.1 Syntax Validation","text":"<pre><code>def validate_syntax(req: ResourceRequirements) -&gt; ValidationResult:\n    errors = []\n\n    # Required fields\n    if req.memory_mb is None:\n        errors.append(\"memory_mb is required\")\n    elif req.memory_mb &lt;= 0:\n        errors.append(\"memory_mb must be &gt; 0\")\n\n    if req.gpu_vram_mb is None:\n        errors.append(\"gpu_vram_mb is required\")\n    elif req.gpu_vram_mb &lt; 0:\n        errors.append(\"gpu_vram_mb must be &gt;= 0\")\n\n    if req.cpu_threads is None:\n        errors.append(\"cpu_threads is required\")\n    elif req.cpu_threads &lt;= 0:\n        errors.append(\"cpu_threads must be &gt; 0\")\n\n    # Optional field validation\n    if req.gpu_count is not None and req.gpu_count &lt; 0:\n        errors.append(\"gpu_count must be &gt;= 0\")\n\n    if req.min_memory_mb is not None:\n        if req.min_memory_mb &lt;= 0:\n            errors.append(\"min_memory_mb must be &gt; 0\")\n        elif req.min_memory_mb &gt; req.memory_mb:\n            errors.append(\"min_memory_mb cannot exceed memory_mb\")\n\n    if req.disk_space_mb is not None and req.disk_space_mb &lt;= 0:\n        errors.append(\"disk_space_mb must be &gt; 0\")\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#52-semantic-validation","title":"5.2 Semantic Validation","text":"<pre><code>def validate_semantic(req: ResourceRequirements) -&gt; ValidationResult:\n    warnings = []\n\n    # Reasonable ranges\n    if req.memory_mb &gt; 100000:  # &gt; 100 GB\n        warnings.append(\"memory_mb unusually high (&gt; 100 GB)\")\n\n    if req.gpu_vram_mb &gt; 80000:  # &gt; 80 GB\n        warnings.append(\"gpu_vram_mb exceeds common GPU capacity\")\n\n    if req.cpu_threads &gt; 64:\n        warnings.append(\"cpu_threads exceeds typical server capacity\")\n\n    # Consistency checks\n    if req.gpu_vram_mb &gt; 0 and (req.gpu_count is None or req.gpu_count == 0):\n        warnings.append(\"gpu_vram_mb &gt; 0 but gpu_count not set (assuming 1)\")\n\n    return ValidationResult(valid=True, warnings=warnings)\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#6-behavior","title":"6. Behavior","text":""},{"location":"data_models/registry/resource_requirements/#61-creation","title":"6.1 Creation","text":"<pre><code>def create_resource_requirements(\n    memory_mb: int,\n    gpu_vram_mb: int = 0,\n    cpu_threads: int = 1,\n    gpu_count: int = None,\n    min_memory_mb: int = None,\n    disk_space_mb: int = None\n) -&gt; ResourceRequirements:\n    \"\"\"Create resource requirements specification\"\"\"\n    return ResourceRequirements(\n        memory_mb=memory_mb,\n        gpu_vram_mb=gpu_vram_mb,\n        cpu_threads=cpu_threads,\n        gpu_count=gpu_count or (1 if gpu_vram_mb &gt; 0 else 0),\n        min_memory_mb=min_memory_mb,\n        disk_space_mb=disk_space_mb\n    )\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#62-resource-checking","title":"6.2 Resource Checking","text":"<pre><code>def fits_in(self, available: ResourceCapacity) -&gt; bool:\n    \"\"\"Check if requirements can be satisfied by available resources\"\"\"\n    return (\n        self.memory_mb &lt;= available.memory_mb and\n        self.gpu_vram_mb &lt;= available.gpu_vram_mb and\n        self.cpu_threads &lt;= available.cpu_threads and\n        (self.gpu_count or 0) &lt;= available.gpu_count\n    )\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#63-value-object-methods","title":"6.3 Value Object Methods","text":"<pre><code>def equals(self, other: ResourceRequirements) -&gt; bool:\n    \"\"\"Structural equality\"\"\"\n    return (\n        self.memory_mb == other.memory_mb and\n        self.gpu_vram_mb == other.gpu_vram_mb and\n        self.cpu_threads == other.cpu_threads and\n        (self.gpu_count or 0) == (other.gpu_count or 0)\n    )\n\ndef hash(self) -&gt; int:\n    \"\"\"Hash based on value\"\"\"\n    return hash((\n        self.memory_mb,\n        self.gpu_vram_mb,\n        self.cpu_threads,\n        self.gpu_count or 0\n    ))\n\ndef toString(self) -&gt; str:\n    \"\"\"Human-readable representation\"\"\"\n    gpu_info = f\"{self.gpu_vram_mb}MB VRAM\" if self.gpu_vram_mb &gt; 0 else \"CPU-only\"\n    return f\"{self.memory_mb}MB RAM, {gpu_info}, {self.cpu_threads} threads\"\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#7-serialization","title":"7. Serialization","text":""},{"location":"data_models/registry/resource_requirements/#71-json-example","title":"7.1 JSON Example","text":"<pre><code>{\n  \"memory_mb\": 2048,\n  \"gpu_vram_mb\": 0,\n  \"cpu_threads\": 4,\n  \"gpu_count\": 0,\n  \"disk_space_mb\": 500\n}\n</code></pre> <pre><code>{\n  \"memory_mb\": 16384,\n  \"gpu_vram_mb\": 16384,\n  \"cpu_threads\": 8,\n  \"gpu_count\": 1,\n  \"min_memory_mb\": 12288,\n  \"disk_space_mb\": 15000\n}\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#72-protocol-buffers","title":"7.2 Protocol Buffers","text":"<pre><code>message ResourceRequirements {\n  int32 memory_mb = 1;\n  int32 gpu_vram_mb = 2;\n  int32 cpu_threads = 3;\n  int32 gpu_count = 4;\n  int32 min_memory_mb = 5;\n  int32 disk_space_mb = 6;\n}\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#8-database-schema","title":"8. Database Schema","text":"<pre><code>-- Embedded in model_version table as JSONB\nCREATE TABLE model_version (\n    ...\n    resource_requirements JSONB NOT NULL,\n    ...\n);\n\n-- Example query: Find models that fit in available resources\nSELECT * FROM model_version\nWHERE (resource_requirements-&gt;&gt;'memory_mb')::int &lt;= 8192\n  AND (resource_requirements-&gt;&gt;'gpu_vram_mb')::int = 0\n  AND (resource_requirements-&gt;&gt;'cpu_threads')::int &lt;= 8;\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#9-usage-examples","title":"9. Usage Examples","text":""},{"location":"data_models/registry/resource_requirements/#91-creating-requirements","title":"9.1 Creating Requirements","text":"<pre><code># CPU-only small model\ncpu_req = ResourceRequirements(\n    memory_mb=512,\n    gpu_vram_mb=0,\n    cpu_threads=2\n)\n\n# GPU-accelerated large model\ngpu_req = ResourceRequirements(\n    memory_mb=16384,\n    gpu_vram_mb=16384,\n    cpu_threads=8,\n    gpu_count=1,\n    disk_space_mb=15000\n)\n\n# Multi-GPU model\nmulti_gpu_req = ResourceRequirements(\n    memory_mb=32768,\n    gpu_vram_mb=40960,  # 40 GB total\n    cpu_threads=16,\n    gpu_count=2\n)\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#92-resource-allocation","title":"9.2 Resource Allocation","text":"<pre><code>def can_load_model(\n    requirements: ResourceRequirements,\n    available: ResourceCapacity\n) -&gt; bool:\n    \"\"\"Check if model can be loaded\"\"\"\n    return requirements.fits_in(available)\n\n# Example\navailable = ResourceCapacity(\n    memory_mb=16384,\n    gpu_vram_mb=0,\n    cpu_threads=8,\n    gpu_count=0\n)\n\nmodel_req = ResourceRequirements(memory_mb=2048, gpu_vram_mb=0, cpu_threads=4)\nif can_load_model(model_req, available):\n    # Load model\n    pass\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#93-resource-profiling","title":"9.3 Resource Profiling","text":"<pre><code>def estimate_requirements(model_path: str) -&gt; ResourceRequirements:\n    \"\"\"Estimate resource requirements from model artifacts\"\"\"\n    # Get model size\n    model_size_mb = get_file_size_mb(model_path)\n\n    # Estimate memory (model size + 50% overhead)\n    memory_mb = int(model_size_mb * 1.5)\n\n    # Check for GPU support\n    model_config = load_config(model_path)\n    gpu_vram_mb = memory_mb if model_config.use_gpu else 0\n\n    return ResourceRequirements(\n        memory_mb=memory_mb,\n        gpu_vram_mb=gpu_vram_mb,\n        cpu_threads=4,  # default\n        disk_space_mb=model_size_mb\n    )\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#94-comparison-and-sorting","title":"9.4 Comparison and Sorting","text":"<pre><code>def sort_by_resource_usage(models: List[Model]) -&gt; List[Model]:\n    \"\"\"Sort models by total resource usage\"\"\"\n    def resource_score(req: ResourceRequirements) -&gt; int:\n        return req.memory_mb + req.gpu_vram_mb\n\n    return sorted(\n        models,\n        key=lambda m: resource_score(m.resource_requirements)\n    )\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#10-common-patterns","title":"10. Common Patterns","text":""},{"location":"data_models/registry/resource_requirements/#101-resource-profiles","title":"10.1 Resource Profiles","text":"<pre><code># Predefined resource profiles\nPROFILES = {\n    \"tiny\": ResourceRequirements(memory_mb=256, gpu_vram_mb=0, cpu_threads=1),\n    \"small\": ResourceRequirements(memory_mb=1024, gpu_vram_mb=0, cpu_threads=2),\n    \"medium\": ResourceRequirements(memory_mb=4096, gpu_vram_mb=4096, cpu_threads=4, gpu_count=1),\n    \"large\": ResourceRequirements(memory_mb=16384, gpu_vram_mb=16384, cpu_threads=8, gpu_count=1),\n    \"xlarge\": ResourceRequirements(memory_mb=32768, gpu_vram_mb=40960, cpu_threads=16, gpu_count=2),\n}\n\n# Usage\nmodel.resource_requirements = PROFILES[\"small\"]\n</code></pre>"},{"location":"data_models/registry/resource_requirements/#11-related-models","title":"11. Related Models","text":"<ul> <li>Model Version - Contains ResourceRequirements</li> <li>Model Lock - Includes ResourceRequirements in locked entries</li> <li>Loaded Model - Uses requirements for allocation</li> <li>Memory Usage - Actual usage vs. requirements</li> </ul>"},{"location":"data_models/registry/task_type/","title":"Task Type Value Object","text":"<p>Context: Registry Type: Value Object Version: 1.0.0 Date: 2025-12-04</p>"},{"location":"data_models/registry/task_type/#1-overview","title":"1. Overview","text":"<p>The <code>TaskType</code> value object represents the ML task capability of a model. It defines supported inference operations and determines input/output data structures.</p>"},{"location":"data_models/registry/task_type/#2-structure","title":"2. Structure","text":""},{"location":"data_models/registry/task_type/#21-plantuml-diagram","title":"2.1 PlantUML Diagram","text":"<pre>d488a7b56757e4b047d83bc41c7c1a1bcb3184be3ba48cfbf575607c859ec083cc0caf6ee9cbaf67e2448c471c7aac2372862e71d8a16c088e8ddb631e16d83e</pre><pre>6ac2bfbb9d36c8a9b618fad52b851ba0c183529efc4d8978d0714f46e7b8c34fa81c655c037c2d3e70414b69e40b1ff9899191f48df124c229a9f01378efa0d0</pre>"},{"location":"data_models/registry/task_type/#3-field-specifications","title":"3. Field Specifications","text":""},{"location":"data_models/registry/task_type/#31-type","title":"3.1 type","text":"<ul> <li>Type: TaskTypeEnum</li> <li>Constraint: MUST, Immutable</li> <li>Description: The specific task type</li> <li>Validation:</li> <li>MUST be one of the defined enum values</li> </ul>"},{"location":"data_models/registry/task_type/#4-task-type-enumeration","title":"4. Task Type Enumeration","text":""},{"location":"data_models/registry/task_type/#41-supported-task-types","title":"4.1 Supported Task Types","text":"Value Description Input Output <code>txt2embed</code> Text to embedding vector Text string Float array <code>txt2txt</code> Text generation/transformation Text string Text string <code>txt2img</code> Text to image generation Text prompt Image (bytes) <code>img2txt</code> Image captioning/OCR Image (bytes) Text string <code>img2img</code> Image transformation Image (bytes) Image (bytes) <code>audio2txt</code> Speech-to-text Audio (bytes) Text string <code>txt2audio</code> Text-to-speech Text string Audio (bytes) <code>classification</code> Text/image classification Text or Image Class labels + scores <code>object_detection</code> Object detection in images Image (bytes) Bounding boxes + labels <code>question_answering</code> Question answering Question + Context Answer text"},{"location":"data_models/registry/task_type/#42-task-type-details","title":"4.2 Task Type Details","text":""},{"location":"data_models/registry/task_type/#txt2embed","title":"txt2embed","text":"<ul> <li>Purpose: Generate dense vector embeddings from text</li> <li>Use Cases: Semantic search, similarity, clustering</li> <li>Input: Single text string or batch of strings</li> <li>Output: Float array(s) of fixed dimension</li> <li>Example Models: <code>sentence-transformers/all-MiniLM-L6-v2</code></li> </ul>"},{"location":"data_models/registry/task_type/#txt2txt","title":"txt2txt","text":"<ul> <li>Purpose: Generate or transform text</li> <li>Use Cases: Completion, summarization, translation, chat</li> <li>Input: Text prompt/input</li> <li>Output: Generated text</li> <li>Example Models: <code>meta-llama/Llama-3.1-8B</code>, <code>gpt2</code></li> </ul>"},{"location":"data_models/registry/task_type/#txt2img","title":"txt2img","text":"<ul> <li>Purpose: Generate images from text descriptions</li> <li>Use Cases: Image generation, creative AI</li> <li>Input: Text prompt + optional parameters (size, style)</li> <li>Output: Image binary data</li> <li>Example Models: <code>stabilityai/stable-diffusion-xl-base-1.0</code></li> </ul>"},{"location":"data_models/registry/task_type/#img2txt","title":"img2txt","text":"<ul> <li>Purpose: Generate text descriptions from images</li> <li>Use Cases: Image captioning, OCR, visual QA</li> <li>Input: Image binary data</li> <li>Output: Text description</li> <li>Example Models: <code>Salesforce/blip-image-captioning-large</code></li> </ul>"},{"location":"data_models/registry/task_type/#img2img","title":"img2img","text":"<ul> <li>Purpose: Transform or edit images</li> <li>Use Cases: Style transfer, super-resolution, inpainting</li> <li>Input: Image binary data + optional parameters</li> <li>Output: Transformed image binary data</li> <li>Example Models: <code>timbrooks/instruct-pix2pix</code></li> </ul>"},{"location":"data_models/registry/task_type/#audio2txt","title":"audio2txt","text":"<ul> <li>Purpose: Transcribe speech to text</li> <li>Use Cases: Speech recognition, transcription</li> <li>Input: Audio binary data (WAV, MP3)</li> <li>Output: Transcribed text</li> <li>Example Models: <code>openai/whisper-large-v3</code></li> </ul>"},{"location":"data_models/registry/task_type/#txt2audio","title":"txt2audio","text":"<ul> <li>Purpose: Synthesize speech from text</li> <li>Use Cases: Text-to-speech, voice generation</li> <li>Input: Text string</li> <li>Output: Audio binary data</li> <li>Example Models: <code>suno/bark</code></li> </ul>"},{"location":"data_models/registry/task_type/#classification","title":"classification","text":"<ul> <li>Purpose: Classify text or images into categories</li> <li>Use Cases: Sentiment analysis, topic classification</li> <li>Input: Text string or image</li> <li>Output: Class labels with confidence scores</li> <li>Example Models: <code>distilbert-base-uncased-finetuned-sst-2-english</code></li> </ul>"},{"location":"data_models/registry/task_type/#object_detection","title":"object_detection","text":"<ul> <li>Purpose: Detect and locate objects in images</li> <li>Use Cases: Object detection, localization</li> <li>Input: Image binary data</li> <li>Output: Bounding boxes with labels and scores</li> <li>Example Models: <code>facebook/detr-resnet-50</code></li> </ul>"},{"location":"data_models/registry/task_type/#question_answering","title":"question_answering","text":"<ul> <li>Purpose: Answer questions based on context</li> <li>Use Cases: QA systems, information extraction</li> <li>Input: Question + context text</li> <li>Output: Answer text + confidence</li> <li>Example Models: <code>deepset/roberta-base-squad2</code></li> </ul>"},{"location":"data_models/registry/task_type/#5-constraints","title":"5. Constraints","text":""},{"location":"data_models/registry/task_type/#51-value-object-constraints","title":"5.1 Value Object Constraints","text":"<ul> <li><code>type</code> MUST be immutable after creation</li> <li>Equality based on <code>type</code> value (structural)</li> <li>No identity field required</li> </ul>"},{"location":"data_models/registry/task_type/#52-business-rules","title":"5.2 Business Rules","text":"<ul> <li>TaskType determines required input/output formats</li> <li>Models MUST implement task-specific interfaces</li> <li>Cannot change TaskType of a registered model</li> <li>Input validation rules vary by TaskType</li> </ul>"},{"location":"data_models/registry/task_type/#6-validation","title":"6. Validation","text":""},{"location":"data_models/registry/task_type/#61-syntax-validation","title":"6.1 Syntax Validation","text":"<pre><code>def validate_syntax(task_type: TaskType) -&gt; ValidationResult:\n    errors = []\n\n    if not task_type.type:\n        errors.append(\"type is required\")\n\n    valid_types = [\n        \"txt2embed\", \"txt2txt\", \"txt2img\", \"img2txt\", \"img2img\",\n        \"audio2txt\", \"txt2audio\", \"classification\",\n        \"object_detection\", \"question_answering\"\n    ]\n\n    if task_type.type not in valid_types:\n        errors.append(f\"type must be one of: {', '.join(valid_types)}\")\n\n    return ValidationResult(valid=len(errors) == 0, errors=errors)\n</code></pre>"},{"location":"data_models/registry/task_type/#7-behavior","title":"7. Behavior","text":""},{"location":"data_models/registry/task_type/#71-creation","title":"7.1 Creation","text":"<pre><code>def create_task_type(type_str: str) -&gt; TaskType:\n    \"\"\"Create a TaskType from string\"\"\"\n    return TaskType(type=TaskTypeEnum[type_str.upper().replace(\"-\", \"_\")])\n</code></pre>"},{"location":"data_models/registry/task_type/#72-schema-retrieval","title":"7.2 Schema Retrieval","text":"<pre><code>def get_input_schema(self) -&gt; Schema:\n    \"\"\"Get input data schema for this task type\"\"\"\n    schemas = {\n        TaskTypeEnum.TXT2EMBED: {\n            \"type\": \"object\",\n            \"properties\": {\n                \"text\": {\"type\": \"string\"}\n            },\n            \"required\": [\"text\"]\n        },\n        TaskTypeEnum.TXT2TXT: {\n            \"type\": \"object\",\n            \"properties\": {\n                \"prompt\": {\"type\": \"string\"},\n                \"max_tokens\": {\"type\": \"integer\", \"default\": 100}\n            },\n            \"required\": [\"prompt\"]\n        },\n        # ... other task types\n    }\n    return schemas[self.type]\n\ndef get_output_schema(self) -&gt; Schema:\n    \"\"\"Get output data schema for this task type\"\"\"\n    schemas = {\n        TaskTypeEnum.TXT2EMBED: {\n            \"type\": \"object\",\n            \"properties\": {\n                \"embedding\": {\n                    \"type\": \"array\",\n                    \"items\": {\"type\": \"number\"}\n                }\n            }\n        },\n        # ... other task types\n    }\n    return schemas[self.type]\n</code></pre>"},{"location":"data_models/registry/task_type/#73-value-object-methods","title":"7.3 Value Object Methods","text":"<pre><code>def equals(self, other: TaskType) -&gt; bool:\n    \"\"\"Structural equality\"\"\"\n    return self.type == other.type\n\ndef hash(self) -&gt; int:\n    \"\"\"Hash based on value\"\"\"\n    return hash(self.type)\n\ndef toString(self) -&gt; str:\n    \"\"\"String representation\"\"\"\n    return self.type.lower()\n</code></pre>"},{"location":"data_models/registry/task_type/#8-serialization","title":"8. Serialization","text":""},{"location":"data_models/registry/task_type/#81-json-example","title":"8.1 JSON Example","text":"<pre><code>{\n  \"task_type\": \"txt2embed\"\n}\n</code></pre>"},{"location":"data_models/registry/task_type/#82-protocol-buffers","title":"8.2 Protocol Buffers","text":"<pre><code>enum TaskType {\n  TASK_TYPE_UNSPECIFIED = 0;\n  TASK_TYPE_TXT2EMBED = 1;\n  TASK_TYPE_TXT2TXT = 2;\n  TASK_TYPE_TXT2IMG = 3;\n  TASK_TYPE_IMG2TXT = 4;\n  TASK_TYPE_IMG2IMG = 5;\n  TASK_TYPE_AUDIO2TXT = 6;\n  TASK_TYPE_TXT2AUDIO = 7;\n  TASK_TYPE_CLASSIFICATION = 8;\n  TASK_TYPE_OBJECT_DETECTION = 9;\n  TASK_TYPE_QUESTION_ANSWERING = 10;\n}\n</code></pre>"},{"location":"data_models/registry/task_type/#9-database-schema","title":"9. Database Schema","text":"<pre><code>-- Stored as VARCHAR with CHECK constraint\nCREATE TYPE task_type_enum AS ENUM (\n    'txt2embed',\n    'txt2txt',\n    'txt2img',\n    'img2txt',\n    'img2img',\n    'audio2txt',\n    'txt2audio',\n    'classification',\n    'object_detection',\n    'question_answering'\n);\n\n-- In model table\nCREATE TABLE model (\n    ...\n    task_type task_type_enum NOT NULL,\n    ...\n);\n\nCREATE INDEX idx_model_task_type ON model(task_type);\n</code></pre>"},{"location":"data_models/registry/task_type/#10-usage-examples","title":"10. Usage Examples","text":""},{"location":"data_models/registry/task_type/#101-creating-task-types","title":"10.1 Creating Task Types","text":"<pre><code># From string\ntask = TaskType.from_string(\"txt2embed\")\n\n# From enum\ntask = TaskType(type=TaskTypeEnum.TXT2EMBED)\n\n# Comparison\ntask1 = TaskType.from_string(\"txt2embed\")\ntask2 = TaskType.from_string(\"txt2embed\")\nassert task1.equals(task2)  # True (structural equality)\n</code></pre>"},{"location":"data_models/registry/task_type/#102-input-validation","title":"10.2 Input Validation","text":"<pre><code>def validate_input_for_task(task_type: TaskType, input_data: dict) -&gt; bool:\n    \"\"\"Validate input data matches task type schema\"\"\"\n    schema = task_type.get_input_schema()\n    return validate_against_schema(input_data, schema)\n\n# Example usage\ntask = TaskType.from_string(\"txt2embed\")\ninput_data = {\"text\": \"Hello world\"}\nif validate_input_for_task(task, input_data):\n    # Process inference request\n    pass\n</code></pre>"},{"location":"data_models/registry/task_type/#103-task-specific-processing","title":"10.3 Task-Specific Processing","text":"<pre><code>def process_by_task_type(task_type: TaskType, input_data: dict):\n    \"\"\"Route processing based on task type\"\"\"\n    if task_type.type == TaskTypeEnum.TXT2EMBED:\n        return embedding_pipeline(input_data[\"text\"])\n    elif task_type.type == TaskTypeEnum.TXT2TXT:\n        return generation_pipeline(input_data[\"prompt\"])\n    elif task_type.type == TaskTypeEnum.TXT2IMG:\n        return image_generation_pipeline(input_data[\"prompt\"])\n    # ... other task types\n</code></pre>"},{"location":"data_models/registry/task_type/#11-related-models","title":"11. Related Models","text":"<ul> <li>Model Entity - Uses TaskType to define capabilities</li> <li>Input Data - Task-specific input structures</li> <li>Output Data - Task-specific output structures</li> </ul>"},{"location":"deployment/docker/","title":"Docker Deployment","text":"<p>Deploy ModelMora using Docker containers.</p>"},{"location":"deployment/docker/#quick-start","title":"Quick Start","text":"<pre><code>docker run -p 8000:8000 ghcr.io/jomarjunior/modelmora:latest\n</code></pre>"},{"location":"deployment/docker/#docker-compose","title":"Docker Compose","text":"<p>Create <code>docker-compose.yml</code>:</p> <pre><code>services:\n  modelmora:\n    image: ghcr.io/jomarjunior/modelmora:latest\n    ports:\n      - \"8000:8000\"\n      - \"50051:50051\"\n    environment:\n      - MODELMORA_DB_PATH=/data/models.db\n      - MODELMORA_CACHE_DIR=/cache\n      - MODELMORA_MAX_WORKERS=4\n      - MODELMORA_DEVICE=cuda\n    volumes:\n      - ./data:/data\n      - ./cache:/cache\n      - ./config:/app/config\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre> <p>Start:</p> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"deployment/docker/#build-from-source","title":"Build from Source","text":"<pre><code>docker build -t modelmora:local .\ndocker run -p 8000:8000 modelmora:local\n</code></pre>"},{"location":"deployment/docker/#gpu-support","title":"GPU Support","text":"<p>Ensure NVIDIA Container Toolkit is installed:</p> <pre><code># Install nvidia-container-toolkit\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n</code></pre> <p>Run with GPU:</p> <pre><code>docker run --gpus all -p 8000:8000 ghcr.io/jomarjunior/modelmora:latest\n</code></pre>"},{"location":"deployment/docker/#next-steps","title":"Next Steps","text":"<ul> <li>Kubernetes Deployment</li> <li>Configuration Guide</li> </ul>"},{"location":"deployment/kubernetes/","title":"Kubernetes Deployment","text":"<p>Deploy ModelMora on Kubernetes clusters.</p>"},{"location":"deployment/kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.20+</li> <li>kubectl configured</li> <li>Helm 3.0+ (optional)</li> </ul>"},{"location":"deployment/kubernetes/#quick-start","title":"Quick Start","text":"<pre><code>kubectl apply -f kubernetes/\n</code></pre>"},{"location":"deployment/kubernetes/#deployment-manifests","title":"Deployment Manifests","text":"<p>Coming soon in Phase 4 (Week 19-20).</p>"},{"location":"deployment/kubernetes/#helm-chart","title":"Helm Chart","text":"<p>Coming soon in Phase 4 (Week 19-20).</p>"},{"location":"deployment/kubernetes/#next-steps","title":"Next Steps","text":"<ul> <li>Docker Deployment</li> <li>Architecture Overview</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>ModelMora provides flexible configuration through environment variables, YAML files, and command-line arguments.</p>"},{"location":"getting-started/configuration/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<p>Configuration is loaded in the following order (later sources override earlier ones):</p> <ol> <li>Default values</li> <li>YAML configuration file</li> <li>Environment variables</li> <li>Command-line arguments</li> </ol>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/configuration/#core-settings","title":"Core Settings","text":"Variable Description Default <code>MODELMORA_HOST</code> Server host <code>0.0.0.0</code> <code>MODELMORA_PORT</code> REST API port <code>8000</code> <code>MODELMORA_GRPC_PORT</code> gRPC API port <code>50051</code> <code>MODELMORA_LOG_LEVEL</code> Logging level <code>INFO</code> <code>MODELMORA_DB_PATH</code> Database file path <code>./data/models.db</code> <code>MODELMORA_CACHE_DIR</code> Model cache directory <code>~/.cache/modelmora</code>"},{"location":"getting-started/configuration/#resource-management","title":"Resource Management","text":"Variable Description Default <code>MODELMORA_MAX_WORKERS</code> Maximum worker processes <code>4</code> <code>MODELMORA_MAX_MEMORY_GB</code> Memory limit (GB) <code>8</code> <code>MODELMORA_GPU_MEMORY_FRACTION</code> GPU memory fraction <code>0.8</code> <code>MODELMORA_WORKER_TIMEOUT</code> Worker timeout (seconds) <code>300</code>"},{"location":"getting-started/configuration/#queue-settings","title":"Queue Settings","text":"Variable Description Default <code>MODELMORA_QUEUE_SIZE</code> Request queue size <code>1000</code> <code>MODELMORA_BATCH_SIZE</code> Default batch size <code>16</code> <code>MODELMORA_BATCH_TIMEOUT_MS</code> Batch timeout (ms) <code>100</code>"},{"location":"getting-started/configuration/#model-settings","title":"Model Settings","text":"Variable Description Default <code>MODELMORA_LAZY_LOAD</code> Enable lazy loading <code>true</code> <code>MODELMORA_DEVICE</code> Default device <code>cuda</code> <code>MODELMORA_MODEL_TTL</code> Model TTL (seconds) <code>3600</code>"},{"location":"getting-started/configuration/#yaml-configuration","title":"YAML Configuration","text":""},{"location":"getting-started/configuration/#basic-configuration","title":"Basic Configuration","text":"<p><code>config/modelmora.yml</code>:</p> <pre><code>server:\n  host: 0.0.0.0\n  port: 8000\n  grpc_port: 50051\n  log_level: INFO\n\nresources:\n  max_workers: 4\n  max_memory_gb: 8\n  gpu_memory_fraction: 0.8\n  worker_timeout: 300\n\nqueue:\n  size: 1000\n  batch_size: 16\n  batch_timeout_ms: 100\n\nmodels:\n  lazy_load: true\n  device: cuda\n  ttl: 3600\n  cache_dir: ${HOME}/.cache/modelmora\n</code></pre>"},{"location":"getting-started/configuration/#model-definitions","title":"Model Definitions","text":"<p><code>config/models.yml</code>:</p> <pre><code>models:\n  # Text embedding model\n  - name: all-MiniLM-L6-v2\n    source: sentence-transformers/all-MiniLM-L6-v2\n    task: text-embedding\n    version: 1.0.0\n    device: ${DEVICE:-cuda}\n    priority: high\n    config:\n      max_seq_length: 512\n      batch_size: 32\n      normalize_embeddings: true\n\n  # Image embedding model\n  - name: clip-vit-base\n    source: openai/clip-vit-base-patch32\n    task: image-embedding\n    device: cuda\n    config:\n      image_size: 224\n      batch_size: 16\n\n  # Text generation model\n  - name: llama-7b\n    source: meta-llama/Llama-2-7b-hf\n    task: text-generation\n    device: cuda:0\n    config:\n      max_length: 2048\n      temperature: 0.7\n      top_p: 0.9\n\n  # Image generation model\n  - name: stable-diffusion\n    source: runwayml/stable-diffusion-v1-5\n    task: text-to-image\n    device: cuda:1\n    config:\n      height: 512\n      width: 512\n      num_inference_steps: 50\n      guidance_scale: 7.5\n</code></pre>"},{"location":"getting-started/configuration/#environment-variable-interpolation","title":"Environment Variable Interpolation","text":"<pre><code># Use environment variables with defaults\ndatabase:\n  path: ${DB_PATH:-./data/models.db}\n  pool_size: ${DB_POOL_SIZE:-10}\n\ncache:\n  dir: ${CACHE_DIR:-~/.cache/modelmora}\n  max_size_gb: ${CACHE_SIZE_GB:-50}\n\n# Required environment variables (no default)\nauth:\n  jwt_secret: ${JWT_SECRET}\n  token_url: ${AUTH_TOKEN_URL}\n</code></pre>"},{"location":"getting-started/configuration/#command-line-arguments","title":"Command-Line Arguments","text":"<pre><code># Start server with custom configuration\npoetry run modelmora serve \\\n  --host 0.0.0.0 \\\n  --port 8080 \\\n  --config config/modelmora.yml \\\n  --models config/models.yml \\\n  --log-level DEBUG \\\n  --workers 8\n\n# Initialize with specific device\npoetry run modelmora init \\\n  --config config/models.yml \\\n  --device cuda:0\n\n# Install model with custom cache\npoetry run modelmora install \\\n  sentence-transformers/all-MiniLM-L6-v2 \\\n  --cache-dir /mnt/models \\\n  --device cpu\n</code></pre>"},{"location":"getting-started/configuration/#docker-configuration","title":"Docker Configuration","text":""},{"location":"getting-started/configuration/#environment-file","title":"Environment File","text":"<p><code>.env</code>:</p> <pre><code>MODELMORA_HOST=0.0.0.0\nMODELMORA_PORT=8000\nMODELMORA_LOG_LEVEL=INFO\nMODELMORA_DB_PATH=/data/models.db\nMODELMORA_CACHE_DIR=/cache\nMODELMORA_MAX_WORKERS=4\nMODELMORA_DEVICE=cuda\n</code></pre>"},{"location":"getting-started/configuration/#docker-compose","title":"Docker Compose","text":"<p><code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\n\nservices:\n  modelmora:\n    image: ghcr.io/jomarjunior/modelmora:latest\n    ports:\n      - \"8000:8000\"\n      - \"50051:50051\"\n    env_file:\n      - .env\n    environment:\n      - MODELMORA_MAX_WORKERS=8\n    volumes:\n      - ./data:/data\n      - ./cache:/cache\n      - ./config:/app/config\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n</code></pre>"},{"location":"getting-started/configuration/#kubernetes-configuration","title":"Kubernetes Configuration","text":""},{"location":"getting-started/configuration/#configmap","title":"ConfigMap","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: modelmora-config\ndata:\n  modelmora.yml: |\n    server:\n      host: 0.0.0.0\n      port: 8000\n    resources:\n      max_workers: 4\n      max_memory_gb: 16\n</code></pre>"},{"location":"getting-started/configuration/#deployment","title":"Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: modelmora\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: modelmora\n        image: ghcr.io/jomarjunior/modelmora:latest\n        env:\n        - name: MODELMORA_PORT\n          value: \"8000\"\n        - name: MODELMORA_DB_PATH\n          value: \"/data/models.db\"\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n        - name: cache\n          mountPath: /cache\n      volumes:\n      - name: config\n        configMap:\n          name: modelmora-config\n      - name: cache\n        persistentVolumeClaim:\n          claimName: modelmora-cache\n</code></pre>"},{"location":"getting-started/configuration/#validation","title":"Validation","text":"<p>Validate your configuration:</p> <pre><code># Check configuration syntax\npoetry run modelmora config validate --file config/modelmora.yml\n\n# Show effective configuration\npoetry run modelmora config show\n\n# Test model loading\npoetry run modelmora config test-models --config config/models.yml\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference</li> <li>Deployment Guide</li> <li>Architecture Overview</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install and set up ModelMora.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher (3.10, 3.11, 3.12 supported)</li> <li>Poetry 1.8.0 or higher</li> <li>Docker (optional, for containerized deployment)</li> <li>CUDA 13.0+ (optional, for GPU support)</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<ol> <li>Clone the repository</li> </ol> <pre><code>git clone https://github.com/JomarJunior/modelmora.git\ncd modelmora\n</code></pre> <ol> <li>Install dependencies with Poetry</li> </ol> <pre><code>poetry install\n</code></pre> <ol> <li>Activate the virtual environment</li> </ol> <pre><code>poetry shell\n</code></pre> <ol> <li>Verify installation</li> </ol> <pre><code>modelmora --version\n</code></pre>"},{"location":"getting-started/installation/#using-docker","title":"Using Docker","text":"<ol> <li>Pull the Docker image</li> </ol> <pre><code>docker pull ghcr.io/jomarjunior/modelmora:latest\n</code></pre> <ol> <li>Run the container</li> </ol> <pre><code>docker run -p 8000:8000 ghcr.io/jomarjunior/modelmora:latest\n</code></pre>"},{"location":"getting-started/installation/#using-docker-compose","title":"Using Docker Compose","text":"<ol> <li>Create a <code>docker-compose.yml</code></li> </ol> <pre><code>version: '3.8'\n\nservices:\n  modelmora:\n    image: ghcr.io/jomarjunior/modelmora:latest\n    ports:\n      - \"8000:8000\"\n      - \"50051:50051\"  # gRPC\n    environment:\n      - MODELMORA_DB_PATH=/data/models.db\n      - MODELMORA_CACHE_DIR=/cache\n    volumes:\n      - ./data:/data\n      - ./cache:/cache\n      - ./config:/app/config\n</code></pre> <ol> <li>Start the service</li> </ol> <pre><code>docker-compose up -d\n</code></pre>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>For development work, install with dev dependencies:</p> <pre><code>poetry install --with dev,docs\n</code></pre> <p>Install pre-commit hooks:</p> <pre><code>poetry run pre-commit install\n</code></pre>"},{"location":"getting-started/installation/#gpu-support","title":"GPU Support","text":""},{"location":"getting-started/installation/#cuda-setup","title":"CUDA Setup","text":"<p>ModelMora uses PyTorch with CUDA 13.0 support. Ensure you have:</p> <ol> <li>NVIDIA GPU with compute capability 3.5 or higher</li> <li>NVIDIA drivers (version 525.85 or higher)</li> <li>CUDA Toolkit 13.0+</li> </ol> <p>Verify GPU availability:</p> <pre><code>poetry run python -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n</code></pre>"},{"location":"getting-started/installation/#cpu-only-installation","title":"CPU-Only Installation","text":"<p>For CPU-only environments, modify the torch source in <code>pyproject.toml</code>:</p> <pre><code>torch = {version = \"^2.9.0\"}\ntorchvision = {version = \"^0.24.0\"}\n</code></pre> <p>Then run:</p> <pre><code>poetry lock --no-update\npoetry install\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code># Check health\ncurl http://localhost:8000/health\n\n# Run tests\npoetry run pytest\n\n# Check linting\npoetry run black --check .\npoetry run pylint src/modelmora\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>Configuration</li> <li>API Reference</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get ModelMora up and running in minutes.</p>"},{"location":"getting-started/quickstart/#start-the-service","title":"Start the Service","text":"<pre><code># Using Poetry\npoetry run modelmora serve\n\n# Using Docker\ndocker run -p 8000:8000 ghcr.io/jomarjunior/modelmora:latest\n</code></pre> <p>The service will start on: - REST API: <code>http://localhost:8000</code> - gRPC API: <code>localhost:50051</code> - API Documentation: <code>http://localhost:8000/docs</code></p>"},{"location":"getting-started/quickstart/#register-your-first-model","title":"Register Your First Model","text":""},{"location":"getting-started/quickstart/#using-cli","title":"Using CLI","text":"<pre><code># Install a model from HuggingFace Hub\npoetry run modelmora install sentence-transformers/all-MiniLM-L6-v2\n\n# List installed models\npoetry run modelmora list\n\n# Get model info\npoetry run modelmora info all-MiniLM-L6-v2\n</code></pre>"},{"location":"getting-started/quickstart/#using-rest-api","title":"Using REST API","text":"<pre><code># Register a model\ncurl -X POST http://localhost:8000/models \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"all-MiniLM-L6-v2\",\n    \"source\": \"sentence-transformers/all-MiniLM-L6-v2\",\n    \"task\": \"text-embedding\",\n    \"device\": \"cuda\"\n  }'\n\n# List models\ncurl http://localhost:8000/models\n\n# Get model details\ncurl http://localhost:8000/models/all-MiniLM-L6-v2\n</code></pre>"},{"location":"getting-started/quickstart/#run-inference","title":"Run Inference","text":""},{"location":"getting-started/quickstart/#text-embedding-example","title":"Text Embedding Example","text":"<pre><code>curl -X POST http://localhost:8000/infer/all-MiniLM-L6-v2 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"text\": \"ModelMora is a high-performance inference server\"\n  }'\n</code></pre> <p>Response:</p> <pre><code>{\n  \"result\": {\n    \"embeddings\": [0.123, -0.456, 0.789, ...]\n  },\n  \"metadata\": {\n    \"model\": \"all-MiniLM-L6-v2\",\n    \"version\": \"1.0.0\",\n    \"device\": \"cuda:0\"\n  },\n  \"timing\": {\n    \"queue_time_ms\": 0.7,\n    \"inference_time_ms\": 15.2,\n    \"total_time_ms\": 15.9\n  }\n}\n</code></pre>"},{"location":"getting-started/quickstart/#using-python-client","title":"Using Python Client","text":"<pre><code>import requests\n\n# Submit inference request\nresponse = requests.post(\n    \"http://localhost:8000/infer/all-MiniLM-L6-v2\",\n    json={\"text\": \"Hello, ModelMora!\"}\n)\n\nresult = response.json()\nembeddings = result[\"result\"][\"embeddings\"]\nprint(f\"Embedding dimension: {len(embeddings)}\")\n</code></pre>"},{"location":"getting-started/quickstart/#using-grpc-client","title":"Using gRPC Client","text":"<pre><code>import grpc\nfrom modelmora.protos import inference_pb2, inference_pb2_grpc\n\n# Create channel and stub\nchannel = grpc.insecure_channel('localhost:50051')\nstub = inference_pb2_grpc.InferenceServiceStub(channel)\n\n# Submit request\nrequest = inference_pb2.InferenceRequest(\n    model_name=\"all-MiniLM-L6-v2\",\n    text=\"Hello, ModelMora!\"\n)\n\nresponse = stub.Infer(request)\nprint(f\"Result: {response.embeddings}\")\n</code></pre>"},{"location":"getting-started/quickstart/#configuration-file","title":"Configuration File","text":"<p>Create <code>config/models.yml</code>:</p> <pre><code>models:\n  - name: all-MiniLM-L6-v2\n    source: sentence-transformers/all-MiniLM-L6-v2\n    task: text-embedding\n    device: ${DEVICE:-cuda}\n    config:\n      max_seq_length: 512\n      batch_size: 32\n\n  - name: stable-diffusion-v1-5\n    source: runwayml/stable-diffusion-v1-5\n    task: text-to-image\n    device: cuda\n    config:\n      guidance_scale: 7.5\n      num_inference_steps: 50\n</code></pre> <p>Load models from config:</p> <pre><code>poetry run modelmora init --config config/models.yml\n</code></pre>"},{"location":"getting-started/quickstart/#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:8000/health\n</code></pre> <p>Response:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"version\": \"0.1.0\",\n  \"uptime_seconds\": 3600,\n  \"models_loaded\": 2,\n  \"total_requests\": 1523,\n  \"workers\": {\n    \"active\": 2,\n    \"idle\": 1\n  }\n}\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide</li> <li>REST API Reference</li> <li>gRPC API Reference</li> <li>Deployment Guide</li> </ul>"},{"location":"getting-started/quickstart/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quickstart/#model-not-found","title":"Model Not Found","text":"<pre><code># Download model cache\npoetry run modelmora install &lt;model-name&gt;\n</code></pre>"},{"location":"getting-started/quickstart/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<pre><code># Reduce batch size in config/models.yml\nconfig:\n  batch_size: 8  # Lower value\n</code></pre>"},{"location":"getting-started/quickstart/#port-already-in-use","title":"Port Already in Use","text":"<pre><code># Change port\npoetry run modelmora serve --port 8080\n</code></pre>"},{"location":"getting-started/testing/","title":"Testing","text":"<p>This guide defines how to write and structure tests for ModelMora to ensure code quality and reliability.</p>"},{"location":"getting-started/testing/#testing-framework","title":"Testing Framework","text":"<p>We use <code>pytest</code> as our primary testing framework. Make sure you have it installed in your development environment.</p>"},{"location":"getting-started/testing/#writing-tests","title":"Writing Tests","text":"<ol> <li>Type of Tests:</li> <li>Unit Tests: Test individual functions or classes in isolation. Should be placed in the <code>tests/unit/</code> directory.</li> <li>Integration Tests: Test interactions between different components. Should be placed in the <code>tests/integration/</code> directory.</li> <li>End-to-End Tests: Simulate real user scenarios to validate the entire application flow. Should be placed in the <code>tests/e2e/</code> directory.</li> <li>Directory Structure:</li> <li>Organize tests in a directory structure that mirrors the application structure for easier navigation.</li> <li>Naming Conventions:</li> <li>Test files should be named with a <code>test_</code> prefix (e.g., <code>test_module.py</code>).</li> <li>Test functions should also start with <code>test_</code> (e.g., <code>def test_function():</code>).</li> <li>Use descriptive names for test functions to indicate their purpose. The name should specify the expected behavior or condition being tested. Examples: <code>def test_add_user_with_valid_data_should_succeed():</code>, <code>def test_calculate_discount_with_invalid_input_should_raise_error():</code>, <code>def test_process_payment_with_insufficient_funds_should_fail():</code>.</li> <li>Assertions:</li> <li>Use clear and specific assertions to validate expected outcomes. Prefer using <code>assert</code> statements provided by <code>pytest</code>.</li> <li>Example: <code>assert result == expected_value</code></li> <li>For exception testing, use <code>with pytest.raises(ExpectedException):</code>.</li> <li>Structure of a Test:</li> <li>Arrange: Set up the necessary preconditions and inputs.</li> <li>Act: Execute the code being tested.</li> <li>Assert: Verify that the outcome matches the expected result.</li> <li>Fixtures:</li> <li>Use <code>pytest</code> fixtures to set up and tear down test environments. This helps in reusing code and maintaining clean tests.</li> <li> <p>Example:</p> <p><code>python  @pytest.fixture  def sample_data():      return {\"key\": \"value\"}</code></p> </li> <li> <p>Clear Bugs:</p> </li> <li>If, when running tests, you find clear bugs in the code, and only if it is clear that the fix is straightforward and does not require extensive discussion or design changes, please feel free to fix the implementation directly.</li> </ol>"},{"location":"getting-started/testing/#coverage","title":"Coverage","text":"<p>We use <code>pytest-cov</code> to measure code coverage. The goal is to maintain close to 99% coverage across the codebase.</p>"},{"location":"getting-started/testing/#running-tests","title":"Running Tests","text":"<p>The tests can be executed through VS Code's testing interface if available, with coverage support showing which lines are covered by tests. Alternatively, you can run tests from the command line:</p> <pre><code>./.venv/bin/poetry run pytest tests # Linux / MacOS\n</code></pre> <pre><code>.\\.venv\\Scripts\\poetry run pytest tests # Windows\n</code></pre> <p>Do not forget to use the .venv environment to ensure all dependencies are correctly resolved.</p>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/","title":"#1 - RFC Writting Patterns","text":"<p>Date published: 2025-12-11 \\ Status: Approved</p> Author Jomar J\u00fanior de Souza Pereira"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#table-of-contents","title":"\ud83d\udcdd Table of contents","text":"<ul> <li>\ud83c\udf0e Overview and context</li> <li>\ud83c\udfaf Goals and requirements</li> <li>\ud83d\uddd3\ufe0f Timeline and milestones</li> <li>\u2705 Proposal solution</li> <li>\ud83d\udfe0 Alternative options</li> <li>\u2753 Frequently asked questions (FAQ)</li> <li>\ud83d\udcce Apendix</li> <li>\ud83d\udcda References</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#overview-and-context","title":"\ud83c\udf0e Overview and context","text":"<p>This document describes the patterns and best practices for writing RFCs (Request for Comments) within our organization. The goal is to ensure clarity, consistency, and effectiveness in our RFC documentation process.</p>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#glossary-and-terms","title":"\ud83d\udcd1 Glossary and terms","text":"<ul> <li>RFC: Request for Comments, a formal document used to propose changes or new features.</li> <li>Author: The individual who writes and submits the RFC.</li> <li>Reviewer: The individual(s) who review the RFC for clarity, completeness, and feasibility.</li> <li>Stakeholder: Any individual or group affected by the RFC.</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#customer-business-impact","title":"\ud83d\udcbc Customer / Business impact","text":"<p>Clearly defined RFCs lead to better communication, reduced misunderstandings, and more efficient implementation of changes, ultimately benefiting the business and its customers.</p>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#goals-and-requirements","title":"\ud83c\udfaf Goals and requirements","text":"Goal Description Clarity Ensure that RFCs are easy to understand. Consistency Maintain a uniform structure across all RFCs. Effectiveness Facilitate the review and approval process. Accessibility Make RFCs easily accessible to all stakeholders. Traceability Ensure that changes can be tracked back to their RFCs."},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#out-of-scope","title":"\ud83d\udeab Out of scope","text":"<ul> <li>Technical implementation details.</li> <li>Post-approval processes.</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#timeline-and-milestones","title":"\ud83d\uddd3\ufe0f Timeline and milestones","text":"Milestone Date Draft Completion 2025-12-11 Review Period 2025-12-11 Final Approval 2025-12-11 Publication 2025-12-11"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#proposal-solution","title":"\u2705 Proposal solution","text":""},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#rfc-document-structure","title":"RFC Document Structure","text":"<p>All RFCs in the ModelMora project shall follow this standardized template:</p>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#1-header-section","title":"1. Header Section","text":"<pre><code># #&lt;number&gt; - &lt;Title&gt;\n\nDate published: YYYY-MM-DD\nStatus: **Draft | Under Review | Approved | Rejected | Completed**\n\n|Author|&lt;Full Name&gt;|\n|---|---|\n</code></pre> <p>Status definitions:</p> <ul> <li>Draft: Initial version, not yet ready for review</li> <li>Under Review: Currently being reviewed by stakeholders</li> <li>Approved: Accepted and ready for implementation</li> <li>Rejected: Not accepted, includes reasoning</li> <li>Completed: Implementation finished and verified</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#2-table-of-contents","title":"2. Table of Contents","text":"<p>Must include all major sections with anchors for easy navigation.</p>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#3-overview-and-context","title":"3. Overview and Context","text":"<p>Purpose: Provide background information and explain why the RFC is needed.</p> <p>Required subsections:</p> <ul> <li>Glossary and terms: Define domain-specific terminology</li> <li>Customer / Business impact: Describe the value proposition</li> </ul> <p>Guidelines:</p> <ul> <li>Keep it concise (200-400 words)</li> <li>Avoid technical jargon in this section</li> <li>Focus on the \"why\" rather than the \"how\"</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#4-goals-and-requirements","title":"4. Goals and Requirements","text":"<p>Purpose: Define clear, measurable objectives and constraints.</p> <p>Required content:</p> <ul> <li>Goals table with descriptions</li> <li>Out of scope section (explicitly state what is NOT covered)</li> </ul> <p>Guidelines:</p> <ul> <li>Use SMART criteria (Specific, Measurable, Achievable, Relevant, Time-bound)</li> <li>Maximum 5-7 primary goals</li> <li>Be explicit about boundaries and limitations</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#5-timeline-and-milestones","title":"5. Timeline and Milestones","text":"<p>Purpose: Establish a project schedule with key dates.</p> <p>Required format:</p> <pre><code>| Milestone | Date |\n|---|---|\n| Draft Completion | YYYY-MM-DD |\n| Review Period | YYYY-MM-DD |\n| Final Approval | YYYY-MM-DD |\n| Implementation Start | YYYY-MM-DD |\n| Implementation Complete | YYYY-MM-DD |\n</code></pre> <p>Guidelines:</p> <ul> <li>Include realistic timelines</li> <li>Account for review cycles</li> <li>Add buffer time for unexpected delays</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#6-proposal-solution","title":"6. Proposal Solution","text":"<p>Purpose: Present the recommended approach in detail.</p> <p>Required content:</p> <ul> <li>Technical design overview</li> <li>Architecture diagrams (when applicable)</li> <li>Implementation strategy</li> <li>Success metrics</li> <li>Risk assessment and mitigation</li> </ul> <p>Guidelines:</p> <ul> <li>Use diagrams, flowcharts, and code samples</li> <li>Break down complex solutions into subsections</li> <li>Include concrete examples</li> <li>Reference industry standards (IEEE, IETF, W3C, etc.)</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#7-alternative-options","title":"7. Alternative Options","text":"<p>Purpose: Document other considered approaches and why they were not chosen.</p> <p>Required format:</p> <pre><code>### Option 1: &lt;Name&gt;\n**Description**: ...\n**Pros**: ...\n**Cons**: ...\n**Reason for rejection**: ...\n</code></pre> <p>Guidelines:</p> <ul> <li>Present at least 2-3 alternatives</li> <li>Be objective in evaluation</li> <li>Explain trade-offs clearly</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#8-frequently-asked-questions-faq","title":"8. Frequently Asked Questions (FAQ)","text":"<p>Purpose: Address common questions and concerns proactively.</p> <p>Guidelines:</p> <ul> <li>Include questions raised during drafting</li> <li>Add technical clarifications</li> <li>Address implementation concerns</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#9-appendix","title":"9. Appendix","text":"<p>Purpose: Provide supplementary information.</p> <p>Common items:</p> <ul> <li>Detailed technical specifications</li> <li>Code examples</li> <li>Performance benchmarks</li> <li>Security considerations</li> <li>Compatibility matrices</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#10-references","title":"10. References","text":"<p>Purpose: Cite all external sources and standards.</p> <p>Required format:</p> <pre><code>[1] Author. \"Title.\" Publication, Year. URL\n[2] IEEE Standard XXX-YYYY, \"Title,\" Year.\n</code></pre>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#writing-style-guidelines","title":"Writing Style Guidelines","text":""},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#language-and-tone","title":"Language and Tone","text":"<ul> <li>Clarity: Use simple, direct language</li> <li>Conciseness: Eliminate redundant words</li> <li>Consistency: Maintain uniform terminology</li> <li>Objectivity: Present facts without bias</li> <li>Precision: Be specific and unambiguous</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#formatting-standards","title":"Formatting Standards","text":"<ul> <li>Headings: Use hierarchical markdown headings (H1 \u2192 H6)</li> <li>Lists: Use bullet points for unordered items, numbers for sequences</li> <li>Tables: Align columns and use consistent spacing</li> <li>Code blocks: Always specify the language for syntax highlighting</li> <li>Emphasis: Use bold for key terms, italics for emphasis</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#technical-content","title":"Technical Content","text":"<ul> <li>Diagrams: Use PlantUML on most cases, ASCII art only when necessary</li> <li>Code samples: Include complete, runnable examples</li> <li>Metrics: Quantify impacts with concrete numbers</li> <li>Standards: Reference specific IEEE, ISO, or IETF standards</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#review-and-approval-process","title":"Review and Approval Process","text":""},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#review-stages","title":"Review Stages","text":"<ol> <li>Self-review: Author reviews own RFC for completeness</li> <li>Peer review: Technical peers review for accuracy</li> <li>Stakeholder review: Business stakeholders review impact</li> <li>Final approval: Designated approver(s) sign off</li> </ol>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#review-checklist","title":"Review Checklist","text":"<ul> <li>[ ] All required sections are present</li> <li>[ ] Goals are clear and measurable</li> <li>[ ] Timeline is realistic</li> <li>[ ] Technical solution is detailed</li> <li>[ ] Alternatives are considered</li> <li>[ ] Risks are identified and mitigated</li> <li>[ ] References are complete and accurate</li> <li>[ ] Diagrams are clear and accurate</li> <li>[ ] Code examples compile/run</li> <li>[ ] Grammar and spelling are correct</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#feedback-process","title":"Feedback Process","text":"<ul> <li>Use GitHub Pull Requests for RFC submissions</li> <li>Address feedback inline with threaded comments</li> <li>Track decisions in the PR description</li> <li>Update RFC status based on review outcomes</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#versioning-and-updates","title":"Versioning and Updates","text":""},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#version-control","title":"Version Control","text":"<ul> <li>Store RFCs in <code>/docs/rfc/</code> directory</li> <li>Use sequential numbering: <code>#1</code>, <code>#2</code>, <code>#3</code>, etc.</li> <li>Include date in filename for major revisions: <code>#1 - RFC Writting Patterns [2025-12-11].md</code></li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#amendment-process","title":"Amendment Process","text":"<p>For approved RFCs requiring changes:</p> <ol> <li>Create new section: \"## \ud83d\udcdd Amendments\"</li> <li>Document change with date and rationale</li> <li>Update status if needed</li> <li>Notify stakeholders of changes</li> </ol> <p>Amendment format:</p> <pre><code>### Amendment 1 - 2025-MM-DD\n**Change**: &lt;description&gt;\n**Rationale**: &lt;reason&gt;\n**Impact**: &lt;scope of change&gt;\n</code></pre>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#ieee-compliance","title":"IEEE Compliance","text":"<p>This RFC template aligns with IEEE standards, specifically:</p> <ul> <li>IEEE 830-1998: Software Requirements Specifications</li> <li>IEEE 1063-2001: Software User Documentation</li> <li>IEEE 2001-2002: Recommended Practice for Internet Practices</li> </ul> <p>Key IEEE principles applied:</p> <ol> <li>Completeness: All necessary information is included</li> <li>Consistency: Uniform terminology and structure</li> <li>Correctness: Accurate technical content</li> <li>Modifiability: Clear versioning and amendment process</li> <li>Traceability: References and decision documentation</li> <li>Verifiability: Measurable goals and success criteria</li> </ol>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#alternative-options","title":"\ud83d\udfe0 Alternative options","text":""},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#option-1-lightweight-markdown-documents","title":"Option 1: Lightweight Markdown Documents","text":"<p>Description: Use simple markdown files without strict structure or templates.</p> <p>Pros:</p> <ul> <li>Faster to write</li> <li>Less overhead</li> <li>More flexibility</li> </ul> <p>Cons:</p> <ul> <li>Inconsistent documentation</li> <li>Missing critical information</li> <li>Difficult to review systematically</li> <li>Poor traceability</li> </ul> <p>Reason for rejection: Lack of structure leads to incomplete proposals and miscommunication, especially in complex technical decisions.</p>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#option-2-adopt-ietf-rfc-format-strictly","title":"Option 2: Adopt IETF RFC Format Strictly","text":"<p>Description: Follow the exact IETF RFC format (RFC 7322) with ASCII text files.</p> <p>Pros:</p> <ul> <li>Industry-standard format</li> <li>Well-documented process</li> <li>Proven track record</li> </ul> <p>Cons:</p> <ul> <li>Too formal for internal use</li> <li>Limited formatting options</li> <li>Steep learning curve</li> <li>Not optimized for diagrams/code</li> </ul> <p>Reason for rejection: While excellent for internet standards, IETF format is overly formal for internal software project RFCs and lacks modern markdown features.</p>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#option-3-use-architecture-decision-records-adrs","title":"Option 3: Use Architecture Decision Records (ADRs)","text":"<p>Description: Implement lightweight ADR format (MADR template).</p> <p>Pros:</p> <ul> <li>Simple and focused</li> <li>Quick to write</li> <li>Good for small decisions</li> </ul> <p>Cons:</p> <ul> <li>Too brief for complex proposals</li> <li>Lacks business context</li> <li>No timeline tracking</li> <li>Limited structure for large changes</li> </ul> <p>Reason for rejection: ADRs are complementary but insufficient for comprehensive project proposals requiring business impact analysis and detailed timelines.</p>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#frequently-asked-questions-faq","title":"\u2753 Frequently asked questions (FAQ)","text":""},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#q1-when-should-i-write-an-rfc","title":"Q1: When should I write an RFC?","text":"<p>A: Write an RFC for:</p> <ul> <li>Major architectural changes</li> <li>New features affecting multiple components</li> <li>Changes to public APIs or interfaces</li> <li>Process or workflow modifications</li> <li>Decisions with long-term impact</li> </ul> <p>Do NOT write an RFC for:</p> <ul> <li>Bug fixes</li> <li>Minor refactoring</li> <li>Documentation updates</li> <li>Routine maintenance</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#q2-how-long-should-an-rfc-be","title":"Q2: How long should an RFC be?","text":"<p>A: Typical length ranges:</p> <ul> <li>Simple proposals: 2-4 pages</li> <li>Standard proposals: 5-10 pages</li> <li>Complex proposals: 10-20 pages</li> </ul> <p>Focus on clarity over length. Include enough detail for informed decision-making.</p>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#q3-who-should-review-my-rfc","title":"Q3: Who should review my RFC?","text":"<p>A: Required reviewers:</p> <ul> <li>Technical lead for the affected component</li> <li>At least 2 peers with relevant expertise</li> <li>Product owner or business stakeholder</li> <li>Security/compliance team (for security-related changes)</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#q4-can-i-update-an-approved-rfc","title":"Q4: Can I update an approved RFC?","text":"<p>A: Yes, through the amendment process:</p> <ol> <li>Add amendment section with date and details</li> <li>Submit as PR for review</li> <li>Get approval from original approvers</li> <li>Update RFC status if scope changes significantly</li> </ol>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#q5-what-if-my-rfc-is-rejected","title":"Q5: What if my RFC is rejected?","text":"<p>A: Document the rejection:</p> <ol> <li>Update status to \"Rejected\"</li> <li>Add rejection rationale</li> <li>Keep the RFC for historical reference</li> <li>Consider alternative approaches or resubmit with modifications</li> </ol>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#q6-should-i-include-code-in-rfcs","title":"Q6: Should I include code in RFCs?","text":"<p>A: Yes, when it helps clarify the proposal:</p> <ul> <li>Use code snippets for API designs</li> <li>Show before/after examples</li> <li>Include pseudocode for algorithms</li> <li>Keep examples concise and relevant</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#q7-how-do-i-handle-confidential-information","title":"Q7: How do I handle confidential information?","text":"<p>A:</p> <ul> <li>Mark RFC as \"Confidential\" in header</li> <li>Store in private repository if needed</li> <li>Redact sensitive details in public versions</li> <li>Follow company security policies</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#q8-whats-the-difference-between-rfc-and-design-documents","title":"Q8: What's the difference between RFC and design documents?","text":"<p>A:</p> <ul> <li>RFC: Proposes changes, seeks approval, includes alternatives</li> <li>Design doc: Details implementation of approved RFC</li> <li>RFCs answer \"what and why\", design docs answer \"how\"</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#apendix","title":"\ud83d\udcce Apendix","text":""},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#a-complete-rfc-template","title":"A. Complete RFC Template","text":"<pre><code># #&lt;number&gt; - &lt;Title&gt;\n\nDate published: YYYY-MM-DD\nStatus: **Draft**\n\n|Author|&lt;Full Name&gt;|\n|---|---|\n\n## \ud83d\udcdd Table of contents\n\n- [\ud83c\udf0e Overview and context](#\ud83c\udf0e-overview-and-context)\n- [\ud83c\udfaf Goals and requirements](#\ud83c\udfaf-goals-and-requirements)\n- [\ud83d\uddd3\ufe0f Timeline and milestones](#\ud83d\uddd3\ufe0f-timeline-and-milestones)\n- [\u2705 Proposal solution](#\u2705-proposal-solution)\n- [\ud83d\udfe0 Alternative options](#\ud83d\udfe0-alternative-options)\n- [\u2753 Frequently asked questions (FAQ)](#\u2753-frequently-asked-questions-faq)\n- [\ud83d\udcce Apendix](#\ud83d\udcce-apendix)\n- [\ud83d\udcda References](#\ud83d\udcda-references)\n\n## \ud83c\udf0e Overview and context\n\n[Provide background and motivation]\n\n### \ud83d\udcd1 Glossary and terms\n\n- **Term 1**: Definition\n- **Term 2**: Definition\n\n### \ud83d\udcbc Customer / Business impact\n\n[Describe value and impact]\n\n## \ud83c\udfaf Goals and requirements\n\n| **Goal** | **Description** |\n|---|---|\n| Goal 1 | Description |\n\n### \ud83d\udeab Out of scope\n\n- Item 1\n- Item 2\n\n## \ud83d\uddd3\ufe0f Timeline and milestones\n\n| **Milestone** | **Date** |\n|---|---|\n| Draft Completion | YYYY-MM-DD |\n\n## \u2705 Proposal solution\n\n[Detailed proposal]\n\n## \ud83d\udfe0 Alternative options\n\n### Option 1: [Name]\n**Description**: ...\n**Pros**: ...\n**Cons**: ...\n**Reason for rejection**: ...\n\n## \u2753 Frequently asked questions (FAQ)\n\n### Q: Question?\n**A**: Answer\n\n## \ud83d\udcce Apendix\n\n[Supplementary information]\n\n## \ud83d\udcda References\n\n[1] Citation\n</code></pre>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#b-example-diagrams","title":"B. Example Diagrams","text":""},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#example-1-architecture-diagram","title":"Example 1: Architecture Diagram","text":"<pre>85a4e0b7cf1de08999356aa299dbb4cd900169aaaf4bbd69264654faa79b0e179f364d8bcdaaf5580bdfadefe34c6de21a1ca09322e8992689e37c4c315d48ad</pre><pre>6a563f1f9aaa37808744c89f399d1ffaa526b462c03821657b2a091601ef553b60ebca7683f5f96161da341446041d091362c5fb44ae7af05a38e52242ed519c</pre>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#example-2-sequence-diagram","title":"Example 2: Sequence Diagram","text":"<pre>8a725ed1f11cd92861f9afdbc76771ae80f45763dbafd0a418fb07a32113a970e5d87a4e80c375cb9e64ae99ecd1e6accd827f543e80595b7acaea7d48e570dc</pre><pre>c96f9fd2c3b5c04834722ba473243ce46ba090db83e7cf68d42a3edd1a77b5d7c65922f56591a4b34c5f31c8c6bae403de16b5b98a10aafd68e620fd5c4632ef</pre>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#example-3-flowchart","title":"Example 3: Flowchart","text":"<pre>5fcf15e5700197857950866c01864aba9a140a7dc3d7f4b0deec975c2e99d18d6df9e3fcada81d87f9acb32a22bf2d81ee43537d070dea438383214b3ac26361</pre><pre>8360bdfbce2c7ad7b66ffd6b7d0238dc6061676780164d37b0f080cad21c6866a7b34e0a1762dbaf6f07edbb008257a6641762c6e250d0bf98ca4dc9b499fa94</pre>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#c-status-transition-matrix","title":"C. Status Transition Matrix","text":"From \u2192 To Valid Requires Draft \u2192 Under Review \u2705 Complete all sections Under Review \u2192 Draft \u2705 Major revision needed Under Review \u2192 Approved \u2705 All reviewers approve Under Review \u2192 Rejected \u2705 Document rationale Approved \u2192 Completed \u2705 Implementation verified Approved \u2192 Under Review \u26a0\ufe0f Significant changes only Rejected \u2192 Draft \u2705 Substantial rework"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#d-checklist-for-authors","title":"D. Checklist for Authors","text":"<p>Before Submission:</p> <ul> <li>[x] RFC number assigned sequentially</li> <li>[x] All sections completed</li> <li>[x] Spell-check and grammar review</li> <li>[x] Technical accuracy verified</li> <li>[x] Code examples tested</li> <li>[x] Diagrams are clear</li> <li>[x] References cited properly</li> <li>[x] Out of scope explicitly stated</li> </ul> <p>During Review:</p> <ul> <li>[x] Respond to all comments</li> <li>[x] Update RFC based on feedback</li> <li>[x] Maintain change log</li> <li>[x] Address stakeholder concerns</li> </ul> <p>After Approval:</p> <ul> <li>[x] Update status to \"Approved\"</li> <li>[x] Create implementation tickets</li> <li>[x] Notify relevant teams</li> <li>[x] Archive in documentation</li> </ul>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#e-common-pitfalls-to-avoid","title":"E. Common Pitfalls to Avoid","text":"<ol> <li>Vague goals: Use measurable, specific objectives</li> <li>Missing alternatives: Always consider other approaches</li> <li>No timeline: Include realistic dates and milestones</li> <li>Scope creep: Clearly define what's out of scope</li> <li>Insufficient detail: Provide enough information for implementation</li> <li>Ignoring feedback: Address all review comments</li> <li>No success metrics: Define how to measure success</li> <li>Missing risks: Identify and plan for potential issues</li> </ol>"},{"location":"rfc/%231%20-%20RFC%20Writting%20Patterns/#references","title":"\ud83d\udcda References","text":"<p>[1] IEEE Computer Society. \"IEEE Standard for Software Requirements Specifications,\" IEEE Std 830-1998, 1998.</p> <p>[2] IEEE Computer Society. \"IEEE Standard for Software User Documentation,\" IEEE Std 1063-2001, 2001.</p> <p>[3] Internet Engineering Task Force. \"RFC 7322: RFC Style Guide,\" IETF, September 2014. https://www.rfc-editor.org/rfc/rfc7322</p> <p>[4] Nygard, M. \"Documenting Architecture Decisions,\" 2011. https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions</p> <p>[5] IEEE Computer Society. \"IEEE Recommended Practice for Internet Practices\u2014Web Page Engineering\u2014Intranet/Extranet Applications,\" IEEE Std 2001-2002, 2002.</p> <p>[6] The Linux Foundation. \"CONTRIBUTING.md Guide,\" Best Practices for Open Source Projects, 2023.</p> <p>[7] GitLab. \"RFC Process,\" GitLab Handbook. https://about.gitlab.com/handbook/engineering/architecture/workflow/</p> <p>[8] Atlassian. \"Decision Records,\" Agile at Scale Documentation, 2024.</p>"}]}