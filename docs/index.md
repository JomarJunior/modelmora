# ModelMora

**Neural Network Model Management and Inference Serving Microservice**

---

## Overview

ModelMora is a high-performance inference serving microservice designed to manage and serve multiple neural network models efficiently. Built for the MiraVeja ecosystem, it provides robust model lifecycle management, intelligent request queuing, and multi-process isolation for optimal resource utilization.

## Key Features

- ðŸš€ **High Performance**: Async-first architecture with validated <1Î¼s queue latency
- ðŸ”’ **Memory Isolation**: Multi-process architecture with 5000x better memory cleanup than GC
- ðŸŽ¯ **Intelligent Queuing**: Priority-based request scheduling with 730k ops/sec capacity
- ðŸ”„ **Lazy Loading**: On-demand model loading with 2s load time for cached models
- ðŸ“Š **Multiple Protocols**: REST and gRPC APIs for different use cases
- ðŸ³ **Container Ready**: Docker and Kubernetes deployment support

## Architecture Highlights

ModelMora follows Domain-Driven Design (DDD) principles with clear bounded contexts:

- **Registry**: Model metadata and configuration management
- **Lifecycle**: Model loading, unloading, and health monitoring
- **Inference**: Request queuing, batching, and execution
- **Observability**: Metrics, logging, and tracing

## Quick Start

```bash
# Install dependencies
poetry install

# Start the service
poetry run modelmora serve

# Register a model
poetry run modelmora install sentence-transformers/all-MiniLM-L6-v2

# Run inference
curl -X POST http://localhost:8000/infer/all-MiniLM-L6-v2 \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello, world!"}'
```

## Performance Benchmarks

All architectural decisions validated through production-grade POCs:

| Metric | Value | Status |
|--------|-------|--------|
| Queue Latency | 0.7Î¼s enqueue | âœ… Validated |
| Model Load Time | 2s (cached) | âœ… Validated |
| Memory Cleanup | 0.1MB leak (subprocess) | âœ… Validated |
| gRPC Throughput | 31.92 MB/s | âœ… Validated |
| Queue Capacity | 730k ops/sec | âœ… Validated |

## Use Cases

- **Embedding Services**: Text and image embeddings at scale
- **Text Generation**: LLM inference with batching support
- **Image Generation**: Diffusion models with streaming responses
- **Multi-Model Serving**: Efficient resource sharing across models

## Project Status

- âœ… **Phase 0**: Requirements & Planning - Complete
- ðŸš§ **Phase 1**: MVP Core - In Progress
- ðŸ“… **Phase 2**: Production Ready - Planned
- ðŸ“… **Phase 3**: Scale & Polish - Planned

## Documentation

- [Getting Started](getting-started/installation.md)
- [Architecture](architecture.md)
- [API Reference](api-reference/rest.md)
- [Development Roadmap](roadmap.md)

## License

MIT License - see [LICENSE](https://github.com/JomarJunior/modelmora/blob/main/LICENSE) for details.
